{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n",
      "Contenido de ./data: ['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7b4c3929e350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Dict, List\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"../data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    return mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = [\n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    df = transform_variables(df)\n",
    "    df = scale_numerical_features(df, loaded_scaler)\n",
    "    df = impute_missings(df)\n",
    "    return df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds → hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "     \n",
    "\tnow = pd.Timestamp.now()\n",
    "\n",
    "\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\tmin_ts = 0                  # 1970-01-01\n",
    "\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t# Replace invalid timestamps with NaN\n",
    "\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\n",
    "\t# Convert safe Unix timestamps to datetime\n",
    "\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t# Compute hours ago\n",
    "\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t# Drop the original Unix timestamp columns\n",
    "\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Convert 'hour' from string to integer\n",
    "\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t# Convert hour to radians (full circle = 24 hours)\n",
    "\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t# Create two new columns: sin_hour and cos_hour\n",
    "\tdf['hour_sin'] = np.sin(radians)\n",
    "\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t# Drop the original 'hour' column\n",
    "\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Ensure `now_ts` is a Unix timestamp\n",
    "\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t# Apply to your 4 timestamp columns\n",
    "\tts_cols = [\n",
    "\t\t\"first_request_ts_bundle\",\n",
    "\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\"last_buy_ts_category\",\n",
    "\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t]\n",
    "\n",
    "\tfor col in ts_cols:\n",
    "\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t# Drop the original tuple columns if you want\n",
    "\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trules = {\n",
    "\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\"ctr\": \"sum\",\n",
    "\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t}\n",
    "\n",
    "\tfor col, mode in rules.items():\n",
    "\t\tnew_col = col + \"_agg\"\n",
    "\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t)\n",
    "\n",
    "\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t# If the df contains the target column, transform it with the logarithm\n",
    "\tif TARGET in df.columns:\n",
    "\t\tdf[TARGET] = np.log1p(df[TARGET])\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "    return df\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71f2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v1\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "CHECKPOINT_PATH = \"model_checkpoint_\" + VERSION\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# AMP = False\n",
    "\n",
    "cat_cardinalities = {\n",
    "    feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "    for feat in CATEGORICAL_FEATURES\n",
    "}\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_cardinalities: Dict[str, int],\n",
    "        cat_features: List[str],\n",
    "        num_cont_features: int,          # numéricas + indicadores de missing\n",
    "        embedding_dim: int = 32,\n",
    "        hidden_dims: List[int] = [256, 128, 64],\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_cat = len(cat_features)\n",
    "        self.num_cont_features = num_cont_features\n",
    "\n",
    "        # Embeddings por feature categórica\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "            for feat, cardinality in cat_cardinalities.items()\n",
    "        })\n",
    "\n",
    "        # Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "        total_cat_dim = embedding_dim * self.num_cat\n",
    "        input_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, num_cat + num_cont_features]\n",
    "        columnas en el orden:\n",
    "        [CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Separar categóricas y numéricas del gran vector\n",
    "        x_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "        x_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "        # 2) Embeddings categóricas\n",
    "        emb_list = []\n",
    "        for i, feat in enumerate(self.cat_features):\n",
    "            emb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "            emb_list.append(emb)\n",
    "\n",
    "        if emb_list:\n",
    "            x_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "            x_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "        else:\n",
    "            x_input = x_cont\n",
    "\n",
    "        out = self.mlp(x_input)   # [batch, 1]\n",
    "        return out.squeeze(-1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0a8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    AMP = False\n",
    "\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    if not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "        AMP = False\n",
    "    \n",
    "    train_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "    val_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "    \n",
    "    ddf_train = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=train_filters\n",
    "    )\n",
    "\n",
    "    ddf_val = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=val_filters\n",
    "    )\n",
    "\n",
    "    val_pdf = ddf_val.compute()\n",
    "    val_pdf = process_partition(val_pdf)\n",
    "\n",
    "    val_features_tensor = torch.tensor(\n",
    "        val_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    val_target_tensor = torch.tensor(\n",
    "        val_pdf[TARGET].values,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "    val_loader = DataLoader(dataset=val_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             shuffle=False)\n",
    "\n",
    "    cat_cardinalities = {\n",
    "        feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "        for feat in CATEGORICAL_FEATURES\n",
    "    }\n",
    "    model = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=AMP)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        partition_iterator = ddf_train.to_delayed()\n",
    "        \n",
    "        for partition_idx, partition in enumerate(partition_iterator):\n",
    "            train_loss_sum = 0.0\n",
    "\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {partition_idx+1}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            pdf = process_partition(pdf)\n",
    "            \n",
    "            # 2. Convert Pandas DF to PyTorch Tensors\n",
    "            features_tensor = torch.tensor(\n",
    "                pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            target_tensor = torch.tensor(\n",
    "                pdf[TARGET].values,\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            partition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "            partition_loader = DataLoader(dataset=partition_dataset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=True)\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(partition_loader):\n",
    "\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                if AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss_sum += loss.item()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {partition_idx+1} | Partition Train Loss: {train_loss_sum / (len(partition_loader)):.4f}\")\n",
    "\n",
    "        val_loss_sum = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                val_loss_sum += loss.item()\n",
    "        print(f\"\\n--- Epoch {epoch + 1} Completed ---\")\n",
    "        print(f\"Validation Loss: {val_loss_sum / (len(val_loader)):.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_PATH}_{epoch+1}.pth\")\n",
    "        print(f\"Model checkpoint saved to {CHECKPOINT_PATH}_{epoch+1}.pth\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b65108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- buy_d14\n- buy_d28\n- buy_d7\n- buyer_d1\n- buyer_d14\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m ddf_val = dd.read_parquet(\n\u001b[32m     20\u001b[39m     DATASET_PATH,\n\u001b[32m     21\u001b[39m     engine=\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     columns=COLS_TO_READ,\n\u001b[32m     23\u001b[39m     filters=val_filters\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m val_pdf = ddf_val.compute()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m val_pdf = \u001b[43mprocess_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m val_features_tensor = torch.tensor(\n\u001b[32m     30\u001b[39m     val_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_is_missing\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m NUMERICAL_FEATURES]].values, \n\u001b[32m     31\u001b[39m     dtype=torch.float32\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m val_target_tensor = torch.tensor(\n\u001b[32m     35\u001b[39m     val_pdf[TARGET].values,\n\u001b[32m     36\u001b[39m     dtype=torch.float32\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mprocess_partition\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_partition\u001b[39m(df):\n\u001b[32m      2\u001b[39m     df = transform_variables(df)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df = \u001b[43mscale_numerical_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_scaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     df = impute_missings(df)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mscale_numerical_features\u001b[39m\u001b[34m(df, scaler)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscale_numerical_features\u001b[39m(df, scaler):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     df[NUMERICAL_FEATURES] = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mNUMERICAL_FEATURES\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- buy_d14\n- buy_d28\n- buy_d7\n- buyer_d1\n- buyer_d14\n- ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TEST_PATH = \"../data/test\"\n",
    "\n",
    "ddf_test = dd.read_parquet(\n",
    "    DATASET_TEST_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=COLS_TO_READ + [\"row_id\"]\n",
    ")\n",
    "\n",
    "partition_iterator = ddf_test.to_delayed()\n",
    "\n",
    "for partition in partition_iterator:\n",
    "\n",
    "\n",
    "    test_pdf = partition.compute()\n",
    "\n",
    "\n",
    "row_ids = test_pdf[\"row_id\"].values\n",
    "test_pdf = process_partition(test_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ff9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 row_id  iap_revenue_d7\n",
      "0  819ecc0e-1a97-43ed-83f6-b9ede4f7fc48        2.286234\n",
      "1  0a7fbf18-5041-42af-bd0a-0cb6586b8598        0.001353\n",
      "2  fc1a2689-b136-4ffa-b23b-9d8215bd720f        0.001353\n",
      "3  0340fcc6-50bd-42ab-b9f4-4c1184b640cb        0.001353\n",
      "4  219d253f-bef4-4039-84b2-ed55f009cc43        0.001353\n",
      "Test predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_features_tensor = torch.tensor(\n",
    "    test_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(test_features_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                            batch_size=4096, \n",
    "                            num_workers=0,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "model.load_state_dict(torch.load(\"model_checkpoint_v1_4.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions row_id -> target and save to CSV\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(test_loader):\n",
    "        data = data.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        predictions.extend(output.cpu().numpy())\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"row_id\": row_ids,\n",
    "    \"iap_revenue_d7\": np.expm1(predictions)  # inverse of log1p\n",
    "})\n",
    "print(pred_df.head())\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TEST_PATH = \"../data/test\"\n",
    "\n",
    "ddf_test = dd.read_parquet(\n",
    "    DATASET_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=COLS_TO_READ + [\"row_id\"]\n",
    ")\n",
    "\n",
    "test_pdf = ddf_test.compute()\n",
    "row_ids = test_pdf[\"row_id\"].values\n",
    "test_pdf = process_partition(test_pdf)\n",
    "\n",
    "test_features_tensor = torch.tensor(\n",
    "    ddf_test[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(test_features_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=0,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "model.load_state_dict(torch.load(f\"{CHECKPOINT_PATH}_3\"))\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions row_id -> target and save to CSV\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data,) in enumerate(test_loader):\n",
    "        data = data.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        predictions.extend(output.cpu().numpy())\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"row_id\": row_ids,\n",
    "    \"iap_revenue_d7\": np.expm1(predictions)  # inverse of log1p\n",
    "})\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hack (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
