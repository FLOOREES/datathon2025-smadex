{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n",
      "Contenido de ./data: ['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7888d81f25d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Dict, List\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"../data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    return mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = ['buyer_d1', \n",
    "\t\t\t 'buyer_d7', \n",
    "\t\t\t 'buyer_d14', \n",
    "\t\t\t 'buyer_d28', \n",
    "\t\t\t 'buy_d7', \n",
    "\t\t\t 'buy_d14', \n",
    "\t\t\t 'buy_d28', \n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'iap_revenue_d14', \n",
    "\t\t\t 'iap_revenue_d28', \n",
    "\t\t\t 'registration', \n",
    "\t\t\t 'retention_d1_to_d7', \n",
    "\t\t\t 'retention_d3_to_d7', \n",
    "\t\t\t 'retention_d7_to_d14', \n",
    "\t\t\t 'retention_d1', \n",
    "\t\t\t 'retention_d3', \n",
    "\t\t\t 'retentiond7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    df = transform_variables(df)\n",
    "    df = scale_numerical_features(df, loaded_scaler)\n",
    "    df = impute_missings(df)\n",
    "    return df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds → hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "     \n",
    "\tnow = pd.Timestamp.now()\n",
    "\n",
    "\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\tmin_ts = 0                  # 1970-01-01\n",
    "\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t# Replace invalid timestamps with NaN\n",
    "\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\n",
    "\t# Convert safe Unix timestamps to datetime\n",
    "\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t# Compute hours ago\n",
    "\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t# Drop the original Unix timestamp columns\n",
    "\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Convert 'hour' from string to integer\n",
    "\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t# Convert hour to radians (full circle = 24 hours)\n",
    "\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t# Create two new columns: sin_hour and cos_hour\n",
    "\tdf['hour_sin'] = np.sin(radians)\n",
    "\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t# Drop the original 'hour' column\n",
    "\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Ensure `now_ts` is a Unix timestamp\n",
    "\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t# Apply to your 4 timestamp columns\n",
    "\tts_cols = [\n",
    "\t\t\"first_request_ts_bundle\",\n",
    "\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\"last_buy_ts_category\",\n",
    "\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t]\n",
    "\n",
    "\tfor col in ts_cols:\n",
    "\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t# Drop the original tuple columns if you want\n",
    "\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trules = {\n",
    "\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\"ctr\": \"sum\",\n",
    "\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t}\n",
    "\n",
    "\tfor col, mode in rules.items():\n",
    "\t\tnew_col = col + \"_agg\"\n",
    "\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t)\n",
    "\n",
    "\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t# If the df contains the target column, transform it with the logarithm\n",
    "\tif TARGET in df.columns:\n",
    "\t\tdf[TARGET] = np.log1p(df[TARGET])\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "    return df\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab0a8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "CHECKPOINT_PATH = \"model_checkpoint.pth\"\n",
    "# AMP = False\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_cardinalities: Dict[str, int],\n",
    "        cat_features: List[str],\n",
    "        num_cont_features: int,          # numéricas + indicadores de missing\n",
    "        embedding_dim: int = 16,\n",
    "        hidden_dims: List[int] = [128, 64],\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_cat = len(cat_features)\n",
    "        self.num_cont_features = num_cont_features\n",
    "\n",
    "        # Embeddings por feature categórica\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "            for feat, cardinality in cat_cardinalities.items()\n",
    "        })\n",
    "\n",
    "        # Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "        total_cat_dim = embedding_dim * self.num_cat\n",
    "        input_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, num_cat + num_cont_features]\n",
    "        columnas en el orden:\n",
    "        [CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Separar categóricas y numéricas del gran vector\n",
    "        x_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "        x_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "        # 2) Embeddings categóricas\n",
    "        emb_list = []\n",
    "        for i, feat in enumerate(self.cat_features):\n",
    "            emb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "            emb_list.append(emb)\n",
    "\n",
    "        if emb_list:\n",
    "            x_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "            x_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "        else:\n",
    "            x_input = x_cont\n",
    "\n",
    "        out = self.mlp(x_input)   # [batch, 1]\n",
    "        return out.squeeze(-1)   \n",
    "\n",
    "\n",
    "def main():\n",
    "    AMP = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "        AMP = False\n",
    "    \n",
    "    train_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "    val_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "    \n",
    "    ddf_train = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=train_filters\n",
    "    )\n",
    "\n",
    "    ddf_val = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=val_filters\n",
    "    )\n",
    "\n",
    "    val_pdf = ddf_val.compute()\n",
    "    val_pdf = process_partition(val_pdf)\n",
    "\n",
    "    val_features_tensor = torch.tensor(\n",
    "        val_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    val_target_tensor = torch.tensor(\n",
    "        val_pdf[TARGET].values,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "    test_loader = DataLoader(dataset=val_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             shuffle=False)\n",
    "\n",
    "    input_dim = len(NUMERICAL_FEATURES) + len(CATEGORICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numerical + categorical + numerical missing indicators\n",
    "    \n",
    "    cat_cardinalities = {\n",
    "        feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "        for feat in CATEGORICAL_FEATURES\n",
    "    }\n",
    "    model = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=AMP)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        partition_iterator = ddf_train.to_delayed()\n",
    "        \n",
    "        for partition_idx, partition in enumerate(partition_iterator):\n",
    "            train_loss_sum = 0.0\n",
    "\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {partition_idx+1}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            pdf = process_partition(pdf)\n",
    "            \n",
    "            # 2. Convert Pandas DF to PyTorch Tensors\n",
    "            features_tensor = torch.tensor(\n",
    "                pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            target_tensor = torch.tensor(\n",
    "                pdf[TARGET].values,\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            partition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "            partition_loader = DataLoader(dataset=partition_dataset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=True)\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(partition_loader):\n",
    "\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                if AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss_sum += loss.item()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {partition_idx+1} | Partition Train Loss: {train_loss_sum / (len(partition_loader)):.4f}\")\n",
    "\n",
    "        val_loss_sum = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                val_loss_sum += loss.item()\n",
    "        print(f\"\\n--- Epoch {epoch + 1} Completed ---\")\n",
    "        print(f\"Validation Loss: {val_loss_sum / (len(test_loader)):.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_PATH}_{epoch+1}\")\n",
    "        print(f\"Model checkpoint saved to {CHECKPOINT_PATH}_{epoch+1}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Starting Epoch 1/5 ---\n",
      "  Epoch 1 | Processed Partition 1 | Partition Train Loss: 0.1681\n",
      "  Epoch 1 | Processed Partition 2 | Partition Train Loss: 0.0374\n",
      "  Epoch 1 | Processed Partition 3 | Partition Train Loss: 0.0297\n",
      "  Epoch 1 | Processed Partition 4 | Partition Train Loss: 0.0237\n",
      "  Epoch 1 | Processed Partition 5 | Partition Train Loss: 0.0238\n",
      "  Epoch 1 | Processed Partition 6 | Partition Train Loss: 0.0190\n",
      "  Epoch 1 | Processed Partition 7 | Partition Train Loss: 0.0201\n",
      "  Epoch 1 | Processed Partition 8 | Partition Train Loss: 0.0160\n",
      "  Epoch 1 | Processed Partition 9 | Partition Train Loss: 0.0164\n",
      "  Epoch 1 | Processed Partition 10 | Partition Train Loss: 0.0153\n",
      "  Epoch 1 | Processed Partition 11 | Partition Train Loss: 0.0170\n",
      "  Epoch 1 | Processed Partition 12 | Partition Train Loss: 0.0190\n",
      "  Epoch 1 | Processed Partition 13 | Partition Train Loss: 0.0145\n",
      "  Epoch 1 | Processed Partition 14 | Partition Train Loss: 0.0159\n",
      "  Epoch 1 | Processed Partition 15 | Partition Train Loss: 0.0160\n",
      "  Epoch 1 | Processed Partition 16 | Partition Train Loss: 0.0143\n",
      "  Epoch 1 | Processed Partition 17 | Partition Train Loss: 0.0143\n",
      "  Epoch 1 | Processed Partition 18 | Partition Train Loss: 0.0139\n",
      "  Epoch 1 | Processed Partition 19 | Partition Train Loss: 0.0134\n",
      "  Epoch 1 | Processed Partition 20 | Partition Train Loss: 0.0133\n",
      "  Epoch 1 | Processed Partition 21 | Partition Train Loss: 0.0137\n",
      "  Epoch 1 | Processed Partition 22 | Partition Train Loss: 0.0135\n",
      "  Epoch 1 | Processed Partition 23 | Partition Train Loss: 0.0134\n",
      "  Epoch 1 | Processed Partition 24 | Partition Train Loss: 0.0147\n",
      "  Epoch 1 | Processed Partition 25 | Partition Train Loss: 0.0173\n",
      "  Epoch 1 | Processed Partition 26 | Partition Train Loss: 0.0147\n",
      "  Epoch 1 | Processed Partition 27 | Partition Train Loss: 0.0154\n",
      "  Epoch 1 | Processed Partition 28 | Partition Train Loss: 0.0134\n",
      "  Epoch 1 | Processed Partition 29 | Partition Train Loss: 0.0217\n",
      "  Epoch 1 | Processed Partition 30 | Partition Train Loss: 0.0116\n",
      "  Epoch 1 | Processed Partition 31 | Partition Train Loss: 0.0103\n",
      "  Epoch 1 | Processed Partition 32 | Partition Train Loss: 0.0111\n",
      "  Epoch 1 | Processed Partition 33 | Partition Train Loss: 0.0095\n",
      "  Epoch 1 | Processed Partition 34 | Partition Train Loss: 0.0106\n",
      "  Epoch 1 | Processed Partition 35 | Partition Train Loss: 0.0087\n",
      "  Epoch 1 | Processed Partition 36 | Partition Train Loss: 0.0089\n",
      "  Epoch 1 | Processed Partition 37 | Partition Train Loss: 0.0085\n",
      "  Epoch 1 | Processed Partition 38 | Partition Train Loss: 0.0086\n",
      "  Epoch 1 | Processed Partition 39 | Partition Train Loss: 0.0092\n",
      "  Epoch 1 | Processed Partition 40 | Partition Train Loss: 0.0099\n",
      "  Epoch 1 | Processed Partition 41 | Partition Train Loss: 0.0098\n",
      "  Epoch 1 | Processed Partition 42 | Partition Train Loss: 0.0092\n",
      "  Epoch 1 | Processed Partition 43 | Partition Train Loss: 0.0109\n",
      "  Epoch 1 | Processed Partition 44 | Partition Train Loss: 0.0105\n",
      "  Epoch 1 | Processed Partition 45 | Partition Train Loss: 0.0100\n",
      "  Epoch 1 | Processed Partition 46 | Partition Train Loss: 0.0092\n",
      "  Epoch 1 | Processed Partition 47 | Partition Train Loss: 0.0095\n",
      "  Epoch 1 | Processed Partition 48 | Partition Train Loss: 0.0107\n",
      "  Epoch 1 | Processed Partition 49 | Partition Train Loss: 0.0113\n",
      "  Epoch 1 | Processed Partition 50 | Partition Train Loss: 0.0104\n",
      "  Epoch 1 | Processed Partition 51 | Partition Train Loss: 0.0104\n",
      "  Epoch 1 | Processed Partition 52 | Partition Train Loss: 0.0093\n",
      "  Epoch 1 | Processed Partition 53 | Partition Train Loss: 0.0074\n",
      "  Epoch 1 | Processed Partition 54 | Partition Train Loss: 0.0068\n",
      "  Epoch 1 | Processed Partition 55 | Partition Train Loss: 0.0083\n",
      "  Epoch 1 | Processed Partition 56 | Partition Train Loss: 0.0082\n",
      "  Epoch 1 | Processed Partition 57 | Partition Train Loss: 0.0066\n",
      "  Epoch 1 | Processed Partition 58 | Partition Train Loss: 0.0076\n",
      "  Epoch 1 | Processed Partition 59 | Partition Train Loss: 0.0070\n",
      "  Epoch 1 | Processed Partition 60 | Partition Train Loss: 0.0072\n",
      "  Epoch 1 | Processed Partition 61 | Partition Train Loss: 0.0069\n",
      "  Epoch 1 | Processed Partition 62 | Partition Train Loss: 0.0065\n",
      "  Epoch 1 | Processed Partition 63 | Partition Train Loss: 0.0070\n",
      "  Epoch 1 | Processed Partition 64 | Partition Train Loss: 0.0069\n",
      "  Epoch 1 | Processed Partition 65 | Partition Train Loss: 0.0065\n",
      "  Epoch 1 | Processed Partition 66 | Partition Train Loss: 0.0069\n",
      "  Epoch 1 | Processed Partition 67 | Partition Train Loss: 0.0064\n",
      "  Epoch 1 | Processed Partition 68 | Partition Train Loss: 0.0074\n",
      "  Epoch 1 | Processed Partition 69 | Partition Train Loss: 0.0108\n",
      "  Epoch 1 | Processed Partition 70 | Partition Train Loss: 0.0079\n",
      "  Epoch 1 | Processed Partition 71 | Partition Train Loss: 0.0098\n",
      "  Epoch 1 | Processed Partition 72 | Partition Train Loss: 0.0079\n",
      "  Epoch 1 | Processed Partition 73 | Partition Train Loss: 0.0095\n",
      "  Epoch 1 | Processed Partition 74 | Partition Train Loss: 0.0082\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hack (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
