{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n",
      "Contenido de ./data: ['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x73d446fe2490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Dict, List\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"../data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    return mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = ['buyer_d1', \n",
    "\t\t\t 'buyer_d7', \n",
    "\t\t\t 'buyer_d14', \n",
    "\t\t\t 'buyer_d28', \n",
    "\t\t\t 'buy_d7', \n",
    "\t\t\t 'buy_d14', \n",
    "\t\t\t 'buy_d28', \n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'iap_revenue_d14', \n",
    "\t\t\t 'iap_revenue_d28', \n",
    "\t\t\t 'registration', \n",
    "\t\t\t 'retention_d1_to_d7', \n",
    "\t\t\t 'retention_d3_to_d7', \n",
    "\t\t\t 'retention_d7_to_d14', \n",
    "\t\t\t 'retention_d1', \n",
    "\t\t\t 'retention_d3', \n",
    "\t\t\t 'retentiond7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    df = transform_variables(df)\n",
    "    df = scale_numerical_features(df, loaded_scaler)\n",
    "    df = impute_missings(df)\n",
    "    return df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds → hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "     \n",
    "\tnow = pd.Timestamp.now()\n",
    "\n",
    "\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\tmin_ts = 0                  # 1970-01-01\n",
    "\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t# Replace invalid timestamps with NaN\n",
    "\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\n",
    "\t# Convert safe Unix timestamps to datetime\n",
    "\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t# Compute hours ago\n",
    "\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t# Drop the original Unix timestamp columns\n",
    "\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Convert 'hour' from string to integer\n",
    "\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t# Convert hour to radians (full circle = 24 hours)\n",
    "\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t# Create two new columns: sin_hour and cos_hour\n",
    "\tdf['hour_sin'] = np.sin(radians)\n",
    "\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t# Drop the original 'hour' column\n",
    "\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Ensure `now_ts` is a Unix timestamp\n",
    "\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t# Apply to your 4 timestamp columns\n",
    "\tts_cols = [\n",
    "\t\t\"first_request_ts_bundle\",\n",
    "\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\"last_buy_ts_category\",\n",
    "\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t]\n",
    "\n",
    "\tfor col in ts_cols:\n",
    "\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t# Drop the original tuple columns if you want\n",
    "\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trules = {\n",
    "\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\"ctr\": \"sum\",\n",
    "\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t}\n",
    "\n",
    "\tfor col, mode in rules.items():\n",
    "\t\tnew_col = col + \"_agg\"\n",
    "\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t)\n",
    "\t\tprint(f\"Transformed {col} into {new_col} using {mode} aggregation.\")\n",
    "\n",
    "\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t# If the df contains the target column, transform it with the logarithm\n",
    "\tif TARGET in df.columns:\n",
    "\t\tdf[TARGET] = np.log1p(df[TARGET])\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "    return df\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "# AMP = False\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        cat_cardinalities: Dict[str, int],\n",
    "        cat_features: List[str],\n",
    "        embedding_dim: int = 16,\n",
    "        hidden_dims: List[int] = [128, 64],\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        cat_cardinalities: dict {feature_name: vocab_size}\n",
    "        cat_features: lista de nombres de features categóricas en el MISMO orden que en x_cat\n",
    "        num_numerical: número de features numéricas\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        # Embeddings por feature categórica\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "            for feat, cardinality in cat_cardinalities.items()\n",
    "        })\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        # Capa final de regresión (target escalar)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor, x_cat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_num: [batch, num_numerical]\n",
    "        x_cat: [batch, num_categorical] con ids de las categorías\n",
    "        \"\"\"\n",
    "        # Embeddings categóricas\n",
    "        emb_list = []\n",
    "        for i, feat in enumerate(self.cat_features):\n",
    "            # x_cat[:, i] es [batch], int64\n",
    "            emb = self.embeddings[feat](x_cat[:, i])   # [batch, embedding_dim]\n",
    "            emb_list.append(emb)\n",
    "\n",
    "        if emb_list:\n",
    "            x_cat_emb = torch.cat(emb_list, dim=1)     # [batch, embedding_dim * num_cat]\n",
    "            x = torch.cat([x_num, x_cat_emb], dim=1)   # [batch, total_dim]\n",
    "        else:\n",
    "            x = x_num\n",
    "\n",
    "        out = self.mlp(x)  # [batch, 1]\n",
    "        return out.squeeze(-1)  # [batch]\n",
    "\n",
    "\n",
    "def main():\n",
    "    AMP = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "        AMP = False\n",
    "    \n",
    "    train_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "    val_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "    \n",
    "    ddf_train = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=train_filters\n",
    "    )\n",
    "\n",
    "    ddf_val = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=val_filters\n",
    "    )\n",
    "\n",
    "    val_pdf = ddf_val.compute()\n",
    "    val_pdf = process_partition(val_pdf)\n",
    "\n",
    "    val_features_tensor = torch.tensor(\n",
    "        val_pdf[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    val_target_tensor = torch.tensor(\n",
    "        val_pdf[TARGET].values,\n",
    "        dtype=torch.float32\n",
    "    ).view(-1, 1)\n",
    "\n",
    "    val_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "    test_loader = DataLoader(dataset=val_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=4,\n",
    "                             pin_memory=True,\n",
    "                             shuffle=False)\n",
    "\n",
    "    input_dim = len(NUMERICAL_FEATURES) + len(CATEGORICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numerical + categorical + numerical missing indicators\n",
    "    \n",
    "    cat_cardinalities = {\n",
    "        feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "        for feat in CATEGORICAL_FEATURES\n",
    "    }\n",
    "    model = EmbeddingMLP(input_dim=input_dim, cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=AMP)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        train_loss_sum = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        partition_iterator = ddf_train.to_delayed()\n",
    "        \n",
    "        for i, partition in enumerate(partition_iterator):\n",
    "\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {i+1}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            pdf = process_partition(pdf)\n",
    "            \n",
    "            # 2. Convert Pandas DF to PyTorch Tensors\n",
    "            features_tensor = torch.tensor(\n",
    "                pdf[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            target_tensor = torch.tensor(\n",
    "                pdf[TARGET].values,\n",
    "                dtype=torch.float32\n",
    "            ).view(-1, 1)\n",
    "            \n",
    "            partition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "            partition_loader = DataLoader(dataset=partition_dataset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          num_workers=6,\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=True)\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(partition_loader):\n",
    "\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                if AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss_sum += loss.item()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {i+1} | Loss: {train_loss_sum / (batch_idx + 1):.4f}\")\n",
    "\n",
    "        val_loss_sum = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                val_loss_sum += loss.item()\n",
    "        print(f\"\\n--- Epoch {epoch + 1} Completed ---\")\n",
    "        print(f\"Validation Loss: {val_loss_sum / (batch_idx + 1):.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b65108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Transformed iap_revenue_usd_bundle into iap_revenue_usd_bundle_agg using sum aggregation.\n",
      "Transformed num_buys_bundle into num_buys_bundle_agg using sum aggregation.\n",
      "Transformed rev_by_adv into rev_by_adv_agg using sum aggregation.\n",
      "Transformed rwd_prank into rwd_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_num_buys_prank into whale_users_bundle_num_buys_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_revenue_prank into whale_users_bundle_revenue_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_total_num_buys into whale_users_bundle_total_num_buys_agg using sum aggregation.\n",
      "Transformed whale_users_bundle_total_revenue into whale_users_bundle_total_revenue_agg using sum aggregation.\n",
      "Transformed avg_daily_sessions into avg_daily_sessions_agg using mean aggregation.\n",
      "Transformed avg_duration into avg_duration_agg using mean aggregation.\n",
      "Transformed bcat_bottom_taxonomy into bcat_bottom_taxonomy_agg using mean aggregation.\n",
      "Transformed ctr into ctr_agg using sum aggregation.\n",
      "Transformed bundles_cat_bottom_taxonomy into bundles_cat_bottom_taxonomy_agg using sum aggregation.\n",
      "\n",
      "--- Starting Epoch 1/5 ---\n",
      "Transformed iap_revenue_usd_bundle into iap_revenue_usd_bundle_agg using sum aggregation.\n",
      "Transformed num_buys_bundle into num_buys_bundle_agg using sum aggregation.\n",
      "Transformed rev_by_adv into rev_by_adv_agg using sum aggregation.\n",
      "Transformed rwd_prank into rwd_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_num_buys_prank into whale_users_bundle_num_buys_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_revenue_prank into whale_users_bundle_revenue_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_total_num_buys into whale_users_bundle_total_num_buys_agg using sum aggregation.\n",
      "Transformed whale_users_bundle_total_revenue into whale_users_bundle_total_revenue_agg using sum aggregation.\n",
      "Transformed avg_daily_sessions into avg_daily_sessions_agg using mean aggregation.\n",
      "Transformed avg_duration into avg_duration_agg using mean aggregation.\n",
      "Transformed bcat_bottom_taxonomy into bcat_bottom_taxonomy_agg using mean aggregation.\n",
      "Transformed ctr into ctr_agg using sum aggregation.\n",
      "Transformed bundles_cat_bottom_taxonomy into bundles_cat_bottom_taxonomy_agg using sum aggregation.\n",
      "torch.Size([1024, 96])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmbeddingMLP.forward() missing 1 required positional argument: 'x_cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    162\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, enabled=AMP):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     loss = criterion(output, target)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AMP:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/datathon2025-smadex/pablo/venv-hack/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: EmbeddingMLP.forward() missing 1 required positional argument: 'x_cat'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hack (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
