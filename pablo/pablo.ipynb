{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n",
      "Contenido de ./data: ['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"../data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'advertiser_bundle',\n",
    "    'advertiser_category',\n",
    "    'advertiser_subcategory',\n",
    "    'advertiser_bottom_taxonomy_level',\n",
    "    'country',\n",
    "    'dev_make',\n",
    "    'dev_model',\n",
    "    'dev_os',\n",
    "    'dev_osv',\n",
    "    'release_date',\n",
    "    'avg_daily_sessions',\n",
    "    'avg_duration',\n",
    "    'bcat_bottom_taxonomy',\n",
    "    'ctr',\n",
    "\t]\n",
    "NUMERICAL_FEATURES = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'hour', 'release_msrp', 'weekday', 'avg_act_days', 'bundles_cat', 'bundles_cat_bottom_taxonomy', 'first_request_ts_bundle', 'iap_revenue_usd_bundle', 'last_buy', 'last_buy_ts_bundle', 'last_buy_ts_category', 'last_ins', 'user_actions_bundles_action_last_timestamp', 'num_buys_bundle', 'rev_by_adv', 'rwd_prank', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank', 'whale_users_bundle_total_num_buys', 'whale_users_bundle_total_revenue', 'last_buy_safe', 'last_ins_safe', 'last_buy_dt', 'last_ins_dt', 'hours_since_last_buy', 'hours_since_last_ins']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    return mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping(\"embeddings_mappings.json\")\n",
    "COLS_TO_READ = ['buyer_d1', \n",
    "             'buyer_d7', \n",
    "             'buyer_d14', \n",
    "             'buyer_d28', \n",
    "             'buy_d7', \n",
    "             'buy_d14', \n",
    "             'buy_d28', \n",
    "             'iap_revenue_d7', \n",
    "             'iap_revenue_d14', \n",
    "             'iap_revenue_d28', \n",
    "             'registration', \n",
    "             'retention_d1_to_d7', \n",
    "             'retention_d3_to_d7', \n",
    "             'retention_d7_to_d14', \n",
    "             'retention_d1', \n",
    "             'retention_d3', \n",
    "             'retentiond7', \n",
    "             'advertiser_bundle', \n",
    "             'advertiser_category', \n",
    "             'advertiser_subcategory', \n",
    "             'advertiser_bottom_taxonomy_level', \n",
    "             'country', \n",
    "             'dev_make', \n",
    "             'dev_model', \n",
    "             'dev_os', \n",
    "             'dev_osv', \n",
    "             'hour', \n",
    "             'release_date', \n",
    "             'release_msrp', \n",
    "             'weekday', \n",
    "             'avg_act_days', \n",
    "             'avg_daily_sessions', \n",
    "             'avg_duration', \n",
    "             'bcat_bottom_taxonomy', \n",
    "             'bundles_cat', \n",
    "             'bundles_cat_bottom_taxonomy',  \n",
    "             'ctr',  \n",
    "             'first_request_ts_bundle', \n",
    "             'iap_revenue_usd_bundle', \n",
    "             'last_buy', \n",
    "             'last_buy_ts_bundle', \n",
    "             'last_buy_ts_category', \n",
    "             'last_ins', \n",
    "             'user_actions_bundles_action_last_timestamp', \n",
    "             'num_buys_bundle', \n",
    "             'rev_by_adv', \n",
    "             'rwd_prank', \n",
    "             'weekend_ratio', \n",
    "             'weeks_since_first_seen', \n",
    "             'wifi_ratio', \n",
    "             'whale_users_bundle_num_buys_prank', \n",
    "             'whale_users_bundle_revenue_prank', \n",
    "             'whale_users_bundle_total_num_buys', \n",
    "             'whale_users_bundle_total_revenue']\n",
    "\n",
    "train_filters = [(\"datetime\", \">=\", \"2025-10-01-00-00\"), (\"datetime\", \"<\", \"2025-10-06-00-00\")]\n",
    "\n",
    "ddf_train = dd.read_parquet(\n",
    "    DATASET_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=COLS_TO_READ,\n",
    "    filters=train_filters\n",
    ")\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    df = transform_variables(df)\n",
    "    df = scale_numerical_features(df, loaded_scaler)\n",
    "    df = impute_missings(df)\n",
    "    return df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds â†’ hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "    now = pd.Timestamp.now()\n",
    "\n",
    "    # Define reasonable bounds for Unix timestamps (seconds)\n",
    "    min_ts = 0                  # 1970-01-01\n",
    "    max_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "    # Replace invalid timestamps with NaN\n",
    "    df[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "        df[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "    )\n",
    "    df[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "        df[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "    )\n",
    "\n",
    "    # Convert safe Unix timestamps to datetime\n",
    "    df[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "    df[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "    # Compute hours ago\n",
    "    df[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "    df[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "    # Drop the original Unix timestamp columns\n",
    "    df = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert 'hour' from string to integer\n",
    "    df['hour'] = df['hour'].astype(int)\n",
    "\n",
    "    # Convert hour to radians (full circle = 24 hours)\n",
    "    radians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "    # Create two new columns: sin_hour and cos_hour\n",
    "    df['hour_sin'] = np.sin(radians)\n",
    "    df['hour_cos'] = np.cos(radians)\n",
    "\n",
    "    # Drop the original 'hour' column\n",
    "    df.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure `now_ts` is a Unix timestamp\n",
    "    now_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "    # Apply to your 4 timestamp columns\n",
    "    ts_cols = [\n",
    "        \"first_request_ts_bundle\",\n",
    "        \"last_buy_ts_bundle\",\n",
    "        \"last_buy_ts_category\",\n",
    "        \"user_actions_bundles_action_last_timestamp\"\n",
    "    ]\n",
    "\n",
    "    for col in ts_cols:\n",
    "        new_col = col + \"_hours_ago\"\n",
    "        df[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "    # Drop the original tuple columns if you want\n",
    "    df.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    rules = {\n",
    "        \"iap_revenue_usd_bundle\": \"sum\",\n",
    "        \"num_buys_bundle\": \"sum\",\n",
    "        \"rev_by_adv\": \"sum\",\n",
    "        \"rwd_prank\": \"mean\",\n",
    "        \"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "        \"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "        \"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "        \"whale_users_bundle_total_revenue\": \"sum\",\n",
    "        \"avg_daily_sessions\": \"mean\",\n",
    "        \"avg_duration\": \"mean\",\n",
    "        \"bcat_bottom_taxonomy\": \"mean\",\n",
    "        \"ctr\": \"sum\",\n",
    "    }\n",
    "\n",
    "    for col, mode in rules.items():\n",
    "        new_col = col + \"_agg\"\n",
    "        df[new_col] = df[col].apply(\n",
    "            lambda lst: aggregate(extract_numbers(lst), mode)\n",
    "        )\n",
    "\n",
    "    df.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "    return df\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_train = ddf_train.compute()\n",
    "print(\"Training data loaded.\")\n",
    "\n",
    "transform_variables(pdf_train)\n",
    "generate_embeddings_mapping(pdf_train)\n",
    "train_scaler(pdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, 64)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, 1) # Output 1 value (target)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_1(self.layer_1(x))\n",
    "        x = self.relu_2(self.layer_2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    ddf_train = dd.read_parquet(\n",
    "        DATASET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=COLS_TO_READ,\n",
    "        filters=train_filters\n",
    "    )\n",
    "\n",
    "    input_dim = len(NUMERICAL_FEATURES) + len(CATEGORICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numerical + categorical + numerical missing indicators\n",
    "    model = SimpleMLP(input_dim=input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        partition_iterator = ddf_train.to_delayed()\n",
    "        \n",
    "        for i, partition in enumerate(partition_iterator):\n",
    "\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {i+1}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            pdf = process_partition(pdf)\n",
    "            \n",
    "            # 2. Convert Pandas DF to PyTorch Tensors\n",
    "            features_tensor = torch.tensor(\n",
    "                pdf[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            target_tensor = torch.tensor(\n",
    "                pdf[TARGET].values,\n",
    "                dtype=torch.float32\n",
    "            ).view(-1, 1)\n",
    "            \n",
    "            partition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "            partition_loader = DataLoader(dataset=partition_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            for batch_num, (batch_features, batch_target) in enumerate(partition_loader):\n",
    "                # Forward pass\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_target)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {i+1} | Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hack (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
