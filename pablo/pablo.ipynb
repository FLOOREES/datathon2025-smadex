{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n",
      "Contenido de ./data: ['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"../data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'advertiser_bundle',\n",
    "    'advertiser_category',\n",
    "    'advertiser_subcategory',\n",
    "    'advertiser_bottom_taxonomy_level',\n",
    "    'country',\n",
    "    'dev_make',\n",
    "    'dev_model',\n",
    "    'dev_os',\n",
    "    'dev_osv',\n",
    "    'release_date',\n",
    "    'avg_daily_sessions',\n",
    "    'avg_duration',\n",
    "    'bcat',\n",
    "    'bcat_bottom_taxonomy',\n",
    "    'bundles_ins',\n",
    "    'city_hist',\n",
    "    'country_hist',\n",
    "    'ctr',\n",
    "    'dev_language_hist',\n",
    "    'dev_osv_hist',\n",
    "    'hour_ratio',\n",
    "    'user_actions_bundles_action_count',\n",
    "    'new_bundles',\n",
    "    'region_hist',\n",
    "    'user_bundles',\n",
    "    'user_bundles_l28d'\n",
    "]\n",
    "NUMERICAL_FEATURES = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'hour', 'release_msrp', 'weekday', 'avg_act_days', 'bundles_cat', 'bundles_cat_bottom_taxonomy', 'first_request_ts_bundle', 'iap_revenue_usd_bundle', 'last_buy', 'last_buy_ts_bundle', 'last_buy_ts_category', 'last_ins', 'user_actions_bundles_action_last_timestamp', 'num_buys_bundle', 'rev_by_adv', 'rwd_prank', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank', 'whale_users_bundle_total_num_buys', 'whale_users_bundle_total_revenue', 'last_buy_safe', 'last_ins_safe', 'last_buy_dt', 'last_ins_dt', 'hours_since_last_buy', 'hours_since_last_ins']\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    return mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping(\"embeddings_mappings.json\")\n",
    "COLS_TO_READ = ['buyer_d1', \n",
    "             'buyer_d7', \n",
    "             'buyer_d14', \n",
    "             'buyer_d28', \n",
    "             'buy_d7', \n",
    "             'buy_d14', \n",
    "             'buy_d28', \n",
    "             'iap_revenue_d7', \n",
    "             'iap_revenue_d14', \n",
    "             'iap_revenue_d28', \n",
    "             'registration', \n",
    "             'retention_d1_to_d7', \n",
    "             'retention_d3_to_d7', \n",
    "             'retention_d7_to_d14', \n",
    "             'retention_d1', \n",
    "             'retention_d3', \n",
    "             'retentiond7', \n",
    "             'advertiser_bundle', \n",
    "             'advertiser_category', \n",
    "             'advertiser_subcategory', \n",
    "             'advertiser_bottom_taxonomy_level', \n",
    "             'country', \n",
    "             'dev_make', \n",
    "             'dev_model', \n",
    "             'dev_os', \n",
    "             'dev_osv', \n",
    "             'hour', \n",
    "             'release_date', \n",
    "             'release_msrp', \n",
    "             'weekday', \n",
    "             'avg_act_days', \n",
    "             'avg_daily_sessions', \n",
    "             'avg_duration', \n",
    "             'bcat_bottom_taxonomy', \n",
    "             'bundles_cat', \n",
    "             'bundles_cat_bottom_taxonomy',  \n",
    "             'ctr',  \n",
    "             'first_request_ts_bundle', \n",
    "             'iap_revenue_usd_bundle', \n",
    "             'last_buy', \n",
    "             'last_buy_ts_bundle', \n",
    "             'last_buy_ts_category', \n",
    "             'last_ins', \n",
    "             'user_actions_bundles_action_last_timestamp', \n",
    "             'num_buys_bundle', \n",
    "             'rev_by_adv', \n",
    "             'rwd_prank', \n",
    "             'weekend_ratio', \n",
    "             'weeks_since_first_seen', \n",
    "             'wifi_ratio', \n",
    "             'whale_users_bundle_num_buys_prank', \n",
    "             'whale_users_bundle_revenue_prank', \n",
    "             'whale_users_bundle_total_num_buys', \n",
    "             'whale_users_bundle_total_revenue']\n",
    "\n",
    "train_filters = [(\"datetime\", \">=\", \"2025-10-01-00-00\"), (\"datetime\", \"<\", \"2025-10-06-00-00\")]\n",
    "\n",
    "ddf_train = dd.read_parquet(\n",
    "    DATASET_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=COLS_TO_READ,\n",
    "    filters=train_filters\n",
    ")\n",
    "\n",
    "# loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_partition(df):\n",
    "    transform_variables(df)\n",
    "    scale_numerical_features(df, loaded_scaler)\n",
    "    impute_missings(df)\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds â†’ hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "    now = pd.Timestamp.now()\n",
    "\n",
    "    # Define reasonable bounds for Unix timestamps (seconds)\n",
    "    min_ts = 0                  # 1970-01-01\n",
    "    max_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "    # Replace invalid timestamps with NaN\n",
    "    df[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "        df[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "    )\n",
    "    df[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "        df[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "    )\n",
    "\n",
    "    # Convert safe Unix timestamps to datetime\n",
    "    df[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "    df[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "    # Compute hours ago\n",
    "    df[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "    df[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "    # Drop the original Unix timestamp columns\n",
    "    df = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert 'hour' from string to integer\n",
    "    df['hour'] = df['hour'].astype(int)\n",
    "\n",
    "    # Convert hour to radians (full circle = 24 hours)\n",
    "    radians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "    # Create two new columns: sin_hour and cos_hour\n",
    "    df['hour_sin'] = np.sin(radians)\n",
    "    df['hour_cos'] = np.cos(radians)\n",
    "\n",
    "    # Drop the original 'hour' column\n",
    "    df.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure `now_ts` is a Unix timestamp\n",
    "    now_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "    # Apply to your 4 timestamp columns\n",
    "    ts_cols = [\n",
    "        \"first_request_ts_bundle\",\n",
    "        \"last_buy_ts_bundle\",\n",
    "        \"last_buy_ts_category\",\n",
    "        \"user_actions_bundles_action_last_timestamp\"\n",
    "    ]\n",
    "\n",
    "    for col in ts_cols:\n",
    "        new_col = col + \"_hours_ago\"\n",
    "        df[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "    # Drop the original tuple columns if you want\n",
    "    df.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    rules = {\n",
    "        \"iap_revenue_usd_bundle\": \"sum\",\n",
    "        \"num_buys_bundle\": \"sum\",\n",
    "        \"rev_by_adv\": \"sum\",\n",
    "        \"rwd_prank\": \"mean\",\n",
    "        \"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "        \"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "        \"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "        \"whale_users_bundle_total_revenue\": \"sum\",\n",
    "        \"avg_daily_sessions\": \"mean\",\n",
    "        \"avg_duration\": \"mean\",\n",
    "        \"bcat_bottom_taxonomy\": \"mean\",\n",
    "        \"ctr\": \"sum\",\n",
    "    }\n",
    "\n",
    "    for col, mode in rules.items():\n",
    "        new_col = col + \"_agg\"\n",
    "        df[new_col] = df[col].apply(\n",
    "            lambda lst: aggregate(extract_numbers(lst), mode)\n",
    "        )\n",
    "\n",
    "    df.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_train = ddf_train.compute()\n",
    "print(\"Training data loaded.\")\n",
    "\n",
    "transform_variables(pdf_train)\n",
    "generate_embeddings_mapping(pdf_train)\n",
    "train_scaler(pdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f229934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Data:\n",
      "    age   income      city\n",
      "0  20.0  50000.0  New York\n",
      "1  30.0  60000.0    London\n",
      "2  40.0      NaN     Paris\n",
      "3  50.0  80000.0     Tokyo\n",
      "4   NaN  75000.0    Sydney\n",
      "------------------------------\n",
      "Fitting scaler on columns: ['age', 'income']\n",
      "Scaler saved to my_scaler.joblib\n",
      "------------------------------\n",
      "New Data (Before Transform):\n",
      "    age  income    city\n",
      "0  35.0   55000  Berlin\n",
      "1  45.0   90000  Moscow\n",
      "2   NaN   62000  Lisbon\n",
      "Column names before: ['age', 'income', 'city']\n",
      "------------------------------\n",
      "Scaler loaded.\n",
      "\n",
      "New Data (After Transform):\n",
      "        age    income    city\n",
      "0  0.000000 -0.943456  Berlin\n",
      "1  0.894427  1.991741  Moscow\n",
      "2       NaN -0.356417  Lisbon\n",
      "Column names after:  ['age', 'income', 'city']\n",
      "\n",
      "Column names and order unchanged: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# --- 1. Define Data and Columns to Scale ---\n",
    "cols_to_scale = ['age', 'income']\n",
    "scaler_filename = 'my_scaler.joblib'\n",
    "\n",
    "# Create a training DataFrame\n",
    "df_train = pd.DataFrame({\n",
    "    'age': [20, 30, 40, 50, np.nan],\n",
    "    'income': [50000, 60000, np.nan, 80000, 75000],\n",
    "    'city': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney']\n",
    "})\n",
    "\n",
    "print(\"Original Training Data:\")\n",
    "print(df_train)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. Initialize, Fit, and Save Scaler ---\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the specified columns\n",
    "# .fit() ignores NaNs when calculating mean and std\n",
    "print(f\"Fitting scaler on columns: {cols_to_scale}\")\n",
    "scaler.fit(df_train[cols_to_scale])\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3. Load Scaler and Transform New Data ---\n",
    "\n",
    "# Create new \"test\" data to transform\n",
    "df_test = pd.DataFrame({\n",
    "    'age': [35, 45, np.nan],\n",
    "    'income': [55000, 90000, 62000],\n",
    "    'city': ['Berlin', 'Moscow', 'Lisbon']\n",
    "})\n",
    "\n",
    "print(\"New Data (Before Transform):\")\n",
    "print(df_test)\n",
    "print(f\"Column names before: {df_test.columns.tolist()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the scaler from the file\n",
    "loaded_scaler = joblib.load(scaler_filename)\n",
    "print(\"Scaler loaded.\")\n",
    "\n",
    "# Create a copy to avoid SettingWithCopyWarning\n",
    "# df_test_transformed = df_test.copy()\n",
    "\n",
    "# --- This is the key step ---\n",
    "# Transform only the specified columns and assign them back\n",
    "# to the *same columns* in the copied DataFrame.\n",
    "# .transform() scales non-NaNs and keeps NaNs as NaN.\n",
    "df_test[cols_to_scale] = loaded_scaler.transform(df_test[cols_to_scale])\n",
    "\n",
    "# --- 4. Verify Results ---\n",
    "\n",
    "print(\"\\nNew Data (After Transform):\")\n",
    "print(df_test)\n",
    "print(f\"Column names after:  {df_test.columns.tolist()}\")\n",
    "\n",
    "# Check that column names and order are identical\n",
    "are_names_same = df_test.columns.equals(df_test.columns)\n",
    "print(f\"\\nColumn names and order unchanged: {are_names_same}\")\n",
    "\n",
    "# Clean up the created file\n",
    "os.remove(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "TRAIN_DATA_PATH = 'data/smadex-challenge-predict-the-revenue/train/train/'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_COLUMN = 'target' \n",
    "\n",
    "# Suppress Dask warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dask\")\n",
    "\n",
    "# --- 1. Define the PyTorch Model ---\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) for regression.\n",
    "    Assumes all input data is numerical and pre-scaled (0-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, 64)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, 1) # Output 1 value (target)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_1(self.layer_1(x))\n",
    "        x = self.relu_2(self.layer_2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Main Training Function ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Minimal function to load data by partition and train the model on CUDA.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Setup Device (CUDA or CPU) ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load Dask DataFrame (Lazily) ---\n",
    "    print(f\"Loading data from: {TRAIN_DATA_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset lazily just to get metadata (column names)\n",
    "        temp_ddf = dd.read_parquet(TRAIN_DATA_PATH, engine='pyarrow')\n",
    "        all_columns = list(temp_ddf.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet metadata from {TRAIN_DATA_PATH}. {e}\")\n",
    "        print(\"Please check your path.\")\n",
    "        return\n",
    "\n",
    "    # --- Identify Feature and Target Columns ---\n",
    "    if TARGET_COLUMN not in all_columns:\n",
    "        print(f\"Error: Target column '{TARGET_COLUMN}' not found in data.\")\n",
    "        print(f\"Available columns: {all_columns}\")\n",
    "        return\n",
    "        \n",
    "    # Assume all other columns are features (as requested for a minimal example)\n",
    "    FEATURE_COLUMNS = [col for col in all_columns if col != TARGET_COLUMN]\n",
    "    \n",
    "    if not FEATURE_COLUMNS:\n",
    "        print(\"Error: No feature columns found. Check your data.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Identified Target: '{TARGET_COLUMN}'\")\n",
    "    print(f\"Identified {len(FEATURE_COLUMNS)} Features. First 5: {FEATURE_COLUMNS[:5]}...\")\n",
    "    \n",
    "    # Select only the columns we need. This is still lazy.\n",
    "    ddf = temp_ddf[FEATURE_COLUMNS + [TARGET_COLUMN]]\n",
    "\n",
    "    # --- Initialize Model, Loss, Optimizer ---\n",
    "    input_dim = len(FEATURE_COLUMNS)\n",
    "    model = SimpleMLP(input_dim=input_dim).to(device)\n",
    "    criterion = nn.MSELoss() # Mean Squared Error for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    model.train() # Set model to training mode\n",
    "\n",
    "    # --- Training Loop: Partition by Partition ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        # Get a list of delayed tasks, one for each partition file\n",
    "        partition_iterator = ddf.to_delayed()\n",
    "        \n",
    "        for i, partition in enumerate(partition_iterator):\n",
    "            # 1. Load *only this single partition* into memory as a Pandas DF\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {i+1}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            features_tensor = torch.tensor(\n",
    "                pdf[FEATURE_COLUMNS].values, \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            target_tensor = torch.tensor(\n",
    "                pdf[TARGET_COLUMN].values, \n",
    "                dtype=torch.float32\n",
    "            ).view(-1, 1)\n",
    "            \n",
    "            # 3. Create DataLoader for batching within the partition\n",
    "            partition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "            partition_loader = DataLoader(dataset=partition_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            # 4. Mini-Batch Training Loop (for this partition)\n",
    "            for batch_num, (batch_features, batch_target) in enumerate(partition_loader):\n",
    "                # Forward pass\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_target)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {i+1} | Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# --- 1. Create a dummy CSV file for this example ---\n",
    "# (In your real code, you would just have 'data.csv')\n",
    "csv_content = \"\"\"cat_feature,num_feature,target\n",
    "A,1.0,10\n",
    "B,,20\n",
    ",3.0,30\n",
    "A,4.0,40\n",
    "B,5.0,50\n",
    ",,60\n",
    "\"\"\"\n",
    "with open('data.csv', 'w') as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "# --- 2. Define the Custom Dataset ---\n",
    "# This is the \"right place\" to do the transformations.\n",
    "# We do them all at once in __init__ for efficiency.\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load dataframe, treating empty strings as NaN\n",
    "        df = pd.read_csv(csv_file, na_values=[\"\", \" \"])\n",
    "\n",
    "        # --- Categorical Feature Transformation ---\n",
    "        # 1. Impute missings with the new value \"<MISSING>\"\n",
    "        df['cat_feature'] = df['cat_feature'].fillna(\"<MISSING>\")\n",
    "        # 2. Factorize to get numerical codes (e.g., A=0, B=1, <MISSING>=2)\n",
    "        # We save 'self.categories' to know the mapping\n",
    "        self.cat_codes, self.categories = pd.factorize(df['cat_feature'])\n",
    "        self.cat_data = torch.tensor(self.cat_codes, dtype=torch.long)\n",
    "\n",
    "        # --- Numerical Feature Transformation ---\n",
    "        # 1. Create the 'is_missing' binary feature\n",
    "        is_missing_vals = torch.tensor(\n",
    "            df['num_feature'].isna().astype(float), dtype=torch.float32\n",
    "        )\n",
    "        # 2. Impute missings with 0\n",
    "        num_vals = torch.tensor(\n",
    "            df['num_feature'].fillna(0).values, dtype=torch.float32\n",
    "        )\n",
    "        # 3. Stack the numerical feature and its 'is_missing' indicator\n",
    "        # Shape will be (n_samples, 2)\n",
    "        self.num_combined_data = torch.stack([num_vals, is_missing_vals], dim=1)\n",
    "\n",
    "        # --- Target Variable ---\n",
    "        self.targets = torch.tensor(df['target'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the processed features and the target\n",
    "        # x_cat is a Long (for embedding lookup)\n",
    "        # x_num_combined is a Float (for a linear layer)\n",
    "        # y is a Float (the target)\n",
    "        return (self.cat_data[idx], self.num_combined_data[idx]), self.targets[idx]\n",
    "\n",
    "# --- 3. Define a Minimal Model to Consume the Data ---\n",
    "# This model shows how to use the data from our loader.\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim=4):\n",
    "        super().__init__()\n",
    "        # Embedding layer for the categorical feature\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "        # Linear layer for the combined features\n",
    "        # Input: embedding_dim + 2 (from num_feature + is_missing)\n",
    "        self.fc = nn.Linear(embedding_dim + 2, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num_combined):\n",
    "        # x_cat shape: (batch_size)\n",
    "        # x_num_combined shape: (batch_size, 2)\n",
    "        \n",
    "        cat_embed = self.embedding(x_cat) # shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([cat_embed, x_num_combined], dim=1)\n",
    "        \n",
    "        return self.fc(combined_features)\n",
    "\n",
    "# --- 4. Load Data and Demonstrate ---\n",
    "\n",
    "# Initialize Dataset\n",
    "dataset = ImputationDataset('data.csv')\n",
    "\n",
    "# Get number of unique categories (A, B, <MISSING>) for Embedding layer\n",
    "num_categories = len(dataset.categories)\n",
    "print(f\"Categories found: {list(dataset.categories)}\")\n",
    "print(f\"Number of embeddings needed: {num_categories}\\n\")\n",
    "\n",
    "# Initialize DataLoader\n",
    "# batch_size=3 to show how it handles the 6 samples\n",
    "data_loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Initialize Model\n",
    "model = SimpleModel(num_embeddings=num_categories)\n",
    "\n",
    "# --- 5. Run one batch to see the output ---\n",
    "print(\"--- Iterating over DataLoader ---\")\n",
    "for (x_cat, x_num_combined), y in data_loader:\n",
    "    print(f\"Batch X (categorical codes):\\n{x_cat}\")\n",
    "    print(f\"Batch X (numerical, is_missing):\\n{x_num_combined}\")\n",
    "    print(f\"Batch Y (target):\\n{y}\\n\")\n",
    "    \n",
    "    # Pass data to the model\n",
    "    output = model(x_cat, x_num_combined)\n",
    "    \n",
    "    print(f\"Model Output shape: {output.shape}\")\n",
    "    print(f\"Model Output:\\n{output}\\n\")\n",
    "    \n",
    "    # We only run one batch for demonstration\n",
    "    break\n",
    "\n",
    "# Example of what the data looks like before the model:\n",
    "# (Indices will vary due to shuffle=True)\n",
    "#\n",
    "# Original Data:\n",
    "# cat_feature,num_feature,target\n",
    "# A,1.0,10\n",
    "# B,,20\n",
    "# ,3.0,30\n",
    "# A,4.0,40\n",
    "# B,5.0,50\n",
    "# ,,60\n",
    "#\n",
    "# Processed Data (example):\n",
    "# x_cat: [1] (B)\n",
    "# x_num_combined: [0.0, 1.0] (value=0, is_missing=1)\n",
    "# y: [20.]\n",
    "#\n",
    "# x_cat: [2] (<MISSING>)\n",
    "# x_num_combined: [3.0, 0.0] (value=3, is_missing=0)\n",
    "# y: [30.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1df5e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask_ml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_ml.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# --- 1. Configuration & Column Definitions ---\n",
    "\n",
    "# --- Paths & Training ---\n",
    "TRAIN_DATA_PATH = 'data/smadex-challenge-predict-the-revenue/train/train/'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# --- Target Column ---\n",
    "# Let's predict a regression target, e.g., 'iap_revenue_d7'\n",
    "TARGET_COLUMN = 'iap_revenue_d7' \n",
    "\n",
    "# --- Feature Column Subsets (Demonstrating Each Type) ---\n",
    "# We'll use these lists to apply different transformations.\n",
    "# You can expand these lists with all your features.\n",
    "\n",
    "# 1. Categorical: Will be integer-encoded and fed into Embedding layers\n",
    "CAT_COLS = [\n",
    "    'advertiser_category',\n",
    "    'country', \n",
    "    'dev_make', \n",
    "    'dev_os'\n",
    "]\n",
    "\n",
    "# 2. Numerical: Will be log-transformed (if skewed) and scaled\n",
    "# Note: 'release_msrp' is a raw feature. Others will be created by us.\n",
    "NUM_COLS_RAW = [\n",
    "    'release_msrp'\n",
    "]\n",
    "\n",
    "# 3. Cyclical: Will be transformed with sin/cos\n",
    "CYCLICAL_COLS = [\n",
    "    'hour', \n",
    "    'weekday'\n",
    "]\n",
    "\n",
    "# 4. Timestamp: Will be converted to \"time since\" (delta) features\n",
    "TIMESTAMP_COLS = [\n",
    "    'release_date', # Special case (Y-M format)\n",
    "    'last_buy',\n",
    "    'last_ins'\n",
    "]\n",
    "\n",
    "# 5. Map/Dict: Will be unrolled or aggregated\n",
    "MAP_COLS = {\n",
    "    'cpm': ['b', 'i', 'r'], # Unroll 'banner', 'interstitial', 'rewarded'\n",
    "    'iap_revenue_usd_bundle': 'sum' # Aggregate: get total revenue\n",
    "}\n",
    "\n",
    "# 6. List: Will be aggregated\n",
    "LIST_COLS = {\n",
    "    'user_bundles': 'count', # Get len() of the list\n",
    "    'new_bundles': 'count'\n",
    "}\n",
    "\n",
    "# 7. Histogram: Will be aggregated\n",
    "HIST_COLS = {\n",
    "    'city_hist': 'total_requests' # Sum all values in the hist\n",
    "}\n",
    "\n",
    "# --- Preprocessing Parameters ---\n",
    "RARE_THRESHOLD = 10 # Group categories with < 10 occurrences into '<RARE>'\n",
    "EMBEDDING_DIMS = {\n",
    "    'advertiser_category': 10,\n",
    "    'country': 16,\n",
    "    'dev_make': 24,\n",
    "    'dev_os': 8,\n",
    "}\n",
    "\n",
    "# --- Utility Columns (to be dropped before training) ---\n",
    "METADATA_COLS = ['row_id', 'datetime']\n",
    "\n",
    "# Suppress Dask warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dask\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- 2. The Deep Learning Model ---\n",
    "\n",
    "class WideAndDeepModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that handles both categorical (via Embeddings) and \n",
    "    numerical features.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_sizes, embedding_dims, num_numerical_features):\n",
    "        super(WideAndDeepModel, self).__init__()\n",
    "        \n",
    "        # --- Embedding Layers (Categorical) ---\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        total_embedding_dim = 0\n",
    "        for col, vocab_size in vocab_sizes.items():\n",
    "            dim = embedding_dims.get(col, 10) # Default to dim=10 if not specified\n",
    "            self.embedding_layers[col] = nn.Embedding(vocab_size, dim)\n",
    "            total_embedding_dim += dim\n",
    "            \n",
    "        print(f\"Total embedding dimension: {total_embedding_dim}\")\n",
    "        \n",
    "        # --- Deep Tower (Numerical + Embeddings) ---\n",
    "        self.num_numerical_features = num_numerical_features\n",
    "        input_dim = total_embedding_dim + self.num_numerical_features\n",
    "        \n",
    "        print(f\"Total model input dim (Embeds + Numericals): {input_dim}\")\n",
    "        \n",
    "        self.tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1) # Output 1 value (target)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        \"\"\"\n",
    "        x_cat: A dictionary of {feature_name: tensor}\n",
    "        x_num: A single tensor of numerical features\n",
    "        \"\"\"\n",
    "        # --- Process Embeddings ---\n",
    "        embeddings = []\n",
    "        for col, tensor in x_cat.items():\n",
    "            embeddings.append(self.embedding_layers[col](tensor))\n",
    "        \n",
    "        # Concatenate all embedding outputs\n",
    "        x_embed = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # --- Concatenate Numerical ---\n",
    "        x_combined = torch.cat([x_embed, x_num], dim=1)\n",
    "        \n",
    "        # --- Pass through Tower ---\n",
    "        return self.tower(x_combined)\n",
    "\n",
    "\n",
    "# --- 3. Custom PyTorch Dataset ---\n",
    "\n",
    "class HeterogeneousDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset to handle our two types of inputs:\n",
    "    1. A dictionary of categorical tensors\n",
    "    2. A single tensor of numerical features\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_data, num_data, target_data):\n",
    "        self.cat_data = {col: torch.tensor(data, dtype=torch.long) for col, data in cat_data.items()}\n",
    "        self.num_data = torch.tensor(num_data, dtype=torch.float32)\n",
    "        self.target_data = torch.tensor(target_data, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # All inputs must have the same length\n",
    "        return len(self.target_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get categorical features for this index\n",
    "        x_cat = {col: data[idx] for col, data in self.cat_data.items()}\n",
    "        \n",
    "        # Get numerical features for this index\n",
    "        x_num = self.num_data[idx]\n",
    "        \n",
    "        # Get target for this index\n",
    "        y = self.target_data[idx]\n",
    "        \n",
    "        return (x_cat, x_num), y\n",
    "\n",
    "\n",
    "# --- 4. Feature Engineering & Preprocessing Class ---\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"\n",
    "    Handles fitting vocabs/scalers on Dask and transforming\n",
    "    Pandas partitions during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_cols, num_cols_raw, cyclical_cols, ts_cols, map_cols, list_cols, hist_cols, target_col):\n",
    "        # Store column names\n",
    "        self.CAT_COLS = cat_cols\n",
    "        self.NUM_COLS_RAW = num_cols_raw\n",
    "        self.CYCLICAL_COLS = cyclical_cols\n",
    "        self.TS_COLS = ts_cols\n",
    "        self.MAP_COLS = map_cols\n",
    "        self.LIST_COLS = list_cols\n",
    "        self.HIST_COLS = hist_cols\n",
    "        self.TARGET_COL = target_col\n",
    "\n",
    "        # These will be \"fit\"\n",
    "        self.vocabularies = {}\n",
    "        self.vocab_sizes = {}\n",
    "        self.scaler = StandardScaler() # Use sklearn scaler, fit on 1st partition\n",
    "        self.engineered_num_cols = []\n",
    "        self._is_scaler_fit = False\n",
    "\n",
    "    \n",
    "    def fit_vocabs(self, ddf, rare_threshold):\n",
    "        \"\"\"\n",
    "        Fits vocabularies on the full Dask dataframe.\n",
    "        This is a global operation and should be done once.\n",
    "        \"\"\"\n",
    "        print(\"Computing vocabularies...\")\n",
    "        for col in self.CAT_COLS:\n",
    "            print(f\"  Fitting vocab for: {col}\")\n",
    "            # Get value counts, compute, and filter\n",
    "            counts = ddf[col].value_counts().compute()\n",
    "            frequent_labels = counts[counts >= rare_threshold].index.tolist()\n",
    "            \n",
    "            # Create vocab with special tokens\n",
    "            vocab = defaultdict(lambda: 1) # 1 is '<RARE>'\n",
    "            vocab['<MISSING>'] = 0\n",
    "            vocab['<RARE>'] = 1\n",
    "            for i, label in enumerate(frequent_labels, 2):\n",
    "                vocab[label] = i\n",
    "                \n",
    "            self.vocabularies[col] = vocab\n",
    "            self.vocab_sizes[col] = len(vocab)\n",
    "            print(f\"    Vocab size for {col}: {len(vocab)}\")\n",
    "    \n",
    "    \n",
    "    def engineer_features_partition(self, pdf, ref_datetime_col):\n",
    "        \"\"\"\n",
    "        Applies all feature engineering logic to a single Pandas partition.\n",
    "        This function CREATES the new numerical features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a copy to avoid SettingWithCopyWarning\n",
    "        pdf_out = pd.DataFrame(index=pdf.index)\n",
    "        ref_datetime = pd.to_datetime(ref_datetime_col)\n",
    "\n",
    "        # --- 1. Pass-through Raw Features ---\n",
    "        pdf_out[self.CAT_COLS] = pdf[self.CAT_COLS].fillna('<MISSING>')\n",
    "        pdf_out[self.NUM_COLS_RAW] = pdf[self.NUM_COLS_RAW]\n",
    "        pdf_out[self.TARGET_COL] = pdf[self.TARGET_COL]\n",
    "\n",
    "        # --- 2. Cyclical Features ---\n",
    "        for col in self.CYCLICAL_COLS:\n",
    "            if col == 'hour':\n",
    "                max_val = 23\n",
    "            else: # weekday\n",
    "                max_val = 6\n",
    "            pdf_out[f'{col}_sin'] = np.sin(2 * np.pi * pdf[col] / max_val)\n",
    "            pdf_out[f'{col}_cos'] = np.cos(2 * np.pi * pdf[col] / max_val)\n",
    "\n",
    "        # --- 3. Timestamp (Delta) Features ---\n",
    "        for col in self.TS_COLS:\n",
    "            if col == 'release_date':\n",
    "                # Special parser for \"2023_october\"\n",
    "                ts = pd.to_datetime(pdf[col].str.replace('_', ' '), format='%Y %B', errors='coerce')\n",
    "            else:\n",
    "                # Standard unix timestamps\n",
    "                ts = pd.to_datetime(pdf[col], unit='s', errors='coerce')\n",
    "            \n",
    "            # Calculate delta in days\n",
    "            delta_days = (ref_datetime - ts).dt.total_seconds() / (60 * 60 * 24)\n",
    "            pdf_out[f'days_since_{col}'] = delta_days\n",
    "\n",
    "        # --- 4. Map/Dict Features ---\n",
    "        for col, action in self.MAP_COLS.items():\n",
    "            if isinstance(action, list): # Unroll\n",
    "                # Handle missing/empty maps\n",
    "                def safe_parse_map(x):\n",
    "                    if isinstance(x, list): # Handle '[(k, v)]' format\n",
    "                        return {k: v for k, v in x}\n",
    "                    if isinstance(x, dict):\n",
    "                        return x\n",
    "                    return {}\n",
    "                \n",
    "                parsed_maps = pdf[col].apply(safe_parse_map)\n",
    "                for key in action:\n",
    "                    pdf_out[f'{col}_{key}'] = parsed_maps.apply(lambda m: m.get(key))\n",
    "            \n",
    "            elif action == 'sum': # Aggregate\n",
    "                def safe_sum_map_values(x):\n",
    "                    try:\n",
    "                        if isinstance(x, list): # '[(k, v)]'\n",
    "                            return sum(v for k, v in x)\n",
    "                        if isinstance(x, dict):\n",
    "                            return sum(x.values())\n",
    "                    except:\n",
    "                        return 0\n",
    "                    return 0\n",
    "                pdf_out[f'{col}_sum'] = pdf[col].apply(safe_sum_map_values)\n",
    "\n",
    "        # --- 5. List Features ---\n",
    "        for col, action in self.LIST_COLS.items():\n",
    "            if action == 'count':\n",
    "                pdf_out[f'{col}_count'] = pdf[col].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "        # --- 6. Histogram Features ---\n",
    "        for col, action in self.HIST_COLS.items():\n",
    "            if action == 'total_requests':\n",
    "                # Hist format is '[(key, count)]'\n",
    "                def safe_sum_hist(x):\n",
    "                    try:\n",
    "                        return sum(count for key, count in x)\n",
    "                    except:\n",
    "                        return 0\n",
    "                pdf_out[f'{col}_total_requests'] = pdf[col].apply(safe_sum_hist)\n",
    "        \n",
    "        # --- Store list of all engineered numerical columns ---\n",
    "        if not self.engineered_num_cols:\n",
    "            all_cols = set(pdf_out.columns)\n",
    "            cat_target = set(self.CAT_COLS) | set([self.TARGET_COL])\n",
    "            self.engineered_num_cols = sorted(list(all_cols - cat_target))\n",
    "            print(f\"Discovered {len(self.engineered_num_cols)} engineered numerical features.\")\n",
    "            print(f\"First 5: {self.engineered_num_cols[:5]}\")\n",
    "        \n",
    "        return pdf_out[self.CAT_COLS + self.engineered_num_cols + [self.TARGET_COL]]\n",
    "\n",
    "    \n",
    "    def transform_partition(self, pdf, ref_datetime_col):\n",
    "        \"\"\"\n",
    "        Applies all transforms to a partition and returns data\n",
    "        ready for the model.\n",
    "        \"\"\"\n",
    "        # 1. Engineer all features\n",
    "        pdf_fe = self.engineer_features_partition(pdf, ref_datetime_col)\n",
    "        \n",
    "        # 2. Separate data\n",
    "        pdf_cat = pdf_fe[self.CAT_COLS]\n",
    "        pdf_num = pdf_fe[self.engineered_num_cols]\n",
    "        pdf_target = pdf_fe[self.TARGET_COL]\n",
    "\n",
    "        # 3. Apply Vocabularies (Categorical)\n",
    "        cat_data = {}\n",
    "        for col in self.CAT_COLS:\n",
    "            vocab = self.vocabularies[col]\n",
    "            # Map values, fill NaNs from mapping with '<MISSING>' token (0)\n",
    "            cat_data[col] = pdf_cat[col].map(vocab).fillna(0).values\n",
    "        \n",
    "        # 4. Apply Scaling (Numerical)\n",
    "        # Handle NAs/Infs from log/deltas before scaling\n",
    "        pdf_num = pdf_num.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        if not self._is_scaler_fit:\n",
    "            print(\"Fitting StandardScaler on first partition...\")\n",
    "            self.scaler.fit(pdf_num)\n",
    "            self._is_scaler_fit = True\n",
    "        \n",
    "        num_data = self.scaler.transform(pdf_num)\n",
    "        \n",
    "        # 5. Get Target\n",
    "        target_data = pdf_target.fillna(0).values\n",
    "        \n",
    "        return cat_data, num_data, target_data\n",
    "\n",
    "\n",
    "# --- 5. Main Training Function ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to fit preprocessors and train the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Setup Device (CUDA or CPU) ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load Dask DataFrame (Lazily) ---\n",
    "    print(f\"Loading data from: {TRAIN_DATA_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        # Define dtypes for complex columns to help Dask\n",
    "        # This is often necessary for lists/dicts\n",
    "        dtypes = {\n",
    "            'bcat': 'object',\n",
    "            'bcat_bottom_taxonomy': 'object',\n",
    "            'bundles_cat': 'object',\n",
    "            'bundles_cat_bottom_taxonomy': 'object',\n",
    "            'bundles_ins': 'object',\n",
    "            'city_hist': 'object',\n",
    "            'country_hist': 'object',\n",
    "            'cpm': 'object',\n",
    "            'cpm_pct_rk': 'object',\n",
    "            'ctr': 'object',\n",
    "            'ctr_pct_rk': 'object',\n",
    "            'dev_language_hist': 'object',\n",
    "            'dev_osv_hist': 'object',\n",
    "            'first_request_ts_bundle': 'object',\n",
    "            'first_request_ts_category_bottom_taxonomy': 'object',\n",
    "            'hour_ratio': 'object',\n",
    "            'iap_revenue_usd_bundle': 'object',\n",
    "            'iap_revenue_usd_category': 'object',\n",
    "            'iap_revenue_usd_category_bottom_taxonomy': 'object',\n",
    "            'last_buy_ts_bundle': 'object',\n",
    "            'last_buy_ts_category': 'object',\n",
    "            'last_install_ts_bundle': 'object',\n",
    "            'last_install_ts_category': 'object',\n",
    "            'advertiser_actions_action_count': 'object',\n",
    "            'advertiser_actions_action_last_timestamp': 'object',\n",
    "            'user_actions_bundles_action_count': 'object',\n",
    "            'user_actions_bundles_action_last_timestamp': 'object',\n",
    "            'new_bundles': 'object',\n",
    "            'num_buys_bundle': 'object',\n",
    "            'num_buys_category': 'object',\n",
    "            'num_buys_category_bottom_taxonomy': 'object',\n",
    "            'region_hist': 'object',\n",
    "            'rev_by_adv': 'object',\n",
    "            'rwd_prank': 'object',\n",
    "            'user_bundles': 'object',\n",
    "            'user_bundles_l28d': 'object',\n",
    "            'whale_users_bundle_num_buys_prank': 'object',\n",
    "            'whale_users_bundle_revenue_prank': 'object',\n",
    "            'whale_users_bundle_total_num_buys': 'object',\n",
    "            'whale_users_bundle_total_revenue': 'object'\n",
    "        }\n",
    "        \n",
    "        all_cols = (\n",
    "            CAT_COLS + NUM_COLS_RAW + CYCLICAL_COLS + TIMESTAMP_COLS + \n",
    "            list(MAP_COLS.keys()) + list(LIST_COLS.keys()) + \n",
    "            list(HIST_COLS.keys()) + [TARGET_COLUMN] + METADATA_COLS\n",
    "        )\n",
    "        \n",
    "        # Read only the columns we need\n",
    "        ddf = dd.read_parquet(\n",
    "            TRAIN_DATA_PATH, \n",
    "            engine='pyarrow', \n",
    "            columns=list(set(all_cols)) # Use set for uniqueness\n",
    "        )\n",
    "        \n",
    "        # Ensure 'datetime' is parsed as a Dask datetime series\n",
    "        ddf['datetime'] = dd.to_datetime(ddf['datetime'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet metadata from {TRAIN_DATA_PATH}. {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 1. FIT PREPROCESSORS ---\n",
    "    print(\"\\n--- Starting Preprocessing 'fit' Step ---\")\n",
    "    processor = FeatureProcessor(\n",
    "        CAT_COLS, NUM_COLS_RAW, CYCLICAL_COLS, TIMESTAMP_COLS,\n",
    "        MAP_COLS, LIST_COLS, HIST_COLS, TARGET_COLUMN\n",
    "    )\n",
    "    \n",
    "    # Fit vocabularies (requires a pass over the data)\n",
    "    processor.fit_vocabs(ddf, rare_threshold=RARE_THRESHOLD)\n",
    "    \n",
    "    # NOTE: We will fit the scaler on the *first partition* during training.\n",
    "    # Fitting a dask_ml.StandardScaler requires engineering all features\n",
    "    # on the *entire* dask dataframe first, which is another full pass.\n",
    "    # For speed in this example, we'll fit on the first batch.\n",
    "    # For a production model, you should fit on the full ddf.\n",
    "    print(\"--- Preprocessing 'fit' Complete ---\")\n",
    "    \n",
    "\n",
    "    # --- 2. INITIALIZE MODEL ---\n",
    "    # We must wait until after fitting vocabs to know the vocab sizes\n",
    "    \n",
    "    # We also need to know the *final* number of numerical features.\n",
    "    # We'll get this by processing a dummy partition (the metadata).\n",
    "    print(\"Discovering engineered feature dimensions...\")\n",
    "    dummy_pdf = ddf._meta_nonempty.copy()\n",
    "    dummy_dt = pd.Series([pd.Timestamp.now()] * len(dummy_pdf), index=dummy_pdf.index)\n",
    "    _ = processor.engineer_features_partition(dummy_pdf, dummy_dt)\n",
    "    \n",
    "    num_numerical_features = len(processor.engineered_num_cols)\n",
    "    \n",
    "    model = WideAndDeepModel(\n",
    "        vocab_sizes=processor.vocab_sizes,\n",
    "        embedding_dims=EMBEDDING_DIMS,\n",
    "        num_numerical_features=num_numerical_features\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss() # Mean Squared Error for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    model.train() # Set model to training mode\n",
    "\n",
    "    # --- 3. TRAINING LOOP: PARTITION BY PARTITION ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        \n",
    "        # Get a list of delayed tasks, one for each partition file\n",
    "        partition_iterator = ddf.to_delayed()\n",
    "        \n",
    "        for i, partition in enumerate(partition_iterator):\n",
    "            # 1. Load *only this single partition* into memory as a Pandas DF\n",
    "            pdf = partition.compute()\n",
    "            \n",
    "            if pdf.empty:\n",
    "                print(f\"  Skipping empty partition {i+1}\")\n",
    "                continue\n",
    "                \n",
    "            # 2. Extract reference datetime and drop metadata\n",
    "            ref_datetime_col = pdf['datetime']\n",
    "            pdf_features = pdf.drop(columns=METADATA_COLS, errors='ignore')\n",
    "\n",
    "            # 3. Apply all preprocessing and transformations\n",
    "            cat_data, num_data, target_data = processor.transform_partition(\n",
    "                pdf_features, \n",
    "                ref_datetime_col\n",
    "            )\n",
    "            \n",
    "            # 4. Create custom Dataset and DataLoader\n",
    "            partition_dataset = HeterogeneousDataset(cat_data, num_data, target_data)\n",
    "            partition_loader = DataLoader(\n",
    "                dataset=partition_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "            # 5. Mini-Batch Training Loop (for this partition)\n",
    "            for batch_num, ((batch_cat, batch_num_features), batch_target) in enumerate(partition_loader):\n",
    "                \n",
    "                # Move data to GPU\n",
    "                batch_cat = {k: v.to(device) for k, v in batch_cat.items()}\n",
    "                batch_num_features = batch_num_features.to(device)\n",
    "                batch_target = batch_target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_cat, batch_num_features)\n",
    "                loss = criterion(outputs, batch_target)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"  Epoch {epoch+1} | Processed Partition {i+1} | Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hack (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
