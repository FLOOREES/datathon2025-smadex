{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c815e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\n",
      "Contenido de ./data: ['sample_submission.csv', 'split', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\tsys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d0eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x1fb4027cb60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5792dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"./data/train/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "CATEGORICAL_FEATURES = [\n",
    "\t'advertiser_bundle',\n",
    "\t'advertiser_category',\n",
    "\t'advertiser_subcategory',\n",
    "\t'advertiser_bottom_taxonomy_level',\n",
    "\t'country',\n",
    "\t'dev_make',\n",
    "\t'dev_model',\n",
    "\t'dev_os',\n",
    "\t'dev_osv',\n",
    "\t'release_date',\n",
    "\t'avg_daily_sessions',\n",
    "\t'avg_duration',\n",
    "\t'bcat_bottom_taxonomy',\n",
    "\t'ctr',\n",
    "\t]\n",
    "NUMERICAL_FEATURES = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'hour', 'release_msrp', 'weekday', 'avg_act_days', 'bundles_cat_bottom_taxonomy', 'first_request_ts_bundle', 'iap_revenue_usd_bundle', 'last_buy', 'last_buy_ts_bundle', 'last_buy_ts_category', 'last_ins', 'user_actions_bundles_action_last_timestamp', 'num_buys_bundle', 'rev_by_adv', 'rwd_prank', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank', 'whale_users_bundle_total_num_buys', 'whale_users_bundle_total_revenue', 'last_buy_safe', 'last_ins_safe', 'last_buy_dt', 'last_ins_dt', 'hours_since_last_buy', 'hours_since_last_ins']\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "\n",
    "def load_embeddings_mapping():\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "\t\tmappings = json.load(f)\n",
    "\treturn mappings\n",
    "\n",
    "#EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = ['buyer_d1', \n",
    "\t\t\t 'buyer_d7', \n",
    "\t\t\t 'buyer_d14', \n",
    "\t\t\t 'buyer_d28', \n",
    "\t\t\t 'buy_d7', \n",
    "\t\t\t 'buy_d14', \n",
    "\t\t\t 'buy_d28', \n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'iap_revenue_d14', \n",
    "\t\t\t 'iap_revenue_d28', \n",
    "\t\t\t 'registration', \n",
    "\t\t\t 'retention_d1_to_d7', \n",
    "\t\t\t 'retention_d3_to_d7', \n",
    "\t\t\t 'retention_d7_to_d14', \n",
    "\t\t\t 'retention_d1', \n",
    "\t\t\t 'retention_d3', \n",
    "\t\t\t 'retentiond7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "train_filters = [(\"datetime\", \">=\", \"2025-10-01-00-00\"), (\"datetime\", \"<\", \"2025-10-06-00-00\")]\n",
    "\n",
    "ddf_train = dd.read_parquet(\n",
    "\tDATASET_PATH,\n",
    "\tengine=\"pyarrow\",\n",
    "\tcolumns=COLS_TO_READ,\n",
    "\tfilters=train_filters\n",
    ")\n",
    "\n",
    "# loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68426287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_partition(df):\n",
    "\ttransform_variables(df)\n",
    "\tscale_numerical_features(df, loaded_scaler)\n",
    "\timpute_missings(df)\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "\t# Check if the input is a list\n",
    "\tif isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "\t\t# Extract all numeric timestamps from the tuples\n",
    "\t\ttimestamps = []\n",
    "\t\tfor t in tuples_list:\n",
    "\t\t\tif isinstance(t, tuple) and len(t) == 2:\n",
    "\t\t\t\tts = t[1]\n",
    "\t\t\t\tif ts is not None and not isinstance(ts, list):\n",
    "\t\t\t\t\ttimestamps.append(ts)\n",
    "\t\tif len(timestamps) > 0:\n",
    "\t\t\t# Use the largest timestamp (closest to now)\n",
    "\t\t\tmax_ts = max(timestamps)\n",
    "\t\t\treturn (now_ts - max_ts) / 3600  # seconds → hours\n",
    "\treturn np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "\t\"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "\tif isinstance(tuple_list, list):\n",
    "\t\treturn [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "\treturn []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "\t\"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "\tif not values:\n",
    "\t\treturn np.nan\n",
    "\tif mode == \"sum\":\n",
    "\t\treturn sum(values)\n",
    "\treturn sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "\tnow = pd.Timestamp.now()\n",
    "\n",
    "\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\tmin_ts = 0                  # 1970-01-01\n",
    "\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t# Replace invalid timestamps with NaN\n",
    "\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\n",
    "\t# Convert safe Unix timestamps to datetime\n",
    "\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t# Compute hours ago\n",
    "\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t# Drop the original Unix timestamp columns\n",
    "\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Convert 'hour' from string to integer\n",
    "\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t# Convert hour to radians (full circle = 24 hours)\n",
    "\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t# Create two new columns: sin_hour and cos_hour\n",
    "\tdf['hour_sin'] = np.sin(radians)\n",
    "\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t# Drop the original 'hour' column\n",
    "\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Ensure `now_ts` is a Unix timestamp\n",
    "\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t# Apply to your 4 timestamp columns\n",
    "\tts_cols = [\n",
    "\t\t\"first_request_ts_bundle\",\n",
    "\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\"last_buy_ts_category\",\n",
    "\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t]\n",
    "\n",
    "\tfor col in ts_cols:\n",
    "\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t# Drop the original tuple columns if you want\n",
    "\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trules = {\n",
    "\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\"ctr\": \"sum\",\n",
    "\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t}\n",
    "\n",
    "\tfor col, mode in rules.items():\n",
    "\t\tnew_col = col + \"_agg\"\n",
    "\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t)\n",
    "\t\tprint(f\"Transformed {col} into {new_col} using {mode} aggregation.\")\n",
    "\n",
    "\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t# If the df contains the target column, transform it with the logarithm\n",
    "\tif TARGET in df.columns:\n",
    "\t\tdf[TARGET] = np.log1p(df[TARGET])\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "\tfor col in CATEGORICAL_FEATURES:\n",
    "\t\tdf[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "\t\t# convert to embeddings indices (if category not seen before, assign index 0)\n",
    "\t\tmapping = EMBEDDING_MAPPINGS[col]\n",
    "\t\tdf[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "\tfor col in NUMERICAL_FEATURES:\n",
    "\t\t# 1. Create missing indicator\n",
    "\t\tdf[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "\t\t# 2. Impute missings with 0\n",
    "\t\tdf[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "\tdf[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "\n",
    "def train_scaler(df):\n",
    "\tscaler = StandardScaler()\n",
    "\tscaler.fit(df[NUMERICAL_FEATURES])\n",
    "\tjoblib.dump(scaler, SCALER_FILE)\n",
    "\tprint(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(df: pd.DataFrame):\n",
    "\t# Use after transform_variables but BEFORE imputing missings\n",
    "\t\n",
    "\tmappings = {}\n",
    "\tfor feature in CATEGORICAL_FEATURES:\n",
    "\t\tunique_values = [\"<MISSING>\"] + df[feature].dropna().unique().tolist()\n",
    "\t\tembeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "\t\tmappings[feature] = embeddings\n",
    "\t\n",
    "\t# Save mappings to disk\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "\t\tjson.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07182829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded.\n"
     ]
    }
   ],
   "source": [
    "df_train = ddf_train.compute()\n",
    "print(\"Training data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4420dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed iap_revenue_usd_bundle into iap_revenue_usd_bundle_agg using sum aggregation.\n",
      "Transformed num_buys_bundle into num_buys_bundle_agg using sum aggregation.\n",
      "Transformed rev_by_adv into rev_by_adv_agg using sum aggregation.\n",
      "Transformed rwd_prank into rwd_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_num_buys_prank into whale_users_bundle_num_buys_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_revenue_prank into whale_users_bundle_revenue_prank_agg using mean aggregation.\n",
      "Transformed whale_users_bundle_total_num_buys into whale_users_bundle_total_num_buys_agg using sum aggregation.\n",
      "Transformed whale_users_bundle_total_revenue into whale_users_bundle_total_revenue_agg using sum aggregation.\n",
      "Transformed avg_daily_sessions into avg_daily_sessions_agg using mean aggregation.\n",
      "Transformed avg_duration into avg_duration_agg using mean aggregation.\n",
      "Transformed bcat_bottom_taxonomy into bcat_bottom_taxonomy_agg using mean aggregation.\n",
      "Transformed ctr into ctr_agg using sum aggregation.\n",
      "Transformed bundles_cat_bottom_taxonomy into bundles_cat_bottom_taxonomy_agg using sum aggregation.\n"
     ]
    }
   ],
   "source": [
    "df_train = transform_variables(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98c88ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index único: False\n",
      "Columnas únicas: True\n",
      "Columnas con listas: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Index único:\", df_train.index.is_unique)\n",
    "print(\"Columnas únicas:\", df_train.columns.is_unique)\n",
    "\n",
    "if not df_train.columns.is_unique:\n",
    "    print(\"Columnas duplicadas:\")\n",
    "    print(df_train.columns[df_train.columns.duplicated()])\n",
    "    \n",
    "cols_with_lists = []\n",
    "\n",
    "for col in df_train.columns:\n",
    "    has_list = df_train[col].apply(lambda x: isinstance(x, list)).any()\n",
    "    if has_list:\n",
    "        cols_with_lists.append(col)\n",
    "\n",
    "print(\"Columnas con listas:\", cols_with_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276a358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
      "Categorical features: ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n"
     ]
    }
   ],
   "source": [
    "#Check all types\n",
    "\n",
    "# Make a list with the numerical values and another with the categorical ones\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in df_train.columns:\n",
    "\tif pd.api.types.is_numeric_dtype(df_train[col]):\n",
    "\t\tnumerical_features.append(col)\n",
    "\telse:\n",
    "\t\tcategorical_features.append(col)\n",
    "\n",
    "print(\"Numerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "CATEGORICAL_FEATURES = categorical_features\n",
    "NUMERICAL_FEATURES = numerical_features\n",
    "\n",
    "# Remove the objective columns from numerical features\n",
    "if TARGET in NUMERICAL_FEATURES:\n",
    " \tNUMERICAL_FEATURES.remove(TARGET)\n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e29f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved.\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings_mapping(df_train)\n",
    "train_scaler(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# --- 1. Define Data and Columns to Scale ---\n",
    "cols_to_scale = ['age', 'income']\n",
    "scaler_filename = 'my_scaler.joblib'\n",
    "\n",
    "# Create a training DataFrame\n",
    "df_train = pd.DataFrame({\n",
    "\t'age': [20, 30, 40, 50, np.nan],\n",
    "\t'income': [50000, 60000, np.nan, 80000, 75000],\n",
    "\t'city': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney']\n",
    "})\n",
    "\n",
    "print(\"Original Training Data:\")\n",
    "print(df_train)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. Initialize, Fit, and Save Scaler ---\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the specified columns\n",
    "# .fit() ignores NaNs when calculating mean and std\n",
    "print(f\"Fitting scaler on columns: {cols_to_scale}\")\n",
    "scaler.fit(df_train[cols_to_scale])\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3. Load Scaler and Transform New Data ---\n",
    "\n",
    "# Create new \"test\" data to transform\n",
    "df_test = pd.DataFrame({\n",
    "\t'age': [35, 45, np.nan],\n",
    "\t'income': [55000, 90000, 62000],\n",
    "\t'city': ['Berlin', 'Moscow', 'Lisbon']\n",
    "})\n",
    "\n",
    "print(\"New Data (Before Transform):\")\n",
    "print(df_test)\n",
    "print(f\"Column names before: {df_test.columns.tolist()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the scaler from the file\n",
    "loaded_scaler = joblib.load(scaler_filename)\n",
    "print(\"Scaler loaded.\")\n",
    "\n",
    "# Create a copy to avoid SettingWithCopyWarning\n",
    "# df_test_transformed = df_test.copy()\n",
    "\n",
    "# --- This is the key step ---\n",
    "# Transform only the specified columns and assign them back\n",
    "# to the *same columns* in the copied DataFrame.\n",
    "# .transform() scales non-NaNs and keeps NaNs as NaN.\n",
    "df_test[cols_to_scale] = loaded_scaler.transform(df_test[cols_to_scale])\n",
    "\n",
    "# --- 4. Verify Results ---\n",
    "\n",
    "print(\"\\nNew Data (After Transform):\")\n",
    "print(df_test)\n",
    "print(f\"Column names after:  {df_test.columns.tolist()}\")\n",
    "\n",
    "# Check that column names and order are identical\n",
    "are_names_same = df_test.columns.equals(df_test.columns)\n",
    "print(f\"\\nColumn names and order unchanged: {are_names_same}\")\n",
    "\n",
    "# Clean up the created file\n",
    "os.remove(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other variables in df: ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28', 'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28', 'registration', 'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retentiond7', 'hour', 'release_msrp', 'weekday', 'avg_act_days', 'bundles_cat', 'bundles_cat_bottom_taxonomy', 'city_hist', 'country_hist', 'dev_language_hist', 'dev_osv_hist', 'first_request_ts_bundle', 'iap_revenue_usd_bundle', 'last_buy', 'last_buy_ts_bundle', 'last_buy_ts_category', 'last_ins', 'user_actions_bundles_action_last_timestamp', 'num_buys_bundle', 'region_hist', 'rev_by_adv', 'rwd_prank', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank', 'whale_users_bundle_total_num_buys', 'whale_users_bundle_total_revenue', 'last_buy_safe', 'last_ins_safe', 'last_buy_dt', 'last_ins_dt', 'hours_since_last_buy', 'hours_since_last_ins']\n"
     ]
    }
   ],
   "source": [
    "# Check all categorical features are present in df\n",
    "for feature in CATEGORICAL_FEATURES:\n",
    "\tif feature not in df.columns:\n",
    "\t\traise ValueError(f\"Categorical feature '{feature}' not found in dataframe columns.\")\n",
    "\n",
    "# Show the other variables present in df\n",
    "df_vars = [col for col in df.columns if col not in CATEGORICAL_FEATURES]\n",
    "print(\"Other variables in df:\", df_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaef555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== advertiser_bundle ====\n",
      "advertiser_bundle\n",
      "<class 'str'>    17294102\n",
      "Name: count, dtype: int64\n",
      "==== advertiser_category ====\n",
      "advertiser_category\n",
      "<class 'str'>         15609385\n",
      "<class 'NoneType'>     1684717\n",
      "Name: count, dtype: int64\n",
      "==== advertiser_subcategory ====\n",
      "advertiser_subcategory\n",
      "<class 'str'>         15609385\n",
      "<class 'NoneType'>     1684717\n",
      "Name: count, dtype: int64\n",
      "==== advertiser_bottom_taxonomy_level ====\n",
      "advertiser_bottom_taxonomy_level\n",
      "<class 'str'>         10849228\n",
      "<class 'NoneType'>     6444874\n",
      "Name: count, dtype: int64\n",
      "==== country ====\n",
      "country\n",
      "<class 'str'>         17287248\n",
      "<class 'NoneType'>        6854\n",
      "Name: count, dtype: int64\n",
      "==== dev_make ====\n",
      "dev_make\n",
      "<class 'str'>         16974677\n",
      "<class 'NoneType'>      319425\n",
      "Name: count, dtype: int64\n",
      "==== dev_model ====\n",
      "dev_model\n",
      "<class 'str'>         17265823\n",
      "<class 'NoneType'>       28279\n",
      "Name: count, dtype: int64\n",
      "==== dev_os ====\n",
      "dev_os\n",
      "<class 'str'>         17294014\n",
      "<class 'NoneType'>          88\n",
      "Name: count, dtype: int64\n",
      "==== dev_osv ====\n",
      "dev_osv\n",
      "<class 'str'>         17141125\n",
      "<class 'NoneType'>      152977\n",
      "Name: count, dtype: int64\n",
      "==== release_date ====\n",
      "release_date\n",
      "<class 'str'>         17232355\n",
      "<class 'NoneType'>       61747\n",
      "Name: count, dtype: int64\n",
      "==== avg_daily_sessions ====\n",
      "avg_daily_sessions\n",
      "<class 'NoneType'>    13169975\n",
      "<class 'list'>         4124127\n",
      "Name: count, dtype: int64\n",
      "==== avg_duration ====\n",
      "avg_duration\n",
      "<class 'NoneType'>    13169975\n",
      "<class 'list'>         4124127\n",
      "Name: count, dtype: int64\n",
      "==== bcat ====\n",
      "bcat\n",
      "<class 'list'>        9020766\n",
      "<class 'NoneType'>    8273336\n",
      "Name: count, dtype: int64\n",
      "==== bcat_bottom_taxonomy ====\n",
      "bcat_bottom_taxonomy\n",
      "<class 'list'>        9020766\n",
      "<class 'NoneType'>    8273336\n",
      "Name: count, dtype: int64\n",
      "==== ctr ====\n",
      "ctr\n",
      "<class 'NoneType'>    16681055\n",
      "<class 'list'>          613047\n",
      "Name: count, dtype: int64\n",
      "==== hour_ratio ====\n",
      "hour_ratio\n",
      "<class 'list'>        9676269\n",
      "<class 'NoneType'>    7617833\n",
      "Name: count, dtype: int64\n",
      "==== user_actions_bundles_action_count ====\n",
      "user_actions_bundles_action_count\n",
      "<class 'NoneType'>    11124724\n",
      "<class 'list'>         6169378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for feature in CATEGORICAL_FEATURES:\n",
    "\tprint(\"====\", feature, \"====\")\n",
    "\tprint(df[feature].map(type).value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb01089",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_embeddings_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mgenerate_embeddings_mapping\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      4\u001b[39m mappings = {}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m CATEGORICAL_FEATURES:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     unique_values = [\u001b[33m\"\u001b[39m\u001b[33m<MISSING>\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m      7\u001b[39m     embeddings = {val: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_values)}\n\u001b[32m      8\u001b[39m     mappings[feature] = embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:2419\u001b[39m, in \u001b[36mSeries.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m   2357\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[33;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[32m   2359\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2417\u001b[39m \u001b[33;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[32m   2418\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:1029\u001b[39m, in \u001b[36mIndexOpsMixin.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1027\u001b[39m     result = values.unique()\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(values):\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[32m    310\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m \u001b[33;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[39m, in \u001b[36munique_with_mask\u001b[39m\u001b[34m(values, mask)\u001b[39m\n\u001b[32m    438\u001b[39m table = hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     uniques = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     uniques = _reconstruct_data(uniques, original.dtype, original)\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7260\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7203\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "generate_embeddings_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_cardinalities: Dict[str, int],\n",
    "        cat_features: List[str],\n",
    "        num_cont_features: int,          # numéricas + indicadores de missing\n",
    "        embedding_dim: int = 16,\n",
    "        hidden_dims: List[int] = [128, 64],\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_cat = len(cat_features)\n",
    "\n",
    "        # Embeddings por feature categórica\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "            for feat, cardinality in cat_cardinalities.items()\n",
    "        })\n",
    "\n",
    "        # Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "        total_cat_dim = embedding_dim * self.num_cat\n",
    "        input_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, num_cat + num_cont_features]\n",
    "        columnas en el orden:\n",
    "        [CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Separar categóricas y numéricas del gran vector\n",
    "        x_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "        x_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "        # 2) Embeddings categóricas\n",
    "        emb_list = []\n",
    "        for i, feat in enumerate(self.cat_features):\n",
    "            emb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "            emb_list.append(emb)\n",
    "\n",
    "        if emb_list:\n",
    "            x_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "            x_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "        else:\n",
    "            x_input = x_cont\n",
    "\n",
    "        out = self.mlp(x_input)   # [batch, 1]\n",
    "        return out.squeeze(-1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_mappings.json\", \"r\") as f:\n",
    "    EMBEDDING_MAPPINGS = json.load(f)\n",
    "\n",
    "num_cont_features = len(NUMERICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numéricas + _is_missing\n",
    "\n",
    "cat_cardinalities = {\n",
    "    feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "    for feat in CATEGORICAL_FEATURES\n",
    "}\n",
    "\n",
    "model = EmbeddingMLP(\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    cat_features=CATEGORICAL_FEATURES,\n",
    "    num_cont_features=num_cont_features,\n",
    "    embedding_dim=16,\n",
    "    hidden_dims=[128, 64],\n",
    "    dropout=0.1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()   # estás trabajando en log1p(target)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x_num = batch[\"num\"].to(device).float()\n",
    "        x_cat = batch[\"cat\"].to(device).long()\n",
    "        y = batch[\"y\"].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_num, x_cat)          # [batch]\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / max(n_batches, 1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - train_loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validación simple (opcional aquí, pero deberías hacerlo)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader:\n",
    "            x_num = batch[\"num\"].to(device).float()\n",
    "            x_cat = batch[\"cat\"].to(device).long()\n",
    "            y = batch[\"y\"].to(device).float()\n",
    "\n",
    "            preds = model(x_num, x_cat)\n",
    "            loss = criterion(preds, y)\n",
    "            val_loss += loss.item()\n",
    "            n_val += 1\n",
    "\n",
    "    val_loss /= max(n_val, 1)\n",
    "    print(f\"           val_loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22757abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTTransformer(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tcat_cardinalities: Dict[str, int],\n",
    "\t\tcat_features: List[str],\n",
    "\t\tnum_cont_features: int,        # numéricas + indicadores de missing\n",
    "\t\td_token: int = 32,             # dimensión de cada token (embeddings + cont)\n",
    "\t\tn_layers: int = 3,\n",
    "\t\tn_heads: int = 4,\n",
    "\t\tff_mult: float = 4.0,\n",
    "\t\tdropout: float = 0.1,\n",
    "\t\tmlp_hidden_dims: List[int] = [128, 64],\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tassert d_token % n_heads == 0, \"d_token debe ser divisible por n_heads\"\n",
    "\n",
    "\t\tself.cat_features = cat_features\n",
    "\t\tself.num_cat = len(cat_features)\n",
    "\t\tself.num_cont_features = num_cont_features\n",
    "\t\tself.d_token = d_token\n",
    "\n",
    "\t\t# 1) Embeddings por feature categórica (cada una produce un token de dim d_token)\n",
    "\t\tself.cat_embeddings = nn.ModuleDict({\n",
    "\t\t\tfeat: nn.Embedding(num_embeddings=cardinality, embedding_dim=d_token)\n",
    "\t\t\tfor feat, cardinality in cat_cardinalities.items()\n",
    "\t\t})\n",
    "\n",
    "\t\t# 2) \"Embeddings\" para continuas: un token por feature numérica\n",
    "\t\t#    x_cont: [B, num_cont] -> [B, num_cont, d_token]\n",
    "\t\tself.cont_weight = nn.Parameter(torch.randn(num_cont_features, d_token))\n",
    "\t\tself.cont_bias = nn.Parameter(torch.zeros(num_cont_features, d_token))\n",
    "\n",
    "\t\t# 3) Token CLS para agregar toda la fila\n",
    "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "\n",
    "\t\t# 4) Bloques Transformer encoder\n",
    "\t\tencoder_layer = nn.TransformerEncoderLayer(\n",
    "\t\t\td_model=d_token,\n",
    "\t\t\tnhead=n_heads,\n",
    "\t\t\tdim_feedforward=int(ff_mult * d_token),\n",
    "\t\t\tdropout=dropout,\n",
    "\t\t\tbatch_first=True,        # [B, T, C]\n",
    "\t\t\tactivation=\"gelu\",\n",
    "\t\t)\n",
    "\t\tself.transformer = nn.TransformerEncoder(\n",
    "\t\t\tencoder_layer,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t)\n",
    "\n",
    "\t\t# 5) Cabeza final MLP (regresión escalar)\n",
    "\t\tmlp_layers = []\n",
    "\t\tprev_dim = d_token\n",
    "\t\tfor h in mlp_hidden_dims:\n",
    "\t\t\tmlp_layers.append(nn.Linear(prev_dim, h))\n",
    "\t\t\tmlp_layers.append(nn.ReLU())\n",
    "\t\t\tmlp_layers.append(nn.Dropout(dropout))\n",
    "\t\t\tprev_dim = h\n",
    "\t\tmlp_layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "\t\tself.head = nn.Sequential(*mlp_layers)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\"\"\"\n",
    "\t\tx: [batch, num_cat + num_cont_features]\n",
    "\t\tcolumnas en el orden:\n",
    "\t\t[CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# 1) Separar categóricas y numéricas del gran vector\n",
    "\t\tx_cat_ids = x[:, :self.num_cat].long()      # [B, num_cat]\n",
    "\t\tx_cont = x[:, self.num_cat:].float()        # [B, num_cont]\n",
    "\n",
    "\t\t# 2) Tokens categóricos: uno por feature categórica\n",
    "\t\tcat_tokens = []\n",
    "\t\tfor i, feat in enumerate(self.cat_features):\n",
    "\t\t\ttok = self.cat_embeddings[feat](x_cat_ids[:, i])   # [B, d_token]\n",
    "\t\t\tcat_tokens.append(tok)\n",
    "\n",
    "\t\tif cat_tokens:\n",
    "\t\t\tx_cat_tok = torch.stack(cat_tokens, dim=1)        # [B, num_cat, d_token]\n",
    "\t\telse:\n",
    "\t\t\t# Por si acaso, aunque en tu caso siempre hay categóricas\n",
    "\t\t\tx_cat_tok = torch.empty(x.size(0), 0, self.d_token, device=x.device)\n",
    "\n",
    "\t\t# 3) Tokens continuos: uno por feature numérica (incluye indicadores de missing)\n",
    "\t\t# x_cont: [B, num_cont]\n",
    "\t\t# cont_weight: [num_cont, d_token]\n",
    "\t\t# -> [B, num_cont, d_token]\n",
    "\t\tx_cont_exp = x_cont.unsqueeze(-1)                     # [B, num_cont, 1]\n",
    "\t\tcont_tokens = x_cont_exp * self.cont_weight.unsqueeze(0) + self.cont_bias.unsqueeze(0)\n",
    "\n",
    "\t\t# 4) Concatenar tokens: [cat_tokens..., cont_tokens...]\n",
    "\t\ttokens = torch.cat([x_cat_tok, cont_tokens], dim=1)   # [B, T, d_token], T = num_cat + num_cont\n",
    "\n",
    "\t\t# 5) Prepend CLS token\n",
    "\t\tB = x.size(0)\n",
    "\t\tcls = self.cls_token.expand(B, 1, self.d_token)       # [B, 1, d_token]\n",
    "\t\ttokens = torch.cat([cls, tokens], dim=1)              # [B, 1+T, d_token]\n",
    "\n",
    "\t\t# 6) Pasar por el Transformer\n",
    "\t\tencoded = self.transformer(tokens)                    # [B, 1+T, d_token]\n",
    "\n",
    "\t\t# 7) Leer solo el CLS\n",
    "\t\tcls_out = encoded[:, 0, :]                            # [B, d_token]\n",
    "\n",
    "\t\t# 8) Cabeza MLP final\n",
    "\t\tout = self.head(cls_out)                              # [B, 1]\n",
    "\t\treturn out.squeeze(-1)                                # [B]   \n",
    "\n",
    "\n",
    "def main():\n",
    "\tAMP = False\n",
    "\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"Using device: {device}\")\n",
    "\tif not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "\t\tAMP = False\n",
    "\t\n",
    "\ttrain_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "\tval_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "\t\n",
    "\tddf_train = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=train_filters\n",
    "\t)\n",
    "\n",
    "\tddf_val = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=val_filters\n",
    "\t)\n",
    "\n",
    "\tval_pdf = ddf_val.compute()\n",
    "\tval_pdf = process_partition(val_pdf)\n",
    "\n",
    "\tval_features_tensor = torch.tensor(\n",
    "\t\tval_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\t\n",
    "\tval_target_tensor = torch.tensor(\n",
    "\t\tval_pdf[TARGET].values,\n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\n",
    "\tval_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "\ttest_loader = DataLoader(dataset=val_dataset, \n",
    "\t\t\t\t\t\t\t batch_size=BATCH_SIZE, \n",
    "\t\t\t\t\t\t\t num_workers=0,\n",
    "\t\t\t\t\t\t\t pin_memory=True,\n",
    "\t\t\t\t\t\t\t shuffle=False)\n",
    "\n",
    "\tnum_cont_features = len(NUMERICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numéricas + _is_missing\n",
    "\n",
    "\tcat_cardinalities = {\n",
    "\t\tfeat: len(EMBEDDING_MAPPINGS[feat])\n",
    "\t\tfor feat in CATEGORICAL_FEATURES\n",
    "\t}\n",
    "\tmodel = FTTransformer(\n",
    "\t\tcat_cardinalities=cat_cardinalities,\n",
    "\t\tcat_features=CATEGORICAL_FEATURES,\n",
    "\t\tnum_cont_features=num_cont_features,\n",
    "\t\td_token=64,          # puedes subir a 64 si la GPU aguanta\n",
    "\t\tn_layers=3,\n",
    "\t\tn_heads=4,\n",
    "\t\tff_mult=4.0,\n",
    "\t\tdropout=0.1,\n",
    "\t\tmlp_hidden_dims=[128, 64],\n",
    "\t).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon2025-smadex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
