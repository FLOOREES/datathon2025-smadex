{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710a3b2d",
   "metadata": {},
   "source": [
    "# Imports and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.getcwd() \n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\tsys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b021f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Dict, List\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d75c36",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_READ = [\n",
    "\t\t\t 'iap_revenue_d7',  \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "DATASET_PATH = \"./data/train/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "TARGET = \"iap_revenue_d7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    df = transform_variables(df)\n",
    "    df = scale_numerical_features(df, loaded_scaler)\n",
    "    df = impute_missings(df)\n",
    "    return df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "    # Check if the input is a list\n",
    "    if isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "        # Extract all numeric timestamps from the tuples\n",
    "        timestamps = []\n",
    "        for t in tuples_list:\n",
    "            if isinstance(t, tuple) and len(t) == 2:\n",
    "                ts = t[1]\n",
    "                if ts is not None and not isinstance(ts, list):\n",
    "                    timestamps.append(ts)\n",
    "        if len(timestamps) > 0:\n",
    "            # Use the largest timestamp (closest to now)\n",
    "            max_ts = max(timestamps)\n",
    "            return (now_ts - max_ts) / 3600  # seconds → hours\n",
    "    return np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "    \"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "    if isinstance(tuple_list, list):\n",
    "        return [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "    return []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "    \"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "    if not values:\n",
    "        return np.nan\n",
    "    if mode == \"sum\":\n",
    "        return sum(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "     \n",
    "\tnow = pd.Timestamp.now()\n",
    "\n",
    "\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\tmin_ts = 0                  # 1970-01-01\n",
    "\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t# Replace invalid timestamps with NaN\n",
    "\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t)\n",
    "\n",
    "\t# Convert safe Unix timestamps to datetime\n",
    "\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t# Compute hours ago\n",
    "\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t# Drop the original Unix timestamp columns\n",
    "\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Convert 'hour' from string to integer\n",
    "\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t# Convert hour to radians (full circle = 24 hours)\n",
    "\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t# Create two new columns: sin_hour and cos_hour\n",
    "\tdf['hour_sin'] = np.sin(radians)\n",
    "\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t# Drop the original 'hour' column\n",
    "\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Ensure `now_ts` is a Unix timestamp\n",
    "\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t# Apply to your 4 timestamp columns\n",
    "\tts_cols = [\n",
    "\t\t\"first_request_ts_bundle\",\n",
    "\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\"last_buy_ts_category\",\n",
    "\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t]\n",
    "\n",
    "\tfor col in ts_cols:\n",
    "\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t# Drop the original tuple columns if you want\n",
    "\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trules = {\n",
    "\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\"ctr\": \"sum\",\n",
    "\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t}\n",
    "\n",
    "\tfor col, mode in rules.items():\n",
    "\t\tnew_col = col + \"_agg\"\n",
    "\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t)\n",
    "\n",
    "\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t# If the df contains the target column, transform it with the logarithm\n",
    "\tif TARGET in df.columns:\n",
    "\t\tdf[TARGET] = np.log1p(df[TARGET])\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        df[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "        # convert to embeddings indices (if category not seen before, assign index 0)\n",
    "        mapping = EMBEDDING_MAPPINGS[col]\n",
    "        df[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    for col in NUMERICAL_FEATURES:\n",
    "        # 1. Create missing indicator\n",
    "        df[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "        # 2. Impute missings with 0\n",
    "        df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "    df[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "    return df\n",
    "\n",
    "def train_scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "    # Use after transform_variables but BEFORE imputing missings\n",
    "    \n",
    "    mappings = {}\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        unique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "        embeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[feature] = embeddings\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "        json.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ec162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "\n",
    "ddf_train = dd.read_parquet(\n",
    "\tDATASET_PATH,\n",
    "\tengine=\"pyarrow\",\n",
    "\tcolumns=COLS_TO_READ,\n",
    "\tfilters=train_filters\n",
    ")\n",
    "\n",
    "pdf = ddf_train.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd74724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = transform_variables(pdf)\n",
    "generate_embeddings_mapping(pdf)\n",
    "train_scaler(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMLP(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tcat_cardinalities: Dict[str, int],\n",
    "\t\tcat_features: List[str],\n",
    "\t\tnum_cont_features: int,          # numéricas + indicadores de missing\n",
    "\t\tembedding_dim: int = 16,\n",
    "\t\thidden_dims: List[int] = [128, 64],\n",
    "\t\tdropout: float = 0.1,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cat_features = cat_features\n",
    "\t\tself.num_cat = len(cat_features)\n",
    "\t\tself.num_cont_features = num_cont_features\n",
    "\n",
    "\t\t# Embeddings por feature categórica\n",
    "\t\tself.embeddings = nn.ModuleDict({\n",
    "\t\t\tfeat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "\t\t\tfor feat, cardinality in cat_cardinalities.items()\n",
    "\t\t})\n",
    "\n",
    "\t\t# Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "\t\ttotal_cat_dim = embedding_dim * self.num_cat\n",
    "\t\tinput_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tprev_dim = input_dim\n",
    "\t\tfor h in hidden_dims:\n",
    "\t\t\tlayers.append(nn.Linear(prev_dim, h))\n",
    "\t\t\tlayers.append(nn.ReLU())\n",
    "\t\t\tlayers.append(nn.Dropout(dropout))\n",
    "\t\t\tprev_dim = h\n",
    "\n",
    "\t\tlayers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\"\"\"\n",
    "\t\tx: [batch, num_cat + num_cont_features]\n",
    "\t\tcolumnas en el orden:\n",
    "\t\t[CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# 1) Separar categóricas y numéricas del gran vector\n",
    "\t\tx_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "\t\tx_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "\t\t# 2) Embeddings categóricas\n",
    "\t\temb_list = []\n",
    "\t\tfor i, feat in enumerate(self.cat_features):\n",
    "\t\t\temb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "\t\t\temb_list.append(emb)\n",
    "\n",
    "\t\tif emb_list:\n",
    "\t\t\tx_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "\t\t\tx_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "\t\telse:\n",
    "\t\t\tx_input = x_cont\n",
    "\n",
    "\t\tout = self.mlp(x_input)   # [batch, 1]\n",
    "\t\treturn out.squeeze(-1)   \n",
    "\t\n",
    "VERSION = \"v1\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4096\n",
    "LEARNING_RATE = 0.005\n",
    "CHECKPOINT_PATH = \"model_checkpoint_\" + VERSION\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# AMP = False\n",
    "\n",
    "cat_cardinalities = {\n",
    "\tfeat: len(EMBEDDING_MAPPINGS[feat])\n",
    "\tfor feat in CATEGORICAL_FEATURES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40740b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\tAMP = False\n",
    "\n",
    "\t\n",
    "\tprint(f\"Using device: {device}\")\n",
    "\tif not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "\t\tAMP = False\n",
    "\t\n",
    "\ttrain_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "\tval_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "\t\n",
    "\tddf_train = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=train_filters\n",
    "\t)\n",
    "\n",
    "\tddf_val = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=val_filters\n",
    "\t)\n",
    "\n",
    "\tval_pdf = ddf_val.compute()\n",
    "\tval_pdf = process_partition(val_pdf)\n",
    "\n",
    "\tval_features_tensor = torch.tensor(\n",
    "\t\tval_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\t\n",
    "\tval_target_tensor = torch.tensor(\n",
    "\t\tval_pdf[TARGET].values,\n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\n",
    "\tval_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "\tval_loader = DataLoader(dataset=val_dataset, \n",
    "\t\t\t\t\t\t\t batch_size=BATCH_SIZE, \n",
    "\t\t\t\t\t\t\t num_workers=0,\n",
    "\t\t\t\t\t\t\t pin_memory=True,\n",
    "\t\t\t\t\t\t\t shuffle=False)\n",
    "\n",
    "\tcat_cardinalities = {\n",
    "\t\tfeat: len(EMBEDDING_MAPPINGS[feat])\n",
    "\t\tfor feat in CATEGORICAL_FEATURES\n",
    "\t}\n",
    "\tmodel = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "\tcriterion = nn.MSELoss()\n",
    "\toptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\tscaler = torch.amp.GradScaler(device=\"cuda\", enabled=AMP)\n",
    "\n",
    "\tfor epoch in range(EPOCHS):\n",
    "\t\tprint(f\"\\n--- Starting Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "\t\t\n",
    "\t\tmodel.train()\n",
    "\t\t\n",
    "\t\tpartition_iterator = ddf_train.to_delayed()\n",
    "\t\t\n",
    "\t\tfor partition_idx, partition in enumerate(partition_iterator):\n",
    "\t\t\ttrain_loss_sum = 0.0\n",
    "\n",
    "\t\t\tpdf = partition.compute()\n",
    "\t\t\t\n",
    "\t\t\tif pdf.empty:\n",
    "\t\t\t\tprint(f\"  Skipping empty partition {partition_idx+1}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "\t\t\tpdf = process_partition(pdf)\n",
    "\t\t\t\n",
    "\t\t\t# 2. Convert Pandas DF to PyTorch Tensors\n",
    "\t\t\tfeatures_tensor = torch.tensor(\n",
    "\t\t\t\tpdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "\t\t\t\tdtype=torch.float32\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\ttarget_tensor = torch.tensor(\n",
    "\t\t\t\tpdf[TARGET].values,\n",
    "\t\t\t\tdtype=torch.float32\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\tpartition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "\t\t\tpartition_loader = DataLoader(dataset=partition_dataset, \n",
    "\t\t\t\t\t\t\t\t\t\t  batch_size=BATCH_SIZE, \n",
    "\t\t\t\t\t\t\t\t\t\t  num_workers=0,\n",
    "\t\t\t\t\t\t\t\t\t\t  pin_memory=True,\n",
    "\t\t\t\t\t\t\t\t\t\t  shuffle=True)\n",
    "\n",
    "\t\t\tfor batch_idx, (data, target) in enumerate(partition_loader):\n",
    "\n",
    "\t\t\t\tdata, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\t\t\t\t\n",
    "\t\t\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "\t\t\t\t\toutput = model(data)\n",
    "\t\t\t\t\tloss = criterion(output, target)\n",
    "\n",
    "\t\t\t\tif AMP:\n",
    "\t\t\t\t\tscaler.scale(loss).backward()\n",
    "\t\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\t\tscaler.update()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\ttrain_loss_sum += loss.item()\n",
    "\n",
    "\t\t\tprint(f\"  Epoch {epoch+1} | Processed Partition {partition_idx+1} | Partition Train Loss: {train_loss_sum / (len(partition_loader)):.4f}\")\n",
    "\n",
    "\t\tval_loss_sum = 0.0\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor batch_idx, (data, target) in enumerate(val_loader):\n",
    "\t\t\t\tdata, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "\t\t\t\t\toutput = model(data)\n",
    "\t\t\t\t\tloss = criterion(output, target)\n",
    "\n",
    "\t\t\t\tval_loss_sum += loss.item()\n",
    "\t\tprint(f\"\\n--- Epoch {epoch + 1} Completed ---\")\n",
    "\t\tprint(f\"Validation Loss: {val_loss_sum / (len(val_loader)):.4f}\")\n",
    "\n",
    "\t\t# Save checkpoint\n",
    "\t\ttorch.save(model.state_dict(), f\"{CHECKPOINT_PATH}_{epoch+1}.pth\")\n",
    "\t\tprint(f\"Model checkpoint saved to {CHECKPOINT_PATH}_{epoch+1}.pth\")\n",
    "\n",
    "\tprint(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0c9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
