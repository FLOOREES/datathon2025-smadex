{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5756d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 453\u001b[39m\n\u001b[32m    446\u001b[39m     EMBEDDING_MAPPINGS = json.load(f)\n\u001b[32m    448\u001b[39m cat_cardinalities = {\n\u001b[32m    449\u001b[39m     feat: \u001b[38;5;28mlen\u001b[39m(EMBEDDING_MAPPINGS[feat])\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m CATEGORICAL_FEATURES\n\u001b[32m    451\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m model = \u001b[43mEmbeddingMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# o EmbeddingMLP / FTTransformer\u001b[39;49;00m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATEGORICAL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_cont_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_cont_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# pon los mismos que en train\u001b[39;49;00m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m)\u001b[49m.to(device)\n\u001b[32m    462\u001b[39m state = torch.load(CHECKPOINT_PATH, map_location=device)\n\u001b[32m    463\u001b[39m model.load_state_dict(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:485\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    486\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    492\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'"
     ]
    }
   ],
   "source": [
    "# %% imports básicos\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from typing import List, Dict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# %% paths y config (ajusta a tu setup)\n",
    "DATASET_PATH = \"./data/train\"\n",
    "CHECKPOINT_PATH = \"model_finetuned_train_val.pth\"   # <-- pon aquí tu checkpoint final\n",
    "\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "TARGET_BIN = \"buy_d7_binary\"        # 0/1 compra\n",
    "TARGET_LOG = \"iap_revenue_d7_log\"  \n",
    "\n",
    "def load_embeddings_mapping():\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "\t\tmappings = json.load(f)\n",
    "\treturn mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = [\n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)\n",
    "\n",
    "\n",
    "def process_partition(df):\n",
    "\tdf = transform_variables(df)\n",
    "\tdf = scale_numerical_features(df, loaded_scaler)\n",
    "\tdf = impute_missings(df)\n",
    "\treturn df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "\t# Check if the input is a list\n",
    "\tif isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "\t\t# Extract all numeric timestamps from the tuples\n",
    "\t\ttimestamps = []\n",
    "\t\tfor t in tuples_list:\n",
    "\t\t\tif isinstance(t, tuple) and len(t) == 2:\n",
    "\t\t\t\tts = t[1]\n",
    "\t\t\t\tif ts is not None and not isinstance(ts, list):\n",
    "\t\t\t\t\ttimestamps.append(ts)\n",
    "\t\tif len(timestamps) > 0:\n",
    "\t\t\t# Use the largest timestamp (closest to now)\n",
    "\t\t\tmax_ts = max(timestamps)\n",
    "\t\t\treturn (now_ts - max_ts) / 3600  # seconds → hours\n",
    "\treturn np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "\t\"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "\tif isinstance(tuple_list, list):\n",
    "\t\treturn [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "\treturn []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "\t\"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "\tif not values:\n",
    "\t\treturn np.nan\n",
    "\tif mode == \"sum\":\n",
    "\t\treturn sum(values)\n",
    "\treturn sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "\t \n",
    "\ttry:\n",
    "\n",
    "\t\t\tnow = pd.Timestamp.now()\n",
    "\t\t\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\t\t\tmin_ts = 0                  # 1970-01-01\n",
    "\t\t\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t\t\t# Replace invalid timestamps with NaN\n",
    "\t\t\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\t\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t\t\t)\n",
    "\t\t\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\t\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Convert safe Unix timestamps to datetime\n",
    "\t\t\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\t\t\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t\t\t# Compute hours ago\n",
    "\t\t\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\t\t\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t\t\t# Drop the original Unix timestamp columns\n",
    "\t\t\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t# Convert 'hour' from string to integer\n",
    "\t\t\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t\t\t# Convert hour to radians (full circle = 24 hours)\n",
    "\t\t\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t\t\t# Create two new columns: sin_hour and cos_hour\n",
    "\t\t\tdf['hour_sin'] = np.sin(radians)\n",
    "\t\t\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t\t\t# Drop the original 'hour' column\n",
    "\t\t\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t# Ensure `now_ts` is a Unix timestamp\n",
    "\t\t\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t\t\t# Apply to your 4 timestamp columns\n",
    "\t\t\tts_cols = [\n",
    "\t\t\t\t\"first_request_ts_bundle\",\n",
    "\t\t\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\t\t\"last_buy_ts_category\",\n",
    "\t\t\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\tfor col in ts_cols:\n",
    "\t\t\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\t\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t\t\t# Drop the original tuple columns if you want\n",
    "\t\t\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\trules = {\n",
    "\t\t\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\t\t\"ctr\": \"sum\",\n",
    "\t\t\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tfor col, mode in rules.items():\n",
    "\t\t\t\tnew_col = col + \"_agg\"\n",
    "\t\t\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t\t\t# If the df contains the target column, transform it with the logarithm\n",
    "\t\t\tif TARGET in df.columns:\n",
    "\t\t\t\t# 1) binario de compra\n",
    "\t\t\t\tdf[TARGET_BIN] = (df[TARGET] > 0).astype(float)\n",
    "\n",
    "\t\t\t\t# 2) log1p del importe (clip por si acaso)\n",
    "\t\t\t\tdf[TARGET_LOG] = np.log1p(df[TARGET].clip(lower=0))\n",
    "\n",
    "\t# Remove all the variables and create the new ones by setting them to the default values\n",
    "\texcept Exception:\n",
    "\t\t# If something failed, remove any partially created/intermediate columns\n",
    "\t\tts_cols = [\n",
    "\t\t\t\"first_request_ts_bundle\",\n",
    "\t\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\t\"last_buy_ts_category\",\n",
    "\t\t\t\"user_actions_bundles_action_last_timestamp\",\n",
    "\t\t]\n",
    "\n",
    "\t\trules_keys = [\n",
    "\t\t\t\"iap_revenue_usd_bundle\",\n",
    "\t\t\t\"num_buys_bundle\",\n",
    "\t\t\t\"rev_by_adv\",\n",
    "\t\t\t\"rwd_prank\",\n",
    "\t\t\t\"whale_users_bundle_num_buys_prank\",\n",
    "\t\t\t\"whale_users_bundle_revenue_prank\",\n",
    "\t\t\t\"whale_users_bundle_total_num_buys\",\n",
    "\t\t\t\"whale_users_bundle_total_revenue\",\n",
    "\t\t\t\"avg_daily_sessions\",\n",
    "\t\t\t\"avg_duration\",\n",
    "\t\t\t\"bcat_bottom_taxonomy\",\n",
    "\t\t\t\"ctr\",\n",
    "\t\t\t\"bundles_cat_bottom_taxonomy\",\n",
    "\t\t]\n",
    "\n",
    "\t\tintermediate_cols = (\n",
    "\t\t\t[\n",
    "\t\t\t\t\"last_buy\",\n",
    "\t\t\t\t\"last_ins\",\n",
    "\t\t\t\t\"last_buy_safe\",\n",
    "\t\t\t\t\"last_ins_safe\",\n",
    "\t\t\t\t\"last_buy_dt\",\n",
    "\t\t\t\t\"last_ins_dt\",\n",
    "\t\t\t\t\"hour\",\n",
    "\t\t\t\t\"hours_since_last_buy\",\n",
    "\t\t\t\t\"hours_since_last_ins\",\n",
    "\t\t\t\t\"hour_sin\",\n",
    "\t\t\t\t\"hour_cos\",\n",
    "\t\t\t]\n",
    "\t\t\t+ [c + \"_hours_ago\" for c in ts_cols]\n",
    "\t\t\t+ [k + \"_agg\" for k in rules_keys]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Drop any of those columns if they exist (silent if not)\n",
    "\t\tdf.drop(columns=[c for c in intermediate_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\t\t# Create all expected features with default (NaN) values so subsequent steps won't fail\n",
    "\t\tfor col in ALL_FEATURES:\n",
    "\t\t\tif col not in df.columns:\n",
    "\t\t\t\tdf[col] = np.nan\n",
    "\n",
    "\t\t# Ensure target exists (set to NaN)\n",
    "\t\tif TARGET not in df.columns:\n",
    "\t\t\tdf[TARGET] = np.nan\n",
    "\n",
    "\t\t# NUEVO: asegurarnos de que también existen los targets derivados\n",
    "\t\tif TARGET_BIN not in df.columns:\n",
    "\t\t\tdf[TARGET_BIN] = np.nan\n",
    "\n",
    "\t\tif TARGET_LOG not in df.columns:\n",
    "\t\t\tdf[TARGET_LOG] = np.nan\n",
    "\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "\tfor col in CATEGORICAL_FEATURES:\n",
    "\t\tdf[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "\t\t# convert to embeddings indices (if category not seen before, assign index 0)\n",
    "\t\tmapping = EMBEDDING_MAPPINGS[col]\n",
    "\t\tdf[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "\tfor col in NUMERICAL_FEATURES:\n",
    "\t\t# 1. Create missing indicator\n",
    "\t\tdf[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "\t\t# 2. Impute missings with 0\n",
    "\t\tdf[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "\tdf[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "\treturn df\n",
    "\n",
    "def train_scaler(df):\n",
    "\tscaler = StandardScaler()\n",
    "\tscaler.fit(df[NUMERICAL_FEATURES])\n",
    "\tjoblib.dump(scaler, SCALER_FILE)\n",
    "\tprint(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "\t# Use after transform_variables but BEFORE imputing missings\n",
    "\t\n",
    "\tmappings = {}\n",
    "\tfor feature in CATEGORICAL_FEATURES:\n",
    "\t\tunique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "\t\tembeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "\t\tmappings[feature] = embeddings\n",
    "\t\n",
    "\t# Save mappings to disk\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "\t\tjson.dump(mappings, f)\n",
    "\n",
    "\n",
    "cat_cardinalities = {\n",
    "    feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "    for feat in CATEGORICAL_FEATURES\n",
    "}\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def _init_(\n",
    "        self,\n",
    "        cat_cardinalities,\n",
    "        cat_features: List[str],\n",
    "        num_cont_features: int,          # numéricas + indicadores de missing\n",
    "        embedding_dim: int = 64,\n",
    "        hidden_dims: List[int] = [1024, 512, 256, 128, 64],\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super()._init_()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_cat = len(cat_features)\n",
    "        self.num_cont_features = num_cont_features\n",
    "\n",
    "        # Embeddings por feature categórica\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "            for feat, cardinality in cat_cardinalities.items()\n",
    "        })\n",
    "\n",
    "        # Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "        total_cat_dim = embedding_dim * self.num_cat\n",
    "        input_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, num_cat + num_cont_features]\n",
    "        columnas en el orden:\n",
    "        [CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Separar categóricas y numéricas del gran vector\n",
    "        x_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "        x_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "        # 2) Embeddings categóricas\n",
    "        emb_list = []\n",
    "        for i, feat in enumerate(self.cat_features):\n",
    "            emb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "            emb_list.append(emb)\n",
    "\n",
    "        if emb_list:\n",
    "            x_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "            x_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "        else:\n",
    "            x_input = x_cont\n",
    "\n",
    "        out = self.mlp(x_input)   # [batch, 1]\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "# %% aquí deberías importar tu pipeline real:\n",
    "# from tu_modulo import process_partition, DeepEmbeddingMLP  (o EmbeddingMLP/FTTransformer)\n",
    "#\n",
    "# Para que el ejemplo sea autocontenido, asumo que process_partition y el modelo\n",
    "# ya los tienes definidos igual que en el notebook de entrenamiento.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# %% helper para construir el tensor de features en el mismo orden que en train\n",
    "def make_features_tensor(pdf: pd.DataFrame) -> torch.Tensor:\n",
    "    return torch.tensor(\n",
    "        pdf[\n",
    "            CATEGORICAL_FEATURES\n",
    "            + NUMERICAL_FEATURES\n",
    "            + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "        ].values,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "# %% cargar validación (mismos filtros que en entrenamiento)\n",
    "val_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "\n",
    "ddf_val = dd.read_parquet(\n",
    "    DATASET_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=COLS_TO_READ,   # usa la misma COLS_TO_READ del train\n",
    "    filters=val_filters,\n",
    ")\n",
    "\n",
    "val_pdf = ddf_val.compute()\n",
    "val_pdf = process_partition(val_pdf)\n",
    "\n",
    "# y_true en escala original\n",
    "y_true = val_pdf[TARGET].values.copy()\n",
    "\n",
    "# si en el pipeline transformaste TARGET a log1p, asegúrate de que aquí\n",
    "# estás usando la columna original sin log; si no la preservaste, mala decisión,\n",
    "# pero entonces simplemente evalúas en escala log.\n",
    "\n",
    "X_val = make_features_tensor(val_pdf)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, torch.tensor(y_true, dtype=torch.float32))\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4096,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# %% reconstruir el modelo y cargar el checkpoint\n",
    "# usa la MISMA arquitectura y parámetros que en entrenamiento\n",
    "num_cont_features = len(NUMERICAL_FEATURES) * 2\n",
    "\n",
    "with open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "    EMBEDDING_MAPPINGS = json.load(f)\n",
    "\n",
    "cat_cardinalities = {\n",
    "    feat: len(EMBEDDING_MAPPINGS[feat])\n",
    "    for feat in CATEGORICAL_FEATURES\n",
    "}\n",
    "\n",
    "model = EmbeddingMLP(   # o EmbeddingMLP / FTTransformer\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    cat_features=CATEGORICAL_FEATURES,\n",
    "    num_cont_features=num_cont_features,\n",
    "    embedding_dim=64,            # pon los mismos que en train\n",
    "    hidden_dims=[512, 256, 128, 64],\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "# %% obtener predicciones en validación\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (data, y_batch) in val_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "\n",
    "        pred = model(data)    # [batch], normalmente en log1p\n",
    "        all_preds.append(pred.cpu())\n",
    "\n",
    "y_pred = torch.cat(all_preds).numpy()\n",
    "\n",
    "# si el modelo predice en log1p, deshaz la transformación para comparar en escala real\n",
    "if TARGET_LOG:\n",
    "    y_pred_real = np.expm1(y_pred)\n",
    "else:\n",
    "    y_pred_real = y_pred\n",
    "\n",
    "# alineamos por si acaso\n",
    "y_true = y_true[: len(y_pred_real)]\n",
    "\n",
    "print(\"y_true mean:\", y_true.mean())\n",
    "print(\"y_pred mean:\", y_pred_real.mean())\n",
    "\n",
    "# %% gráfico: y_pred vs y_true\n",
    "\n",
    "# para que el gráfico no sea un borrón, muestramos si hay demasiados puntos\n",
    "max_points = 50000\n",
    "n = len(y_true)\n",
    "if n > max_points:\n",
    "    idx = np.random.choice(n, size=max_points, replace=False)\n",
    "    y_true_plot = y_true[idx]\n",
    "    y_pred_plot = y_pred_real[idx]\n",
    "else:\n",
    "    y_true_plot = y_true\n",
    "    y_pred_plot = y_pred_real\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# scatter denso\n",
    "plt.scatter(y_true_plot, y_pred_plot, alpha=0.3, s=5)\n",
    "\n",
    "# línea ideal y = x\n",
    "min_val = min(y_true_plot.min(), y_pred_plot.min())\n",
    "max_val = max(y_true_plot.max(), y_pred_plot.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], \"k--\", linewidth=1)\n",
    "\n",
    "plt.xlabel(\"Real iap_revenue_d7\")\n",
    "plt.ylabel(\"Predicho iap_revenue_d7\")\n",
    "plt.title(\"Validación: predicción vs valor real\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon2025-smadex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
