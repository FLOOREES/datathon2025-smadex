{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3cf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pablo/Documents/datathon2025-smadex\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: '/home/pablo/Documents/datathon2025-smadex\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     sys.path.insert(\u001b[32m0\u001b[39m, PROJECT_ROOT)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPROJECT_ROOT:\u001b[39m\u001b[33m\"\u001b[39m, PROJECT_ROOT)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mContenido de ./data:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] El sistema no puede encontrar la ruta especificada: '/home/pablo/Documents/datathon2025-smadex\\\\data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/home/pablo/Documents/datathon2025-smadex\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\tsys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa02e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\n",
      "Contenido de ./data: ['sample_submission.csv', 'split', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.getcwd() \n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\tsys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Contenido de ./data:\", os.listdir(os.path.join(PROJECT_ROOT, \"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197f0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x1a64d6aafc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Dict, List\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"./data/train\"\n",
    "EMBEDDINGS_MAPPING_FILE = \"embeddings_mappings.json\"\n",
    "SCALER_FILE = \"scaler.joblib\"\n",
    "\n",
    "CATEGORICAL_FEATURES = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'advertiser_bottom_taxonomy_level', 'country', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'release_date']\n",
    "NUMERICAL_FEATURES = ['release_msrp', 'weekday', 'avg_act_days', 'weekend_ratio', 'weeks_since_first_seen', 'wifi_ratio', 'hours_since_last_buy', 'hours_since_last_ins', 'hour_sin', 'hour_cos', 'first_request_ts_bundle_hours_ago', 'last_buy_ts_bundle_hours_ago', 'last_buy_ts_category_hours_ago', 'user_actions_bundles_action_last_timestamp_hours_ago', 'iap_revenue_usd_bundle_agg', 'num_buys_bundle_agg', 'rev_by_adv_agg', 'rwd_prank_agg', 'whale_users_bundle_num_buys_prank_agg', 'whale_users_bundle_revenue_prank_agg', 'whale_users_bundle_total_num_buys_agg', 'whale_users_bundle_total_revenue_agg', 'avg_daily_sessions_agg', 'avg_duration_agg', 'bcat_bottom_taxonomy_agg', 'ctr_agg', 'bundles_cat_bottom_taxonomy_agg']\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "TARGET_BIN = \"buy_d7_binary\"        # 0/1 compra\n",
    "TARGET_LOG = \"iap_revenue_d7_log\"  \n",
    "\n",
    "def load_embeddings_mapping():\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"r\") as f:\n",
    "\t\tmappings = json.load(f)\n",
    "\treturn mappings\n",
    "\n",
    "EMBEDDING_MAPPINGS = load_embeddings_mapping()\n",
    "COLS_TO_READ = [\n",
    "\t\t\t 'iap_revenue_d7', \n",
    "\t\t\t 'advertiser_bundle', \n",
    "\t\t\t 'advertiser_category', \n",
    "\t\t\t 'advertiser_subcategory', \n",
    "\t\t\t 'advertiser_bottom_taxonomy_level', \n",
    "\t\t\t 'country', \n",
    "\t\t\t 'dev_make', \n",
    "\t\t\t 'dev_model', \n",
    "\t\t\t 'dev_os', \n",
    "\t\t\t 'dev_osv', \n",
    "\t\t\t 'hour', \n",
    "\t\t\t 'release_date', \n",
    "\t\t\t 'release_msrp', \n",
    "\t\t\t 'weekday', \n",
    "\t\t\t 'avg_act_days', \n",
    "\t\t\t 'avg_daily_sessions', \n",
    "\t\t\t 'avg_duration', \n",
    "\t\t\t 'bcat_bottom_taxonomy', \n",
    "\t\t\t 'bundles_cat_bottom_taxonomy',  \n",
    "\t\t\t 'ctr',  \n",
    "\t\t\t 'first_request_ts_bundle', \n",
    "\t\t\t 'iap_revenue_usd_bundle', \n",
    "\t\t\t 'last_buy', \n",
    "\t\t\t 'last_buy_ts_bundle', \n",
    "\t\t\t 'last_buy_ts_category', \n",
    "\t\t\t 'last_ins', \n",
    "\t\t\t 'user_actions_bundles_action_last_timestamp', \n",
    "\t\t\t 'num_buys_bundle', \n",
    "\t\t\t 'rev_by_adv', \n",
    "\t\t\t 'rwd_prank', \n",
    "\t\t\t 'weekend_ratio', \n",
    "\t\t\t 'weeks_since_first_seen', \n",
    "\t\t\t 'wifi_ratio', \n",
    "\t\t\t 'whale_users_bundle_num_buys_prank', \n",
    "\t\t\t 'whale_users_bundle_revenue_prank', \n",
    "\t\t\t 'whale_users_bundle_total_num_buys', \n",
    "\t\t\t 'whale_users_bundle_total_revenue']\n",
    "\n",
    "loaded_scaler = joblib.load(SCALER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "\tdf = transform_variables(df)\n",
    "\tdf = scale_numerical_features(df, loaded_scaler)\n",
    "\tdf = impute_missings(df)\n",
    "\treturn df\n",
    "\n",
    "def hours_since_now_from_list(tuples_list, now_ts):\n",
    "\t# Check if the input is a list\n",
    "\tif isinstance(tuples_list, list) and len(tuples_list) > 0:\n",
    "\t\t# Extract all numeric timestamps from the tuples\n",
    "\t\ttimestamps = []\n",
    "\t\tfor t in tuples_list:\n",
    "\t\t\tif isinstance(t, tuple) and len(t) == 2:\n",
    "\t\t\t\tts = t[1]\n",
    "\t\t\t\tif ts is not None and not isinstance(ts, list):\n",
    "\t\t\t\t\ttimestamps.append(ts)\n",
    "\t\tif len(timestamps) > 0:\n",
    "\t\t\t# Use the largest timestamp (closest to now)\n",
    "\t\t\tmax_ts = max(timestamps)\n",
    "\t\t\treturn (now_ts - max_ts) / 3600  # seconds → hours\n",
    "\treturn np.nan\n",
    "\n",
    "def extract_numbers(tuple_list):\n",
    "\t\"\"\"Extract only the numeric part from a list of (id, value) tuples.\"\"\"\n",
    "\tif isinstance(tuple_list, list):\n",
    "\t\treturn [t[1] for t in tuple_list if isinstance(t, tuple) and len(t) >= 2]\n",
    "\treturn []\n",
    "\n",
    "def aggregate(values, mode):\n",
    "\t\"\"\"Apply either sum or mean depending on mode.\"\"\"\n",
    "\tif not values:\n",
    "\t\treturn np.nan\n",
    "\tif mode == \"sum\":\n",
    "\t\treturn sum(values)\n",
    "\treturn sum(values) / len(values)\n",
    "\n",
    "def transform_variables(df):\n",
    "\t \n",
    "\ttry:\n",
    "\n",
    "\t\t\tnow = pd.Timestamp.now()\n",
    "\t\t\t# Define reasonable bounds for Unix timestamps (seconds)\n",
    "\t\t\tmin_ts = 0                  # 1970-01-01\n",
    "\t\t\tmax_ts = 4102444800         # 2100-01-01 in Unix seconds\n",
    "\n",
    "\t\t\t# Replace invalid timestamps with NaN\n",
    "\t\t\tdf[\"last_buy_safe\"] = df[\"last_buy\"].where(\n",
    "\t\t\t\tdf[\"last_buy\"].between(min_ts, max_ts), np.nan\n",
    "\t\t\t)\n",
    "\t\t\tdf[\"last_ins_safe\"] = df[\"last_ins\"].where(\n",
    "\t\t\t\tdf[\"last_ins\"].between(min_ts, max_ts), np.nan\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Convert safe Unix timestamps to datetime\n",
    "\t\t\tdf[\"last_buy_dt\"] = pd.to_datetime(df[\"last_buy_safe\"], unit=\"s\")\n",
    "\t\t\tdf[\"last_ins_dt\"] = pd.to_datetime(df[\"last_ins_safe\"], unit=\"s\")\n",
    "\n",
    "\t\t\t# Compute hours ago\n",
    "\t\t\tdf[\"hours_since_last_buy\"] = (now - df[\"last_buy_dt\"]).dt.total_seconds() / 3600\n",
    "\t\t\tdf[\"hours_since_last_ins\"] = (now - df[\"last_ins_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "\t\t\t# Drop the original Unix timestamp columns\n",
    "\t\t\tdf = df.drop(columns=[\"last_buy\", \"last_ins\", \"last_buy_safe\", \"last_ins_dt\", \"last_buy_dt\", \"last_ins_safe\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t# Convert 'hour' from string to integer\n",
    "\t\t\tdf['hour'] = df['hour'].astype(int)\n",
    "\n",
    "\t\t\t# Convert hour to radians (full circle = 24 hours)\n",
    "\t\t\tradians = df['hour'] * (2 * np.pi / 24)\n",
    "\n",
    "\t\t\t# Create two new columns: sin_hour and cos_hour\n",
    "\t\t\tdf['hour_sin'] = np.sin(radians)\n",
    "\t\t\tdf['hour_cos'] = np.cos(radians)\n",
    "\n",
    "\t\t\t# Drop the original 'hour' column\n",
    "\t\t\tdf.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t# Ensure `now_ts` is a Unix timestamp\n",
    "\t\t\tnow_ts = int(pd.Timestamp.now().timestamp())\n",
    "\n",
    "\t\t\t# Apply to your 4 timestamp columns\n",
    "\t\t\tts_cols = [\n",
    "\t\t\t\t\"first_request_ts_bundle\",\n",
    "\t\t\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\t\t\"last_buy_ts_category\",\n",
    "\t\t\t\t\"user_actions_bundles_action_last_timestamp\"\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\tfor col in ts_cols:\n",
    "\t\t\t\tnew_col = col + \"_hours_ago\"\n",
    "\t\t\t\tdf[new_col] = df[col].apply(lambda x: hours_since_now_from_list(x, now_ts))\n",
    "\n",
    "\t\t\t# Drop the original tuple columns if you want\n",
    "\t\t\tdf.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\trules = {\n",
    "\t\t\t\t\"iap_revenue_usd_bundle\": \"sum\",\n",
    "\t\t\t\t\"num_buys_bundle\": \"sum\",\n",
    "\t\t\t\t\"rev_by_adv\": \"sum\",\n",
    "\t\t\t\t\"rwd_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_num_buys_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_revenue_prank\": \"mean\",\n",
    "\t\t\t\t\"whale_users_bundle_total_num_buys\": \"sum\",\n",
    "\t\t\t\t\"whale_users_bundle_total_revenue\": \"sum\",\n",
    "\t\t\t\t\"avg_daily_sessions\": \"mean\",\n",
    "\t\t\t\t\"avg_duration\": \"mean\",\n",
    "\t\t\t\t\"bcat_bottom_taxonomy\": \"mean\",\n",
    "\t\t\t\t\"ctr\": \"sum\",\n",
    "\t\t\t\t\"bundles_cat_bottom_taxonomy\": \"sum\",\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tfor col, mode in rules.items():\n",
    "\t\t\t\tnew_col = col + \"_agg\"\n",
    "\t\t\t\tdf[new_col] = df[col].apply(\n",
    "\t\t\t\t\tlambda lst: aggregate(extract_numbers(lst), mode)\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tdf.drop(columns=list(rules.keys()), inplace=True)\n",
    "\n",
    "\t\t\t# If the df contains the target column, transform it with the logarithm\n",
    "\t\t\tif TARGET in df.columns:\n",
    "\t\t\t\t# 1) binario de compra\n",
    "\t\t\t\tdf[TARGET_BIN] = (df[TARGET] > 0).astype(float)\n",
    "\n",
    "\t\t\t\t# 2) log1p del importe (clip por si acaso)\n",
    "\t\t\t\tdf[TARGET_LOG] = np.log1p(df[TARGET].clip(lower=0))\n",
    "\n",
    "\t# Remove all the variables and create the new ones by setting them to the default values\n",
    "\texcept Exception:\n",
    "\t\t# If something failed, remove any partially created/intermediate columns\n",
    "\t\tts_cols = [\n",
    "\t\t\t\"first_request_ts_bundle\",\n",
    "\t\t\t\"last_buy_ts_bundle\",\n",
    "\t\t\t\"last_buy_ts_category\",\n",
    "\t\t\t\"user_actions_bundles_action_last_timestamp\",\n",
    "\t\t]\n",
    "\n",
    "\t\trules_keys = [\n",
    "\t\t\t\"iap_revenue_usd_bundle\",\n",
    "\t\t\t\"num_buys_bundle\",\n",
    "\t\t\t\"rev_by_adv\",\n",
    "\t\t\t\"rwd_prank\",\n",
    "\t\t\t\"whale_users_bundle_num_buys_prank\",\n",
    "\t\t\t\"whale_users_bundle_revenue_prank\",\n",
    "\t\t\t\"whale_users_bundle_total_num_buys\",\n",
    "\t\t\t\"whale_users_bundle_total_revenue\",\n",
    "\t\t\t\"avg_daily_sessions\",\n",
    "\t\t\t\"avg_duration\",\n",
    "\t\t\t\"bcat_bottom_taxonomy\",\n",
    "\t\t\t\"ctr\",\n",
    "\t\t\t\"bundles_cat_bottom_taxonomy\",\n",
    "\t\t]\n",
    "\n",
    "\t\tintermediate_cols = (\n",
    "\t\t\t[\n",
    "\t\t\t\t\"last_buy\",\n",
    "\t\t\t\t\"last_ins\",\n",
    "\t\t\t\t\"last_buy_safe\",\n",
    "\t\t\t\t\"last_ins_safe\",\n",
    "\t\t\t\t\"last_buy_dt\",\n",
    "\t\t\t\t\"last_ins_dt\",\n",
    "\t\t\t\t\"hour\",\n",
    "\t\t\t\t\"hours_since_last_buy\",\n",
    "\t\t\t\t\"hours_since_last_ins\",\n",
    "\t\t\t\t\"hour_sin\",\n",
    "\t\t\t\t\"hour_cos\",\n",
    "\t\t\t]\n",
    "\t\t\t+ [c + \"_hours_ago\" for c in ts_cols]\n",
    "\t\t\t+ [k + \"_agg\" for k in rules_keys]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Drop any of those columns if they exist (silent if not)\n",
    "\t\tdf.drop(columns=[c for c in intermediate_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\t\t# Create all expected features with default (NaN) values so subsequent steps won't fail\n",
    "\t\tfor col in ALL_FEATURES:\n",
    "\t\t\tif col not in df.columns:\n",
    "\t\t\t\tdf[col] = np.nan\n",
    "\n",
    "\t\t# Ensure target exists (set to NaN)\n",
    "\t\tif TARGET not in df.columns:\n",
    "\t\t\tdf[TARGET] = np.nan\n",
    "\n",
    "\t\t# NUEVO: asegurarnos de que también existen los targets derivados\n",
    "\t\tif TARGET_BIN not in df.columns:\n",
    "\t\t\tdf[TARGET_BIN] = np.nan\n",
    "\n",
    "\t\tif TARGET_LOG not in df.columns:\n",
    "\t\t\tdf[TARGET_LOG] = np.nan\n",
    "\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def impute_missings(df):\n",
    "\n",
    "\tfor col in CATEGORICAL_FEATURES:\n",
    "\t\tdf[col] = df[col].fillna(\"<MISSING>\")\n",
    "\n",
    "\t\t# convert to embeddings indices (if category not seen before, assign index 0)\n",
    "\t\tmapping = EMBEDDING_MAPPINGS[col]\n",
    "\t\tdf[col] = df[col].map(lambda x: mapping.get(x, 0))\n",
    "\n",
    "\tfor col in NUMERICAL_FEATURES:\n",
    "\t\t# 1. Create missing indicator\n",
    "\t\tdf[f\"{col}_is_missing\"] = df[col].isna().astype(float)\n",
    "\n",
    "\t\t# 2. Impute missings with 0\n",
    "\t\tdf[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def scale_numerical_features(df, scaler):\n",
    "\tdf[NUMERICAL_FEATURES] = scaler.transform(df[NUMERICAL_FEATURES])\n",
    "\treturn df\n",
    "\n",
    "def train_scaler(df):\n",
    "\tscaler = StandardScaler()\n",
    "\tscaler.fit(df[NUMERICAL_FEATURES])\n",
    "\tjoblib.dump(scaler, SCALER_FILE)\n",
    "\tprint(\"Scaler saved.\")\n",
    "\n",
    "def generate_embeddings_mapping(pdf):\n",
    "\t# Use after transform_variables but BEFORE imputing missings\n",
    "\t\n",
    "\tmappings = {}\n",
    "\tfor feature in CATEGORICAL_FEATURES:\n",
    "\t\tunique_values = [\"<MISSING>\"] + pdf[feature].dropna().unique().tolist()\n",
    "\t\tembeddings = {val: idx for idx, val in enumerate(unique_values)}\n",
    "\t\tmappings[feature] = embeddings\n",
    "\t\n",
    "\t# Save mappings to disk\n",
    "\twith open(EMBEDDINGS_MAPPING_FILE, \"w\") as f:\n",
    "\t\tjson.dump(mappings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v1_f\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8192\n",
    "LEARNING_RATE = 0.005\n",
    "CHECKPOINT_PATH = \"model_checkpoint_\" + VERSION\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# AMP = False\n",
    "\n",
    "num_cont_features = len(NUMERICAL_FEATURES) + len(NUMERICAL_FEATURES)  # numéricas + _is_missing\n",
    "\n",
    "cat_cardinalities = {\n",
    "\tfeat: len(EMBEDDING_MAPPINGS[feat])\n",
    "\tfor feat in CATEGORICAL_FEATURES\n",
    "}\n",
    "\n",
    "class DeepEmbeddingMLP(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tcat_cardinalities: Dict[str, int],\n",
    "\t\tcat_features: List[str],\n",
    "\t\tnum_cont_features: int,          # numéricas + _is_missing\n",
    "\t\tembedding_dim: int = 32,\n",
    "\t\thidden_dims: List[int] = [512, 256, 128, 64],\n",
    "\t\tdropout: float = 0.2,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cat_features = cat_features\n",
    "\t\tself.num_cat = len(cat_features)\n",
    "\t\tself.num_cont_features = num_cont_features\n",
    "\n",
    "\t\t# Embeddings por feature categórica\n",
    "\t\tself.embeddings = nn.ModuleDict({\n",
    "\t\t\tfeat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "\t\t\tfor feat, cardinality in cat_cardinalities.items()\n",
    "\t\t})\n",
    "\n",
    "\t\ttotal_cat_dim = embedding_dim * self.num_cat\n",
    "\t\tinput_dim = total_cat_dim + self.num_cont_features\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tprev_dim = input_dim\n",
    "\t\tfor h in hidden_dims:\n",
    "\t\t\tlayers.append(nn.Linear(prev_dim, h))\n",
    "\t\t\tlayers.append(nn.ReLU())\n",
    "\t\t\tlayers.append(nn.Dropout(dropout))\n",
    "\t\t\tprev_dim = h\n",
    "\n",
    "\t\t# salida escalar (logits para clasificación o valor para regresión)\n",
    "\t\tlayers.append(nn.Linear(prev_dim, 1))\n",
    "\t\tself.mlp = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\"\"\"\n",
    "\t\tx: [batch, num_cat + num_cont_features]\n",
    "\t\tcolumnas en el orden:\n",
    "\t\t[CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "\t\t\"\"\"\n",
    "\t\t# separar cat / num\n",
    "\t\tx_cat_ids = x[:, :self.num_cat].long()  # [B, num_cat]\n",
    "\t\tx_cont = x[:, self.num_cat:].float()    # [B, num_cont]\n",
    "\n",
    "\t\t# embeddings categóricas\n",
    "\t\temb_list = []\n",
    "\t\tfor i, feat in enumerate(self.cat_features):\n",
    "\t\t\temb = self.embeddings[feat](x_cat_ids[:, i])  # [B, embedding_dim]\n",
    "\t\t\temb_list.append(emb)\n",
    "\n",
    "\t\tif emb_list:\n",
    "\t\t\tx_cat_emb = torch.cat(emb_list, dim=1)        # [B, embedding_dim * num_cat]\n",
    "\t\t\tx_in = torch.cat([x_cont, x_cat_emb], dim=1)  # [B, input_dim]\n",
    "\t\telse:\n",
    "\t\t\tx_in = x_cont\n",
    "\n",
    "\t\tout = self.mlp(x_in)          # [B, 1]\n",
    "\t\treturn out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_tensor(pdf: pd.DataFrame) -> torch.Tensor:\n",
    "\treturn torch.tensor(\n",
    "\t\tpdf[\n",
    "\t\t\tCATEGORICAL_FEATURES\n",
    "\t\t\t+ NUMERICAL_FEATURES\n",
    "\t\t\t+ [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "\t\t].values,\n",
    "\t\tdtype=torch.float32,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fde2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(\n",
    "\tstage_name: str,\n",
    "\tddf_train,\n",
    "\tddf_val,\n",
    "\ttarget_col: str,\n",
    "\tonly_buyers: bool,\n",
    "\tcriterion,\n",
    "\tcheckpoint_prefix: str,\n",
    "\tepochs: int = EPOCHS,\n",
    "\tamp: bool = False,\n",
    "\tinit_model: nn.Module | None = None,\n",
    "):\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t# --- VALIDACIÓN ---\n",
    "\tval_pdf = ddf_val.compute()\n",
    "\tval_pdf = process_partition(val_pdf)\n",
    "\tif only_buyers:\n",
    "\t\tval_pdf = val_pdf[val_pdf[TARGET_BIN] == 1].copy()\n",
    "\n",
    "\tval_x = make_features_tensor(val_pdf)\n",
    "\tval_y = torch.tensor(val_pdf[target_col].values, dtype=torch.float32)\n",
    "\n",
    "\tval_dataset = TensorDataset(val_x, val_y)\n",
    "\tval_loader = DataLoader(\n",
    "\t\tdataset=val_dataset,\n",
    "\t\tbatch_size=BATCH_SIZE,\n",
    "\t\tnum_workers=0,\n",
    "\t\tpin_memory=True,\n",
    "\t\tshuffle=False,\n",
    "\t)\n",
    "\n",
    "\t# --- MODELO ---\n",
    "\tif init_model is not None:\n",
    "\t\tmodel = init_model\n",
    "\telse:\n",
    "\t\tmodel = DeepEmbeddingMLP(\n",
    "\t\t\tcat_cardinalities=cat_cardinalities,\n",
    "\t\t\tcat_features=CATEGORICAL_FEATURES,\n",
    "\t\t\tnum_cont_features=num_cont_features,\n",
    "\t\t\tembedding_dim=32,\n",
    "\t\t\thidden_dims=[512, 256, 128, 64],\n",
    "\t\t\tdropout=0.2,\n",
    "\t\t).to(device)\n",
    "\n",
    "\toptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\tscaler = torch.amp.GradScaler(device=\"cuda\", enabled=amp)\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tprint(f\"\\n[{stage_name}] --- Epoch {epoch + 1}/{epochs} ---\")\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tpartition_iterator = ddf_train.to_delayed()\n",
    "\t\tfor partition_idx, partition in enumerate(partition_iterator):\n",
    "\t\t\ttrain_loss_sum = 0.0\n",
    "\n",
    "\t\t\tpdf = partition.compute()\n",
    "\t\t\tif pdf.empty:\n",
    "\t\t\t\tprint(f\"  [{stage_name}] Skipping empty partition {partition_idx+1}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tpdf = process_partition(pdf)\n",
    "\t\t\tif only_buyers:\n",
    "\t\t\t\tpdf = pdf[pdf[TARGET_BIN] == 1].copy()\n",
    "\t\t\t\tif pdf.empty:\n",
    "\t\t\t\t\tprint(f\"  [{stage_name}] Partition {partition_idx+1} has no buyers, skipping\")\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tx = make_features_tensor(pdf)\n",
    "\t\t\ty = torch.tensor(pdf[target_col].values, dtype=torch.float32)\n",
    "\n",
    "\t\t\tdataset = TensorDataset(x, y)\n",
    "\t\t\tloader = DataLoader(\n",
    "\t\t\t\tdataset=dataset,\n",
    "\t\t\t\tbatch_size=BATCH_SIZE,\n",
    "\t\t\t\tnum_workers=0,\n",
    "\t\t\t\tpin_memory=True,\n",
    "\t\t\t\tshuffle=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tfor batch_idx, (data, target) in enumerate(loader):\n",
    "\t\t\t\tdata, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\t\t\twith torch.amp.autocast(device_type=\"cuda\", enabled=amp):\n",
    "\t\t\t\t\tout = model(data)         # [B]\n",
    "\t\t\t\t\tloss = criterion(out, target)\n",
    "\n",
    "\t\t\t\tif amp:\n",
    "\t\t\t\t\tscaler.scale(loss).backward()\n",
    "\t\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\t\tscaler.update()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\ttrain_loss_sum += loss.item()\n",
    "\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"  [{stage_name}] Epoch {epoch+1} | Partition {partition_idx+1} \"\n",
    "\t\t\t\tf\"| Loss: {train_loss_sum / len(loader):.4f}\"\n",
    "\t\t\t)\n",
    "\n",
    "\t\t# VALIDACIÓN\n",
    "\t\tval_loss_sum = 0.0\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor data, target in val_loader:\n",
    "\t\t\t\tdata, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=\"cuda\", enabled=amp):\n",
    "\t\t\t\t\tout = model(data)\n",
    "\t\t\t\t\tloss = criterion(out, target)\n",
    "\n",
    "\t\t\t\tval_loss_sum += loss.item()\n",
    "\n",
    "\t\tprint(f\"[{stage_name}] Epoch {epoch + 1} | Val loss: {val_loss_sum / len(val_loader):.4f}\")\n",
    "\t\ttorch.save(model.state_dict(), f\"{checkpoint_prefix}_e{epoch+1}.pth\")\n",
    "\t\tprint(f\"[{stage_name}] Saved: {checkpoint_prefix}_e{epoch+1}.pth\")\n",
    "\n",
    "\tprint(f\"\\n[{stage_name}] --- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor_from_classifier_ckpt(ckpt_path: str) -> nn.Module:\n",
    "\t# Crear el modelo con la MISMA arquitectura\n",
    "\tmodel = DeepEmbeddingMLP(\n",
    "\t\tcat_cardinalities=cat_cardinalities,\n",
    "\t\tcat_features=CATEGORICAL_FEATURES,\n",
    "\t\tnum_cont_features=num_cont_features,\n",
    "\t\tembedding_dim=32,\n",
    "\t\thidden_dims=[512, 256, 128, 64],\n",
    "\t\tdropout=0.2,\n",
    "\t).to(device)\n",
    "\n",
    "\tstate = torch.load(ckpt_path, map_location=device)\n",
    "\tmodel.load_state_dict(state)\n",
    "\n",
    "\t# OPCIONAL: re-inicializar SOLO la última capa lineal (la \"head\")\n",
    "\t# para que no arrastre el sesgo de clasificación\n",
    "\tlast_layer = model.mlp[-1]\n",
    "\tif isinstance(last_layer, nn.Linear):\n",
    "\t\tnn.init.xavier_uniform_(last_layer.weight)\n",
    "\t\tnn.init.zeros_(last_layer.bias)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_stage():\n",
    "\tAMP = False\n",
    "\tif not torch.cuda.is_available() or not torch.amp.autocast_mode.is_autocast_available(\"cuda\"):\n",
    "\t\tAMP = False\n",
    "\n",
    "\ttrain_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "\tval_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "\n",
    "\tddf_train = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=train_filters,\n",
    "\t)\n",
    "\n",
    "\tddf_val = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=val_filters,\n",
    "\t)\n",
    "\n",
    "\t# 1) (Opcional) entrenar el clasificador aquí si no lo tienes ya\n",
    "\t#    Si ya lo tienes, te lo saltas.\n",
    "\n",
    "\t# 2) Regresor inicializado desde el clasificador\n",
    "\tcls_ckpt_path = \"model_checkpoint_v1_f_cls_e1.pth\"  # el que tengas\n",
    "\n",
    "\treg_model = DeepEmbeddingMLP(\n",
    "\tcat_cardinalities=cat_cardinalities,\n",
    "\tcat_features=CATEGORICAL_FEATURES,\n",
    "\tnum_cont_features=num_cont_features,  # = 2 * len(NUMERICAL_FEATURES)\n",
    "\tembedding_dim=64,                     # <-- IGUAL que en el clasificador\n",
    "\thidden_dims=[512, 256, 128, 64],      # <-- IGUAL que en el clasificador\n",
    "\tdropout=0.2,\n",
    "\t).to(device)\n",
    "\tstate = torch.load(\"model_checkpoint_v1_f_cls_e1.pth\", map_location=device)\n",
    "\treg_model.load_state_dict(state) \n",
    "\tlast_layer = reg_model.mlp[-1]\n",
    "\tif isinstance(last_layer, nn.Linear):\n",
    "\t\tnn.init.xavier_uniform_(last_layer.weight)\n",
    "\t\tnn.init.zeros_(last_layer.bias)\n",
    "\n",
    "\ttrain_stage(\n",
    "\t\tstage_name=\"REG\",\n",
    "\t\tddf_train=ddf_train,\n",
    "\t\tddf_val=ddf_val,\n",
    "\t\ttarget_col=TARGET_LOG,          # \"iap_revenue_d7_log\"\n",
    "\t\tonly_buyers=True,\n",
    "\t\tcriterion=nn.MSELoss(),\n",
    "\t\tcheckpoint_prefix=f\"{CHECKPOINT_PATH}_reg\",\n",
    "\t\tepochs=EPOCHS,\n",
    "\t\tamp=AMP,\n",
    "\t\tinit_model=reg_model,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[REG] --- Epoch 1/5 ---\n",
      "  [REG] Epoch 1 | Partition 1 | Loss: 12.1108\n",
      "  [REG] Epoch 1 | Partition 2 | Loss: 5.7939\n",
      "  [REG] Epoch 1 | Partition 3 | Loss: 4.7506\n",
      "  [REG] Epoch 1 | Partition 4 | Loss: 3.8539\n",
      "  [REG] Epoch 1 | Partition 5 | Loss: 2.9583\n",
      "  [REG] Epoch 1 | Partition 6 | Loss: 2.9573\n",
      "  [REG] Epoch 1 | Partition 7 | Loss: 2.8712\n",
      "  [REG] Epoch 1 | Partition 8 | Loss: 2.6486\n",
      "  [REG] Epoch 1 | Partition 9 | Loss: 2.6269\n",
      "  [REG] Epoch 1 | Partition 10 | Loss: 2.5844\n",
      "  [REG] Epoch 1 | Partition 11 | Loss: 2.3911\n",
      "  [REG] Epoch 1 | Partition 12 | Loss: 2.3904\n",
      "  [REG] Epoch 1 | Partition 13 | Loss: 2.3063\n",
      "  [REG] Epoch 1 | Partition 14 | Loss: 2.0670\n",
      "  [REG] Epoch 1 | Partition 15 | Loss: 1.8276\n",
      "  [REG] Epoch 1 | Partition 16 | Loss: 1.7829\n",
      "  [REG] Epoch 1 | Partition 17 | Loss: 1.6787\n",
      "  [REG] Epoch 1 | Partition 18 | Loss: 1.6041\n",
      "  [REG] Epoch 1 | Partition 19 | Loss: 1.5904\n",
      "  [REG] Epoch 1 | Partition 20 | Loss: 1.4943\n",
      "  [REG] Epoch 1 | Partition 21 | Loss: 1.4504\n",
      "  [REG] Epoch 1 | Partition 22 | Loss: 1.4930\n",
      "  [REG] Epoch 1 | Partition 23 | Loss: 1.4183\n",
      "  [REG] Epoch 1 | Partition 24 | Loss: 1.3838\n",
      "  [REG] Epoch 1 | Partition 25 | Loss: 1.2532\n",
      "  [REG] Epoch 1 | Partition 26 | Loss: 1.2515\n",
      "  [REG] Epoch 1 | Partition 27 | Loss: 1.3130\n",
      "  [REG] Epoch 1 | Partition 28 | Loss: 1.3230\n",
      "  [REG] Partition 29 has no buyers, skipping\n",
      "  [REG] Epoch 1 | Partition 30 | Loss: 1.5625\n",
      "  [REG] Epoch 1 | Partition 31 | Loss: 1.4460\n",
      "  [REG] Epoch 1 | Partition 32 | Loss: 1.4762\n",
      "  [REG] Epoch 1 | Partition 33 | Loss: 1.4948\n",
      "  [REG] Epoch 1 | Partition 34 | Loss: 1.5449\n",
      "  [REG] Epoch 1 | Partition 35 | Loss: 1.3844\n",
      "  [REG] Epoch 1 | Partition 36 | Loss: 1.4268\n",
      "  [REG] Epoch 1 | Partition 37 | Loss: 1.3680\n",
      "  [REG] Epoch 1 | Partition 38 | Loss: 1.4261\n",
      "  [REG] Epoch 1 | Partition 39 | Loss: 1.4414\n",
      "  [REG] Epoch 1 | Partition 40 | Loss: 1.3164\n",
      "  [REG] Epoch 1 | Partition 41 | Loss: 1.2842\n",
      "  [REG] Epoch 1 | Partition 42 | Loss: 1.2269\n",
      "  [REG] Epoch 1 | Partition 43 | Loss: 1.2062\n",
      "  [REG] Epoch 1 | Partition 44 | Loss: 1.2245\n",
      "  [REG] Epoch 1 | Partition 45 | Loss: 1.2179\n",
      "  [REG] Epoch 1 | Partition 46 | Loss: 1.1528\n",
      "  [REG] Epoch 1 | Partition 47 | Loss: 1.1548\n",
      "  [REG] Epoch 1 | Partition 48 | Loss: 1.2506\n",
      "  [REG] Epoch 1 | Partition 49 | Loss: 1.1446\n",
      "  [REG] Epoch 1 | Partition 50 | Loss: 1.0728\n",
      "  [REG] Epoch 1 | Partition 51 | Loss: 1.2128\n",
      "  [REG] Epoch 1 | Partition 52 | Loss: 1.1562\n",
      "  [REG] Epoch 1 | Partition 53 | Loss: 1.0729\n",
      "  [REG] Epoch 1 | Partition 54 | Loss: 1.1904\n",
      "  [REG] Epoch 1 | Partition 55 | Loss: 1.2145\n",
      "  [REG] Epoch 1 | Partition 56 | Loss: 1.2259\n",
      "  [REG] Epoch 1 | Partition 57 | Loss: 1.2181\n",
      "  [REG] Epoch 1 | Partition 58 | Loss: 1.2198\n",
      "  [REG] Epoch 1 | Partition 59 | Loss: 1.3136\n",
      "  [REG] Epoch 1 | Partition 60 | Loss: 1.2734\n",
      "  [REG] Epoch 1 | Partition 61 | Loss: 1.2169\n",
      "  [REG] Epoch 1 | Partition 62 | Loss: 1.1826\n",
      "  [REG] Epoch 1 | Partition 63 | Loss: 1.1689\n",
      "  [REG] Epoch 1 | Partition 64 | Loss: 1.1521\n",
      "  [REG] Epoch 1 | Partition 65 | Loss: 1.0936\n",
      "  [REG] Epoch 1 | Partition 66 | Loss: 1.1303\n",
      "  [REG] Epoch 1 | Partition 67 | Loss: 1.1220\n",
      "  [REG] Epoch 1 | Partition 68 | Loss: 1.1064\n",
      "  [REG] Epoch 1 | Partition 69 | Loss: 1.1329\n",
      "  [REG] Epoch 1 | Partition 70 | Loss: 1.0750\n",
      "  [REG] Epoch 1 | Partition 71 | Loss: 1.0887\n",
      "  [REG] Epoch 1 | Partition 72 | Loss: 1.1042\n",
      "  [REG] Epoch 1 | Partition 73 | Loss: 1.0702\n",
      "  [REG] Epoch 1 | Partition 74 | Loss: 1.0507\n",
      "  [REG] Epoch 1 | Partition 75 | Loss: 1.0556\n",
      "  [REG] Epoch 1 | Partition 76 | Loss: 1.1435\n",
      "  [REG] Epoch 1 | Partition 77 | Loss: 1.0829\n",
      "  [REG] Epoch 1 | Partition 78 | Loss: 1.0747\n",
      "  [REG] Epoch 1 | Partition 79 | Loss: 1.1538\n",
      "  [REG] Epoch 1 | Partition 80 | Loss: 1.1851\n",
      "  [REG] Epoch 1 | Partition 81 | Loss: 1.1574\n",
      "  [REG] Epoch 1 | Partition 82 | Loss: 1.0774\n",
      "  [REG] Epoch 1 | Partition 83 | Loss: 1.2056\n",
      "  [REG] Epoch 1 | Partition 84 | Loss: 1.1652\n",
      "  [REG] Epoch 1 | Partition 85 | Loss: 1.1472\n",
      "  [REG] Epoch 1 | Partition 86 | Loss: 1.1616\n",
      "  [REG] Epoch 1 | Partition 87 | Loss: 1.0923\n",
      "  [REG] Epoch 1 | Partition 88 | Loss: 1.0719\n",
      "  [REG] Epoch 1 | Partition 89 | Loss: 1.0817\n",
      "  [REG] Epoch 1 | Partition 90 | Loss: 1.0630\n",
      "  [REG] Epoch 1 | Partition 91 | Loss: 1.0523\n",
      "  [REG] Epoch 1 | Partition 92 | Loss: 1.0872\n",
      "  [REG] Epoch 1 | Partition 93 | Loss: 1.1081\n",
      "  [REG] Epoch 1 | Partition 94 | Loss: 1.0211\n",
      "  [REG] Epoch 1 | Partition 95 | Loss: 1.0057\n",
      "  [REG] Epoch 1 | Partition 96 | Loss: 1.0011\n",
      "  [REG] Epoch 1 | Partition 97 | Loss: 0.9536\n",
      "  [REG] Epoch 1 | Partition 98 | Loss: 0.9813\n",
      "  [REG] Epoch 1 | Partition 99 | Loss: 0.9485\n",
      "  [REG] Epoch 1 | Partition 100 | Loss: 1.0191\n",
      "  [REG] Epoch 1 | Partition 101 | Loss: 1.0919\n",
      "  [REG] Epoch 1 | Partition 102 | Loss: 0.9972\n",
      "  [REG] Epoch 1 | Partition 103 | Loss: 1.1594\n",
      "  [REG] Epoch 1 | Partition 104 | Loss: 1.0743\n",
      "  [REG] Epoch 1 | Partition 105 | Loss: 1.1348\n",
      "  [REG] Epoch 1 | Partition 106 | Loss: 1.1123\n",
      "  [REG] Epoch 1 | Partition 107 | Loss: 1.1218\n",
      "  [REG] Epoch 1 | Partition 108 | Loss: 1.1882\n",
      "  [REG] Epoch 1 | Partition 109 | Loss: 1.0923\n",
      "  [REG] Epoch 1 | Partition 110 | Loss: 1.0915\n",
      "  [REG] Epoch 1 | Partition 111 | Loss: 1.0477\n",
      "  [REG] Epoch 1 | Partition 112 | Loss: 1.0442\n",
      "  [REG] Epoch 1 | Partition 113 | Loss: 1.0389\n",
      "  [REG] Epoch 1 | Partition 114 | Loss: 0.9635\n",
      "  [REG] Epoch 1 | Partition 115 | Loss: 0.9898\n",
      "  [REG] Epoch 1 | Partition 116 | Loss: 0.9759\n",
      "  [REG] Epoch 1 | Partition 117 | Loss: 0.9378\n",
      "  [REG] Epoch 1 | Partition 118 | Loss: 0.9537\n",
      "  [REG] Partition 119 has no buyers, skipping\n",
      "  [REG] Epoch 1 | Partition 120 | Loss: 0.9103\n",
      "  [REG] Epoch 1 | Partition 121 | Loss: 1.0184\n",
      "  [REG] Epoch 1 | Partition 122 | Loss: 0.9876\n",
      "  [REG] Epoch 1 | Partition 123 | Loss: 1.0083\n",
      "  [REG] Epoch 1 | Partition 124 | Loss: 0.9966\n",
      "  [REG] Epoch 1 | Partition 125 | Loss: 1.0971\n",
      "  [REG] Epoch 1 | Partition 126 | Loss: 1.0509\n",
      "  [REG] Epoch 1 | Partition 127 | Loss: 1.0925\n",
      "  [REG] Epoch 1 | Partition 128 | Loss: 1.1438\n",
      "  [REG] Epoch 1 | Partition 129 | Loss: 1.1393\n",
      "  [REG] Epoch 1 | Partition 130 | Loss: 1.1397\n",
      "  [REG] Epoch 1 | Partition 131 | Loss: 1.1512\n",
      "  [REG] Epoch 1 | Partition 132 | Loss: 1.1368\n",
      "  [REG] Epoch 1 | Partition 133 | Loss: 1.1633\n",
      "  [REG] Epoch 1 | Partition 134 | Loss: 1.1603\n",
      "  [REG] Epoch 1 | Partition 135 | Loss: 1.1005\n",
      "  [REG] Epoch 1 | Partition 136 | Loss: 1.1117\n",
      "  [REG] Epoch 1 | Partition 137 | Loss: 1.0018\n",
      "  [REG] Epoch 1 | Partition 138 | Loss: 1.0014\n",
      "  [REG] Epoch 1 | Partition 139 | Loss: 1.0454\n",
      "  [REG] Epoch 1 | Partition 140 | Loss: 1.0039\n",
      "  [REG] Epoch 1 | Partition 141 | Loss: 0.9954\n",
      "  [REG] Epoch 1 | Partition 142 | Loss: 1.0213\n",
      "  [REG] Partition 143 has no buyers, skipping\n",
      "[REG] Epoch 1 | Val loss: 0.9518\n",
      "[REG] Saved: model_checkpoint_v1_f_reg_e1.pth\n",
      "\n",
      "[REG] --- Epoch 2/5 ---\n",
      "  [REG] Epoch 2 | Partition 1 | Loss: 1.0613\n",
      "  [REG] Epoch 2 | Partition 2 | Loss: 1.0122\n",
      "  [REG] Epoch 2 | Partition 3 | Loss: 1.0668\n",
      "  [REG] Epoch 2 | Partition 4 | Loss: 0.9853\n",
      "  [REG] Epoch 2 | Partition 5 | Loss: 1.0669\n",
      "  [REG] Epoch 2 | Partition 6 | Loss: 1.0728\n",
      "  [REG] Epoch 2 | Partition 7 | Loss: 1.0402\n",
      "  [REG] Epoch 2 | Partition 8 | Loss: 1.1545\n",
      "  [REG] Epoch 2 | Partition 9 | Loss: 1.1313\n",
      "  [REG] Epoch 2 | Partition 10 | Loss: 1.1127\n",
      "  [REG] Epoch 2 | Partition 11 | Loss: 1.1980\n",
      "  [REG] Epoch 2 | Partition 12 | Loss: 1.2079\n",
      "  [REG] Epoch 2 | Partition 13 | Loss: 1.2338\n",
      "  [REG] Epoch 2 | Partition 14 | Loss: 1.1135\n",
      "  [REG] Epoch 2 | Partition 15 | Loss: 1.0202\n",
      "  [REG] Epoch 2 | Partition 16 | Loss: 1.0975\n",
      "  [REG] Epoch 2 | Partition 17 | Loss: 1.0483\n",
      "  [REG] Epoch 2 | Partition 18 | Loss: 1.0490\n",
      "  [REG] Epoch 2 | Partition 19 | Loss: 1.0215\n",
      "  [REG] Epoch 2 | Partition 20 | Loss: 1.0469\n",
      "  [REG] Epoch 2 | Partition 21 | Loss: 0.9910\n",
      "  [REG] Epoch 2 | Partition 22 | Loss: 1.0508\n",
      "  [REG] Epoch 2 | Partition 23 | Loss: 1.0295\n",
      "  [REG] Epoch 2 | Partition 24 | Loss: 1.0193\n",
      "  [REG] Epoch 2 | Partition 25 | Loss: 0.9373\n",
      "  [REG] Epoch 2 | Partition 26 | Loss: 0.9765\n",
      "  [REG] Epoch 2 | Partition 27 | Loss: 0.9851\n",
      "  [REG] Epoch 2 | Partition 28 | Loss: 0.9986\n",
      "  [REG] Epoch 2 | Partition 29 | Loss: 1.0887\n",
      "  [REG] Partition 30 has no buyers, skipping\n",
      "  [REG] Epoch 2 | Partition 31 | Loss: 1.0841\n",
      "  [REG] Epoch 2 | Partition 32 | Loss: 1.1786\n",
      "  [REG] Epoch 2 | Partition 33 | Loss: 1.1466\n",
      "  [REG] Epoch 2 | Partition 34 | Loss: 1.2089\n",
      "  [REG] Epoch 2 | Partition 35 | Loss: 1.0871\n",
      "  [REG] Epoch 2 | Partition 36 | Loss: 1.1088\n",
      "  [REG] Epoch 2 | Partition 37 | Loss: 1.0646\n",
      "  [REG] Epoch 2 | Partition 38 | Loss: 1.1196\n",
      "  [REG] Epoch 2 | Partition 39 | Loss: 1.0486\n",
      "  [REG] Epoch 2 | Partition 40 | Loss: 1.0611\n",
      "  [REG] Epoch 2 | Partition 41 | Loss: 0.9920\n",
      "  [REG] Epoch 2 | Partition 42 | Loss: 0.9891\n",
      "  [REG] Epoch 2 | Partition 43 | Loss: 0.9894\n",
      "  [REG] Epoch 2 | Partition 44 | Loss: 1.0131\n",
      "  [REG] Epoch 2 | Partition 45 | Loss: 1.0351\n",
      "  [REG] Epoch 2 | Partition 46 | Loss: 0.9627\n",
      "  [REG] Epoch 2 | Partition 47 | Loss: 0.9505\n",
      "  [REG] Epoch 2 | Partition 48 | Loss: 0.9935\n",
      "  [REG] Epoch 2 | Partition 49 | Loss: 0.9772\n",
      "  [REG] Epoch 2 | Partition 50 | Loss: 0.9283\n",
      "  [REG] Epoch 2 | Partition 51 | Loss: 1.0236\n",
      "  [REG] Epoch 2 | Partition 52 | Loss: 1.0029\n",
      "  [REG] Epoch 2 | Partition 53 | Loss: 0.9315\n",
      "  [REG] Epoch 2 | Partition 54 | Loss: 1.0633\n",
      "  [REG] Epoch 2 | Partition 55 | Loss: 1.0454\n",
      "  [REG] Epoch 2 | Partition 56 | Loss: 1.0436\n",
      "  [REG] Epoch 2 | Partition 57 | Loss: 1.0874\n",
      "  [REG] Epoch 2 | Partition 58 | Loss: 1.0640\n",
      "  [REG] Epoch 2 | Partition 59 | Loss: 1.1240\n",
      "  [REG] Epoch 2 | Partition 60 | Loss: 1.1097\n",
      "  [REG] Epoch 2 | Partition 61 | Loss: 1.0789\n",
      "  [REG] Epoch 2 | Partition 62 | Loss: 1.0313\n",
      "  [REG] Epoch 2 | Partition 63 | Loss: 1.0310\n",
      "  [REG] Epoch 2 | Partition 64 | Loss: 1.0102\n",
      "  [REG] Epoch 2 | Partition 65 | Loss: 0.9786\n",
      "  [REG] Epoch 2 | Partition 66 | Loss: 1.0060\n",
      "  [REG] Epoch 2 | Partition 67 | Loss: 0.9832\n",
      "  [REG] Epoch 2 | Partition 68 | Loss: 0.9819\n",
      "  [REG] Epoch 2 | Partition 69 | Loss: 1.0382\n",
      "  [REG] Epoch 2 | Partition 70 | Loss: 0.9481\n",
      "  [REG] Epoch 2 | Partition 71 | Loss: 0.9701\n",
      "  [REG] Epoch 2 | Partition 72 | Loss: 0.9841\n",
      "  [REG] Epoch 2 | Partition 73 | Loss: 0.9700\n",
      "  [REG] Epoch 2 | Partition 74 | Loss: 0.9480\n",
      "  [REG] Epoch 2 | Partition 75 | Loss: 0.9585\n",
      "  [REG] Epoch 2 | Partition 76 | Loss: 1.0440\n",
      "  [REG] Epoch 2 | Partition 77 | Loss: 0.9721\n",
      "  [REG] Epoch 2 | Partition 78 | Loss: 0.9968\n",
      "  [REG] Epoch 2 | Partition 79 | Loss: 1.0427\n",
      "  [REG] Epoch 2 | Partition 80 | Loss: 1.0511\n",
      "  [REG] Epoch 2 | Partition 81 | Loss: 1.0410\n",
      "  [REG] Epoch 2 | Partition 82 | Loss: 0.9975\n",
      "  [REG] Epoch 2 | Partition 83 | Loss: 1.0760\n",
      "  [REG] Epoch 2 | Partition 84 | Loss: 1.0813\n",
      "  [REG] Epoch 2 | Partition 85 | Loss: 1.0579\n",
      "  [REG] Epoch 2 | Partition 86 | Loss: 1.0442\n",
      "  [REG] Epoch 2 | Partition 87 | Loss: 0.9992\n",
      "  [REG] Epoch 2 | Partition 88 | Loss: 0.9829\n",
      "  [REG] Epoch 2 | Partition 89 | Loss: 0.9782\n",
      "  [REG] Epoch 2 | Partition 90 | Loss: 0.9621\n",
      "  [REG] Epoch 2 | Partition 91 | Loss: 0.9730\n",
      "  [REG] Epoch 2 | Partition 92 | Loss: 1.0114\n",
      "  [REG] Epoch 2 | Partition 93 | Loss: 1.0400\n",
      "  [REG] Epoch 2 | Partition 94 | Loss: 0.9545\n",
      "  [REG] Epoch 2 | Partition 95 | Loss: 0.9380\n",
      "  [REG] Epoch 2 | Partition 96 | Loss: 0.9263\n",
      "  [REG] Epoch 2 | Partition 97 | Loss: 0.8901\n",
      "  [REG] Epoch 2 | Partition 98 | Loss: 0.9807\n",
      "  [REG] Epoch 2 | Partition 99 | Loss: 0.9080\n",
      "  [REG] Epoch 2 | Partition 100 | Loss: 0.9387\n",
      "  [REG] Epoch 2 | Partition 101 | Loss: 1.0274\n",
      "  [REG] Epoch 2 | Partition 102 | Loss: 0.9523\n",
      "  [REG] Epoch 2 | Partition 103 | Loss: 1.0661\n",
      "  [REG] Epoch 2 | Partition 104 | Loss: 1.0176\n",
      "  [REG] Epoch 2 | Partition 105 | Loss: 1.0578\n",
      "  [REG] Epoch 2 | Partition 106 | Loss: 1.0509\n",
      "  [REG] Epoch 2 | Partition 107 | Loss: 1.0649\n",
      "  [REG] Epoch 2 | Partition 108 | Loss: 1.1230\n",
      "  [REG] Epoch 2 | Partition 109 | Loss: 1.0288\n",
      "  [REG] Epoch 2 | Partition 110 | Loss: 1.0356\n",
      "  [REG] Epoch 2 | Partition 111 | Loss: 1.0055\n",
      "  [REG] Epoch 2 | Partition 112 | Loss: 0.9722\n",
      "  [REG] Epoch 2 | Partition 113 | Loss: 1.0157\n",
      "  [REG] Epoch 2 | Partition 114 | Loss: 0.9253\n",
      "  [REG] Epoch 2 | Partition 115 | Loss: 0.9155\n",
      "  [REG] Epoch 2 | Partition 116 | Loss: 0.9513\n",
      "  [REG] Epoch 2 | Partition 117 | Loss: 0.8959\n",
      "  [REG] Epoch 2 | Partition 118 | Loss: 0.9270\n",
      "  [REG] Partition 119 has no buyers, skipping\n",
      "  [REG] Epoch 2 | Partition 120 | Loss: 0.8543\n",
      "  [REG] Epoch 2 | Partition 121 | Loss: 0.9010\n",
      "  [REG] Epoch 2 | Partition 122 | Loss: 0.9385\n",
      "  [REG] Epoch 2 | Partition 123 | Loss: 0.9594\n",
      "  [REG] Epoch 2 | Partition 124 | Loss: 0.9289\n",
      "  [REG] Epoch 2 | Partition 125 | Loss: 1.0083\n",
      "  [REG] Epoch 2 | Partition 126 | Loss: 0.9895\n",
      "  [REG] Epoch 2 | Partition 127 | Loss: 1.0077\n",
      "  [REG] Epoch 2 | Partition 128 | Loss: 1.0802\n",
      "  [REG] Epoch 2 | Partition 129 | Loss: 1.0504\n",
      "  [REG] Epoch 2 | Partition 130 | Loss: 1.0745\n",
      "  [REG] Epoch 2 | Partition 131 | Loss: 1.0827\n",
      "  [REG] Epoch 2 | Partition 132 | Loss: 1.0655\n",
      "  [REG] Epoch 2 | Partition 133 | Loss: 1.0877\n",
      "  [REG] Epoch 2 | Partition 134 | Loss: 1.1004\n",
      "  [REG] Epoch 2 | Partition 135 | Loss: 1.0118\n",
      "  [REG] Epoch 2 | Partition 136 | Loss: 1.0537\n",
      "  [REG] Epoch 2 | Partition 137 | Loss: 0.9322\n",
      "  [REG] Epoch 2 | Partition 138 | Loss: 1.0272\n",
      "  [REG] Epoch 2 | Partition 139 | Loss: 1.0040\n",
      "  [REG] Epoch 2 | Partition 140 | Loss: 0.9369\n",
      "  [REG] Epoch 2 | Partition 141 | Loss: 0.9639\n",
      "  [REG] Partition 142 has no buyers, skipping\n",
      "  [REG] Partition 143 has no buyers, skipping\n",
      "[REG] Epoch 2 | Val loss: 0.9084\n",
      "[REG] Saved: model_checkpoint_v1_f_reg_e2.pth\n",
      "\n",
      "[REG] --- Epoch 3/5 ---\n",
      "  [REG] Epoch 3 | Partition 1 | Loss: 1.0158\n",
      "  [REG] Epoch 3 | Partition 2 | Loss: 0.9519\n",
      "  [REG] Epoch 3 | Partition 3 | Loss: 1.0073\n",
      "  [REG] Epoch 3 | Partition 4 | Loss: 0.9533\n",
      "  [REG] Epoch 3 | Partition 5 | Loss: 1.0294\n",
      "  [REG] Epoch 3 | Partition 6 | Loss: 1.0458\n",
      "  [REG] Epoch 3 | Partition 7 | Loss: 0.9876\n",
      "  [REG] Epoch 3 | Partition 8 | Loss: 1.0977\n",
      "  [REG] Epoch 3 | Partition 9 | Loss: 1.1488\n",
      "  [REG] Epoch 3 | Partition 10 | Loss: 1.0742\n",
      "  [REG] Epoch 3 | Partition 11 | Loss: 1.1769\n",
      "  [REG] Epoch 3 | Partition 12 | Loss: 1.1768\n",
      "  [REG] Epoch 3 | Partition 13 | Loss: 1.1440\n",
      "  [REG] Epoch 3 | Partition 14 | Loss: 1.0709\n",
      "  [REG] Epoch 3 | Partition 15 | Loss: 0.9665\n",
      "  [REG] Epoch 3 | Partition 16 | Loss: 1.0094\n",
      "  [REG] Epoch 3 | Partition 17 | Loss: 0.9818\n",
      "  [REG] Epoch 3 | Partition 18 | Loss: 1.0105\n",
      "  [REG] Epoch 3 | Partition 19 | Loss: 0.9621\n",
      "  [REG] Epoch 3 | Partition 20 | Loss: 1.0357\n",
      "  [REG] Epoch 3 | Partition 21 | Loss: 0.9753\n",
      "  [REG] Epoch 3 | Partition 22 | Loss: 1.0029\n",
      "  [REG] Epoch 3 | Partition 23 | Loss: 0.9900\n",
      "  [REG] Epoch 3 | Partition 24 | Loss: 0.9543\n",
      "  [REG] Epoch 3 | Partition 25 | Loss: 0.9002\n",
      "  [REG] Epoch 3 | Partition 26 | Loss: 0.9423\n",
      "  [REG] Epoch 3 | Partition 27 | Loss: 0.9697\n",
      "  [REG] Epoch 3 | Partition 28 | Loss: 0.9620\n",
      "  [REG] Epoch 3 | Partition 29 | Loss: 1.0768\n",
      "  [REG] Epoch 3 | Partition 30 | Loss: 1.1131\n",
      "  [REG] Epoch 3 | Partition 31 | Loss: 1.0559\n",
      "  [REG] Epoch 3 | Partition 32 | Loss: 1.1182\n",
      "  [REG] Epoch 3 | Partition 33 | Loss: 1.1107\n",
      "  [REG] Epoch 3 | Partition 34 | Loss: 1.1891\n",
      "  [REG] Epoch 3 | Partition 35 | Loss: 1.0386\n",
      "  [REG] Epoch 3 | Partition 36 | Loss: 1.0726\n",
      "  [REG] Epoch 3 | Partition 37 | Loss: 1.0416\n",
      "  [REG] Epoch 3 | Partition 38 | Loss: 1.0597\n",
      "  [REG] Epoch 3 | Partition 39 | Loss: 1.0977\n",
      "  [REG] Epoch 3 | Partition 40 | Loss: 1.0107\n",
      "  [REG] Epoch 3 | Partition 41 | Loss: 0.9484\n",
      "  [REG] Epoch 3 | Partition 42 | Loss: 0.9302\n",
      "  [REG] Epoch 3 | Partition 43 | Loss: 0.9764\n",
      "  [REG] Epoch 3 | Partition 44 | Loss: 0.9918\n",
      "  [REG] Epoch 3 | Partition 45 | Loss: 0.9922\n",
      "  [REG] Epoch 3 | Partition 46 | Loss: 0.9486\n",
      "  [REG] Epoch 3 | Partition 47 | Loss: 0.9393\n",
      "  [REG] Epoch 3 | Partition 48 | Loss: 0.9474\n",
      "  [REG] Epoch 3 | Partition 49 | Loss: 0.9497\n",
      "  [REG] Epoch 3 | Partition 50 | Loss: 0.9511\n",
      "  [REG] Epoch 3 | Partition 51 | Loss: 1.0194\n",
      "  [REG] Epoch 3 | Partition 52 | Loss: 0.9833\n",
      "  [REG] Epoch 3 | Partition 53 | Loss: 0.8997\n",
      "  [REG] Epoch 3 | Partition 54 | Loss: 1.0291\n",
      "  [REG] Epoch 3 | Partition 55 | Loss: 0.9987\n",
      "  [REG] Epoch 3 | Partition 56 | Loss: 1.0317\n",
      "  [REG] Epoch 3 | Partition 57 | Loss: 1.0433\n",
      "  [REG] Epoch 3 | Partition 58 | Loss: 1.0008\n",
      "  [REG] Epoch 3 | Partition 59 | Loss: 1.0982\n",
      "  [REG] Epoch 3 | Partition 60 | Loss: 1.0697\n",
      "  [REG] Epoch 3 | Partition 61 | Loss: 1.0565\n",
      "  [REG] Epoch 3 | Partition 62 | Loss: 1.0447\n",
      "  [REG] Epoch 3 | Partition 63 | Loss: 1.0121\n",
      "  [REG] Epoch 3 | Partition 64 | Loss: 0.9971\n",
      "  [REG] Epoch 3 | Partition 65 | Loss: 0.9738\n",
      "  [REG] Epoch 3 | Partition 66 | Loss: 0.9633\n",
      "  [REG] Epoch 3 | Partition 67 | Loss: 0.9429\n",
      "  [REG] Epoch 3 | Partition 68 | Loss: 0.9805\n",
      "  [REG] Epoch 3 | Partition 69 | Loss: 1.0093\n",
      "  [REG] Epoch 3 | Partition 70 | Loss: 0.9399\n",
      "  [REG] Partition 71 has no buyers, skipping\n",
      "  [REG] Epoch 3 | Partition 72 | Loss: 0.9274\n",
      "  [REG] Epoch 3 | Partition 73 | Loss: 0.9721\n",
      "  [REG] Epoch 3 | Partition 74 | Loss: 0.9389\n",
      "  [REG] Epoch 3 | Partition 75 | Loss: 0.9518\n",
      "  [REG] Epoch 3 | Partition 76 | Loss: 1.0259\n",
      "  [REG] Epoch 3 | Partition 77 | Loss: 0.9844\n",
      "  [REG] Epoch 3 | Partition 78 | Loss: 0.9679\n",
      "  [REG] Epoch 3 | Partition 79 | Loss: 1.0313\n",
      "  [REG] Epoch 3 | Partition 80 | Loss: 1.0523\n",
      "  [REG] Epoch 3 | Partition 81 | Loss: 0.9803\n",
      "  [REG] Epoch 3 | Partition 82 | Loss: 0.9537\n",
      "  [REG] Epoch 3 | Partition 83 | Loss: 1.0487\n",
      "  [REG] Epoch 3 | Partition 84 | Loss: 1.0358\n",
      "  [REG] Epoch 3 | Partition 85 | Loss: 1.0264\n",
      "  [REG] Epoch 3 | Partition 86 | Loss: 1.0119\n",
      "  [REG] Epoch 3 | Partition 87 | Loss: 0.9932\n",
      "  [REG] Epoch 3 | Partition 88 | Loss: 0.9590\n",
      "  [REG] Epoch 3 | Partition 89 | Loss: 0.9607\n",
      "  [REG] Epoch 3 | Partition 90 | Loss: 0.9302\n",
      "  [REG] Epoch 3 | Partition 91 | Loss: 0.9564\n",
      "  [REG] Epoch 3 | Partition 92 | Loss: 0.9891\n",
      "  [REG] Epoch 3 | Partition 93 | Loss: 1.0010\n",
      "  [REG] Epoch 3 | Partition 94 | Loss: 0.9382\n",
      "  [REG] Epoch 3 | Partition 95 | Loss: 0.9075\n",
      "  [REG] Epoch 3 | Partition 96 | Loss: 0.8950\n",
      "  [REG] Epoch 3 | Partition 97 | Loss: 0.8432\n",
      "  [REG] Epoch 3 | Partition 98 | Loss: 0.9026\n",
      "  [REG] Epoch 3 | Partition 99 | Loss: 0.8776\n",
      "  [REG] Epoch 3 | Partition 100 | Loss: 0.9179\n",
      "  [REG] Epoch 3 | Partition 101 | Loss: 0.9978\n",
      "  [REG] Epoch 3 | Partition 102 | Loss: 0.9216\n",
      "  [REG] Epoch 3 | Partition 103 | Loss: 1.0524\n",
      "  [REG] Epoch 3 | Partition 104 | Loss: 0.9538\n",
      "  [REG] Epoch 3 | Partition 105 | Loss: 1.0253\n",
      "  [REG] Epoch 3 | Partition 106 | Loss: 1.0299\n",
      "  [REG] Epoch 3 | Partition 107 | Loss: 1.0247\n",
      "  [REG] Epoch 3 | Partition 108 | Loss: 1.1075\n",
      "  [REG] Epoch 3 | Partition 109 | Loss: 0.9951\n",
      "  [REG] Epoch 3 | Partition 110 | Loss: 0.9988\n",
      "  [REG] Epoch 3 | Partition 111 | Loss: 0.9882\n",
      "  [REG] Epoch 3 | Partition 112 | Loss: 0.9547\n",
      "  [REG] Epoch 3 | Partition 113 | Loss: 1.1693\n",
      "  [REG] Epoch 3 | Partition 114 | Loss: 0.8863\n",
      "  [REG] Epoch 3 | Partition 115 | Loss: 0.9069\n",
      "  [REG] Epoch 3 | Partition 116 | Loss: 0.9107\n",
      "  [REG] Epoch 3 | Partition 117 | Loss: 0.8852\n",
      "  [REG] Epoch 3 | Partition 118 | Loss: 0.9115\n",
      "  [REG] Partition 119 has no buyers, skipping\n",
      "  [REG] Epoch 3 | Partition 120 | Loss: 0.8452\n",
      "  [REG] Epoch 3 | Partition 121 | Loss: 0.8852\n",
      "  [REG] Epoch 3 | Partition 122 | Loss: 0.9159\n",
      "  [REG] Epoch 3 | Partition 123 | Loss: 0.9310\n",
      "  [REG] Epoch 3 | Partition 124 | Loss: 0.8939\n",
      "  [REG] Epoch 3 | Partition 125 | Loss: 1.0118\n",
      "  [REG] Epoch 3 | Partition 126 | Loss: 0.9799\n",
      "  [REG] Epoch 3 | Partition 127 | Loss: 0.9982\n",
      "  [REG] Epoch 3 | Partition 128 | Loss: 1.0479\n",
      "  [REG] Epoch 3 | Partition 129 | Loss: 0.9972\n",
      "  [REG] Epoch 3 | Partition 130 | Loss: 1.0369\n",
      "  [REG] Epoch 3 | Partition 131 | Loss: 1.0369\n",
      "  [REG] Epoch 3 | Partition 132 | Loss: 1.0516\n",
      "  [REG] Epoch 3 | Partition 133 | Loss: 1.0611\n",
      "  [REG] Epoch 3 | Partition 134 | Loss: 1.0422\n",
      "  [REG] Epoch 3 | Partition 135 | Loss: 0.9915\n",
      "  [REG] Epoch 3 | Partition 136 | Loss: 1.0008\n",
      "  [REG] Epoch 3 | Partition 137 | Loss: 0.9221\n",
      "  [REG] Epoch 3 | Partition 138 | Loss: 1.0098\n",
      "  [REG] Epoch 3 | Partition 139 | Loss: 0.9708\n",
      "  [REG] Epoch 3 | Partition 140 | Loss: 0.9456\n",
      "  [REG] Epoch 3 | Partition 141 | Loss: 0.9346\n",
      "  [REG] Partition 142 has no buyers, skipping\n",
      "  [REG] Partition 143 has no buyers, skipping\n",
      "[REG] Epoch 3 | Val loss: 0.8827\n",
      "[REG] Saved: model_checkpoint_v1_f_reg_e3.pth\n",
      "\n",
      "[REG] --- Epoch 4/5 ---\n",
      "  [REG] Epoch 4 | Partition 1 | Loss: 0.9925\n",
      "  [REG] Epoch 4 | Partition 2 | Loss: 0.9234\n",
      "  [REG] Epoch 4 | Partition 3 | Loss: 0.9905\n",
      "  [REG] Epoch 4 | Partition 4 | Loss: 0.9418\n",
      "  [REG] Epoch 4 | Partition 5 | Loss: 1.0008\n",
      "  [REG] Epoch 4 | Partition 6 | Loss: 0.9927\n",
      "  [REG] Epoch 4 | Partition 7 | Loss: 0.9615\n",
      "  [REG] Epoch 4 | Partition 8 | Loss: 1.0659\n",
      "  [REG] Epoch 4 | Partition 9 | Loss: 1.0921\n",
      "  [REG] Epoch 4 | Partition 10 | Loss: 1.0433\n",
      "  [REG] Epoch 4 | Partition 11 | Loss: 1.1564\n",
      "  [REG] Epoch 4 | Partition 12 | Loss: 1.1521\n",
      "  [REG] Epoch 4 | Partition 13 | Loss: 1.1332\n",
      "  [REG] Epoch 4 | Partition 14 | Loss: 1.0142\n",
      "  [REG] Epoch 4 | Partition 15 | Loss: 0.9492\n",
      "  [REG] Epoch 4 | Partition 16 | Loss: 0.9898\n",
      "  [REG] Epoch 4 | Partition 17 | Loss: 0.9612\n",
      "  [REG] Epoch 4 | Partition 18 | Loss: 0.9903\n",
      "  [REG] Epoch 4 | Partition 19 | Loss: 0.9471\n",
      "  [REG] Epoch 4 | Partition 20 | Loss: 1.0048\n",
      "  [REG] Epoch 4 | Partition 21 | Loss: 0.9497\n",
      "  [REG] Epoch 4 | Partition 22 | Loss: 0.9875\n",
      "  [REG] Epoch 4 | Partition 23 | Loss: 0.9746\n",
      "  [REG] Epoch 4 | Partition 24 | Loss: 0.9300\n",
      "  [REG] Epoch 4 | Partition 25 | Loss: 0.8650\n",
      "  [REG] Epoch 4 | Partition 26 | Loss: 0.9286\n",
      "  [REG] Epoch 4 | Partition 27 | Loss: 0.9342\n",
      "  [REG] Epoch 4 | Partition 28 | Loss: 0.9260\n",
      "  [REG] Epoch 4 | Partition 29 | Loss: 1.0056\n",
      "  [REG] Epoch 4 | Partition 30 | Loss: 1.0749\n",
      "  [REG] Epoch 4 | Partition 31 | Loss: 1.0186\n",
      "  [REG] Epoch 4 | Partition 32 | Loss: 1.1210\n",
      "  [REG] Epoch 4 | Partition 33 | Loss: 1.0640\n",
      "  [REG] Epoch 4 | Partition 34 | Loss: 1.1272\n",
      "  [REG] Epoch 4 | Partition 35 | Loss: 1.0184\n",
      "  [REG] Epoch 4 | Partition 36 | Loss: 1.0536\n",
      "  [REG] Epoch 4 | Partition 37 | Loss: 1.0003\n",
      "  [REG] Epoch 4 | Partition 38 | Loss: 1.0448\n",
      "  [REG] Epoch 4 | Partition 39 | Loss: 3.3180\n",
      "  [REG] Epoch 4 | Partition 40 | Loss: 0.9933\n",
      "  [REG] Epoch 4 | Partition 41 | Loss: 0.9792\n",
      "  [REG] Epoch 4 | Partition 42 | Loss: 0.9215\n",
      "  [REG] Epoch 4 | Partition 43 | Loss: 0.9595\n",
      "  [REG] Epoch 4 | Partition 44 | Loss: 0.9676\n",
      "  [REG] Epoch 4 | Partition 45 | Loss: 0.9751\n",
      "  [REG] Epoch 4 | Partition 46 | Loss: 0.9440\n",
      "  [REG] Epoch 4 | Partition 47 | Loss: 0.9129\n",
      "  [REG] Epoch 4 | Partition 48 | Loss: 0.9722\n",
      "  [REG] Epoch 4 | Partition 49 | Loss: 0.9294\n",
      "  [REG] Epoch 4 | Partition 50 | Loss: 0.9069\n",
      "  [REG] Epoch 4 | Partition 51 | Loss: 0.9946\n",
      "  [REG] Epoch 4 | Partition 52 | Loss: 0.9700\n",
      "  [REG] Epoch 4 | Partition 53 | Loss: 0.8947\n",
      "  [REG] Epoch 4 | Partition 54 | Loss: 1.0022\n",
      "  [REG] Epoch 4 | Partition 55 | Loss: 0.9993\n",
      "  [REG] Epoch 4 | Partition 56 | Loss: 1.0163\n",
      "  [REG] Epoch 4 | Partition 57 | Loss: 1.0211\n",
      "  [REG] Epoch 4 | Partition 58 | Loss: 1.0046\n",
      "  [REG] Epoch 4 | Partition 59 | Loss: 1.0949\n",
      "  [REG] Epoch 4 | Partition 60 | Loss: 1.0709\n",
      "  [REG] Epoch 4 | Partition 61 | Loss: 1.0461\n",
      "  [REG] Epoch 4 | Partition 62 | Loss: 1.0086\n",
      "  [REG] Epoch 4 | Partition 63 | Loss: 1.0003\n",
      "  [REG] Epoch 4 | Partition 64 | Loss: 0.9803\n",
      "  [REG] Epoch 4 | Partition 65 | Loss: 0.9435\n",
      "  [REG] Epoch 4 | Partition 66 | Loss: 0.9629\n",
      "  [REG] Epoch 4 | Partition 67 | Loss: 0.9437\n",
      "  [REG] Epoch 4 | Partition 68 | Loss: 0.9494\n",
      "  [REG] Epoch 4 | Partition 69 | Loss: 1.0120\n",
      "  [REG] Epoch 4 | Partition 70 | Loss: 0.9266\n",
      "  [REG] Partition 71 has no buyers, skipping\n",
      "  [REG] Epoch 4 | Partition 72 | Loss: 0.9242\n",
      "  [REG] Epoch 4 | Partition 73 | Loss: 0.9425\n",
      "  [REG] Epoch 4 | Partition 74 | Loss: 0.9246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain_two_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_two_stage\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m     nn.init.xavier_uniform_(last_layer.weight)\n\u001b[32m     42\u001b[39m     nn.init.zeros_(last_layer.bias)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mtrain_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mREG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mddf_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mddf_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddf_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET_LOG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# \"iap_revenue_d7_log\"\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43monly_buyers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_reg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAMP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreg_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_stage\u001b[39m\u001b[34m(stage_name, ddf_train, ddf_val, target_col, only_buyers, criterion, checkpoint_prefix, epochs, amp, init_model)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m partition_idx, partition \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(partition_iterator):\n\u001b[32m     55\u001b[39m     train_loss_sum = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     pdf = \u001b[43mpartition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pdf.empty:\n\u001b[32m     59\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Skipping empty partition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition_idx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\dask\\base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\ttrain_two_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_with_val(best_ckpt_path: str):\n",
    "\tAMP = False\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t# 1) Cargar train y val y concatenarlos\n",
    "\ttrain_filters = [(\"datetime\", \"<\", \"2025-10-06-23-00\")]\n",
    "\tval_filters = [(\"datetime\", \">=\", \"2025-10-06-23-00\")]\n",
    "\n",
    "\tddf_train = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=train_filters,\n",
    "\t)\n",
    "\tddf_val = dd.read_parquet(\n",
    "\t\tDATASET_PATH,\n",
    "\t\tengine=\"pyarrow\",\n",
    "\t\tcolumns=COLS_TO_READ,\n",
    "\t\tfilters=val_filters,\n",
    "\t)\n",
    "\n",
    "\tddf_full = dd.concat([ddf_train, ddf_val])\n",
    "\n",
    "\tcat_cardinalities = {\n",
    "\t\tfeat: len(EMBEDDING_MAPPINGS[feat])\n",
    "\t\tfor feat in CATEGORICAL_FEATURES\n",
    "\t}\n",
    "\n",
    "\tmodel = EmbeddingMLP(\n",
    "\t\tcat_cardinalities=cat_cardinalities,\n",
    "\t\tcat_features=CATEGORICAL_FEATURES,\n",
    "\t\tnum_cont_features=2 * len(NUMERICAL_FEATURES),\n",
    "\t).to(device)\n",
    "\n",
    "\tstate_dict = torch.load(best_ckpt_path, map_location=device)\n",
    "\tmodel.load_state_dict(state_dict)\n",
    "\n",
    "\tcriterion = nn.MSELoss()\n",
    "\toptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE * 0.3)  # LR más bajo para fine-tune\n",
    "\tscaler = torch.amp.GradScaler(device=\"cuda\", enabled=AMP)\n",
    "\n",
    "\tFINE_TUNE_EPOCHS = 2  # por ejemplo\n",
    "\n",
    "\tfor epoch in range(FINE_TUNE_EPOCHS):\n",
    "\t\tprint(f\"\\n--- FINE-TUNE Epoch {epoch + 1}/{FINE_TUNE_EPOCHS} ---\")\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tpartition_iterator = ddf_full.to_delayed()\n",
    "\n",
    "\t\tfor partition_idx, partition in enumerate(partition_iterator):\n",
    "\t\t\ttrain_loss_sum = 0.0\n",
    "\n",
    "\t\t\tpdf = partition.compute()\n",
    "\t\t\tif pdf.empty:\n",
    "\t\t\t\tprint(f\"  Skipping empty partition {partition_idx+1}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tpdf = process_partition(pdf)\n",
    "\n",
    "\t\t\tfeatures_tensor = torch.tensor(\n",
    "\t\t\t\tpdf[CATEGORICAL_FEATURES\n",
    "\t\t\t\t\t+ NUMERICAL_FEATURES\n",
    "\t\t\t\t\t+ [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]\n",
    "\t\t\t\t].values,\n",
    "\t\t\t\tdtype=torch.float32,\n",
    "\t\t\t)\n",
    "\t\t\ttarget_tensor = torch.tensor(\n",
    "\t\t\t\tpdf[TARGET].values,\n",
    "\t\t\t\tdtype=torch.float32,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tpartition_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "\t\t\tpartition_loader = DataLoader(\n",
    "\t\t\t\tdataset=partition_dataset,\n",
    "\t\t\t\tbatch_size=BATCH_SIZE,\n",
    "\t\t\t\tnum_workers=0,\n",
    "\t\t\t\tpin_memory=True,\n",
    "\t\t\t\tshuffle=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tfor batch_idx, (data, target) in enumerate(partition_loader):\n",
    "\t\t\t\tdata, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\t\t\twith torch.amp.autocast(device_type=\"cuda\", enabled=AMP):\n",
    "\t\t\t\t\toutput = model(data)\n",
    "\t\t\t\t\tloss = criterion(output, target)\n",
    "\n",
    "\t\t\t\tif AMP:\n",
    "\t\t\t\t\tscaler.scale(loss).backward()\n",
    "\t\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\t\tscaler.update()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\ttrain_loss_sum += loss.item()\n",
    "\n",
    "\t\t\tprint(f\"  Fine-tune Epoch {epoch+1} | Partition {partition_idx+1} | Loss: {train_loss_sum / len(partition_loader):.4f}\")\n",
    "\n",
    "\ttorch.save(model.state_dict(), \"model_finetuned_train_val.pth\")\n",
    "\tprint(\"\\n--- FINE-TUNE Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30cfd151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINE-TUNE Epoch 1/2 ---\n",
      "  Fine-tune Epoch 1 | Partition 1 | Loss: 0.2329\n",
      "  Fine-tune Epoch 1 | Partition 2 | Loss: 0.1928\n",
      "  Fine-tune Epoch 1 | Partition 3 | Loss: 0.1800\n",
      "  Fine-tune Epoch 1 | Partition 4 | Loss: 0.1844\n",
      "  Fine-tune Epoch 1 | Partition 5 | Loss: 0.1992\n",
      "  Fine-tune Epoch 1 | Partition 6 | Loss: 0.2009\n",
      "  Fine-tune Epoch 1 | Partition 7 | Loss: 0.2181\n",
      "  Fine-tune Epoch 1 | Partition 8 | Loss: 0.2184\n",
      "  Fine-tune Epoch 1 | Partition 9 | Loss: 0.2137\n",
      "  Fine-tune Epoch 1 | Partition 10 | Loss: 0.1873\n",
      "  Fine-tune Epoch 1 | Partition 11 | Loss: 0.1721\n",
      "  Fine-tune Epoch 1 | Partition 12 | Loss: 0.1577\n",
      "  Fine-tune Epoch 1 | Partition 13 | Loss: 0.1610\n",
      "  Fine-tune Epoch 1 | Partition 14 | Loss: 0.1808\n",
      "  Fine-tune Epoch 1 | Partition 15 | Loss: 0.1942\n",
      "  Fine-tune Epoch 1 | Partition 16 | Loss: 0.1958\n",
      "  Fine-tune Epoch 1 | Partition 17 | Loss: 0.1901\n",
      "  Fine-tune Epoch 1 | Partition 18 | Loss: 0.1562\n",
      "  Fine-tune Epoch 1 | Partition 19 | Loss: 0.1536\n",
      "  Fine-tune Epoch 1 | Partition 20 | Loss: 0.1546\n",
      "  Fine-tune Epoch 1 | Partition 21 | Loss: 0.1635\n",
      "  Fine-tune Epoch 1 | Partition 22 | Loss: 0.1693\n",
      "  Fine-tune Epoch 1 | Partition 23 | Loss: 0.1853\n",
      "  Fine-tune Epoch 1 | Partition 24 | Loss: 0.1922\n",
      "  Fine-tune Epoch 1 | Partition 25 | Loss: 0.1850\n",
      "  Fine-tune Epoch 1 | Partition 26 | Loss: 0.1463\n",
      "  Fine-tune Epoch 1 | Partition 27 | Loss: 0.1504\n",
      "  Fine-tune Epoch 1 | Partition 28 | Loss: 0.1563\n",
      "  Fine-tune Epoch 1 | Partition 29 | Loss: 0.1613\n",
      "  Fine-tune Epoch 1 | Partition 30 | Loss: 0.1721\n",
      "  Fine-tune Epoch 1 | Partition 31 | Loss: 0.1795\n",
      "  Fine-tune Epoch 1 | Partition 32 | Loss: 0.1931\n",
      "  Fine-tune Epoch 1 | Partition 33 | Loss: 0.1860\n",
      "  Fine-tune Epoch 1 | Partition 34 | Loss: 0.1530\n",
      "  Fine-tune Epoch 1 | Partition 35 | Loss: 0.1392\n",
      "  Fine-tune Epoch 1 | Partition 36 | Loss: 0.1545\n",
      "  Fine-tune Epoch 1 | Partition 37 | Loss: 0.1634\n",
      "  Fine-tune Epoch 1 | Partition 38 | Loss: 0.1798\n",
      "  Fine-tune Epoch 1 | Partition 39 | Loss: 0.1949\n",
      "  Fine-tune Epoch 1 | Partition 40 | Loss: 0.1990\n",
      "  Fine-tune Epoch 1 | Partition 41 | Loss: 0.1943\n",
      "  Fine-tune Epoch 1 | Partition 42 | Loss: 0.1545\n",
      "  Fine-tune Epoch 1 | Partition 43 | Loss: 0.1525\n",
      "  Fine-tune Epoch 1 | Partition 44 | Loss: 0.1605\n",
      "  Fine-tune Epoch 1 | Partition 45 | Loss: 0.1618\n",
      "  Fine-tune Epoch 1 | Partition 46 | Loss: 0.1726\n",
      "  Fine-tune Epoch 1 | Partition 47 | Loss: 0.1965\n",
      "  Fine-tune Epoch 1 | Partition 48 | Loss: 0.2000\n",
      "  Fine-tune Epoch 1 | Partition 49 | Loss: 0.2042\n",
      "\n",
      "--- FINE-TUNE Epoch 2/2 ---\n",
      "  Fine-tune Epoch 2 | Partition 1 | Loss: 0.1919\n",
      "  Fine-tune Epoch 2 | Partition 2 | Loss: 0.1592\n",
      "  Fine-tune Epoch 2 | Partition 3 | Loss: 0.1480\n",
      "  Fine-tune Epoch 2 | Partition 4 | Loss: 0.1527\n",
      "  Fine-tune Epoch 2 | Partition 5 | Loss: 0.1612\n",
      "  Fine-tune Epoch 2 | Partition 6 | Loss: 0.1647\n",
      "  Fine-tune Epoch 2 | Partition 7 | Loss: 0.1790\n",
      "  Fine-tune Epoch 2 | Partition 8 | Loss: 0.1798\n",
      "  Fine-tune Epoch 2 | Partition 9 | Loss: 0.1765\n",
      "  Fine-tune Epoch 2 | Partition 10 | Loss: 0.1526\n",
      "  Fine-tune Epoch 2 | Partition 11 | Loss: 0.1436\n",
      "  Fine-tune Epoch 2 | Partition 12 | Loss: 0.1449\n",
      "  Fine-tune Epoch 2 | Partition 13 | Loss: 0.1519\n",
      "  Fine-tune Epoch 2 | Partition 14 | Loss: 0.1719\n",
      "  Fine-tune Epoch 2 | Partition 15 | Loss: 0.1867\n",
      "  Fine-tune Epoch 2 | Partition 16 | Loss: 0.1899\n",
      "  Fine-tune Epoch 2 | Partition 17 | Loss: 0.1826\n",
      "  Fine-tune Epoch 2 | Partition 18 | Loss: 0.1497\n",
      "  Fine-tune Epoch 2 | Partition 19 | Loss: 0.1490\n",
      "  Fine-tune Epoch 2 | Partition 20 | Loss: 0.1504\n",
      "  Fine-tune Epoch 2 | Partition 21 | Loss: 0.1602\n",
      "  Fine-tune Epoch 2 | Partition 22 | Loss: 0.1662\n",
      "  Fine-tune Epoch 2 | Partition 23 | Loss: 0.1817\n",
      "  Fine-tune Epoch 2 | Partition 24 | Loss: 0.1882\n",
      "  Fine-tune Epoch 2 | Partition 25 | Loss: 0.1804\n",
      "  Fine-tune Epoch 2 | Partition 26 | Loss: 0.1432\n",
      "  Fine-tune Epoch 2 | Partition 27 | Loss: 0.1469\n",
      "  Fine-tune Epoch 2 | Partition 28 | Loss: 0.1538\n",
      "  Fine-tune Epoch 2 | Partition 29 | Loss: 0.1578\n",
      "  Fine-tune Epoch 2 | Partition 30 | Loss: 0.1688\n",
      "  Fine-tune Epoch 2 | Partition 31 | Loss: 0.1742\n",
      "  Fine-tune Epoch 2 | Partition 32 | Loss: 0.1894\n",
      "  Fine-tune Epoch 2 | Partition 33 | Loss: 0.1817\n",
      "  Fine-tune Epoch 2 | Partition 34 | Loss: 0.1502\n",
      "  Fine-tune Epoch 2 | Partition 35 | Loss: 0.1354\n",
      "  Fine-tune Epoch 2 | Partition 36 | Loss: 0.1546\n",
      "  Fine-tune Epoch 2 | Partition 37 | Loss: 0.1604\n",
      "  Fine-tune Epoch 2 | Partition 38 | Loss: 0.1749\n",
      "  Fine-tune Epoch 2 | Partition 39 | Loss: 0.1913\n",
      "  Fine-tune Epoch 2 | Partition 40 | Loss: 0.1963\n",
      "  Fine-tune Epoch 2 | Partition 41 | Loss: 0.1893\n",
      "  Fine-tune Epoch 2 | Partition 42 | Loss: 0.1502\n",
      "  Fine-tune Epoch 2 | Partition 43 | Loss: 0.1505\n",
      "  Fine-tune Epoch 2 | Partition 44 | Loss: 0.1578\n",
      "  Fine-tune Epoch 2 | Partition 45 | Loss: 0.1585\n",
      "  Fine-tune Epoch 2 | Partition 46 | Loss: 0.1695\n",
      "  Fine-tune Epoch 2 | Partition 47 | Loss: 0.1931\n",
      "  Fine-tune Epoch 2 | Partition 48 | Loss: 0.1966\n",
      "  Fine-tune Epoch 2 | Partition 49 | Loss: 0.2028\n",
      "\n",
      "--- FINE-TUNE Finished ---\n"
     ]
    }
   ],
   "source": [
    "fine_tune_with_val(best_ckpt_path=\"model_checkpoint_v1_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e87c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMLP(nn.Module):\n",
    "\tdef _init_(\n",
    "\t\tself,\n",
    "\t\tcat_cardinalities: Dict[str, int],\n",
    "\t\tcat_features: List[str],\n",
    "\t\tnum_cont_features: int,          # numéricas + indicadores de missing\n",
    "\t\tembedding_dim: int = 64,\n",
    "\t\thidden_dims: List[int] = [1024, 512, 256, 128, 64],\n",
    "\t\tdropout: float = 0.1,\n",
    "\t):\n",
    "\t\tsuper()._init_()\n",
    "\t\tself.cat_features = cat_features\n",
    "\t\tself.num_cat = len(cat_features)\n",
    "\t\tself.num_cont_features = num_cont_features\n",
    "\n",
    "\t\t# Embeddings por feature categórica\n",
    "\t\tself.embeddings = nn.ModuleDict({\n",
    "\t\t\tfeat: nn.Embedding(num_embeddings=cardinality, embedding_dim=embedding_dim)\n",
    "\t\t\tfor feat, cardinality in cat_cardinalities.items()\n",
    "\t\t})\n",
    "\n",
    "\t\t# Dimensión de entrada al MLP: [contínuas + embeddings]\n",
    "\t\ttotal_cat_dim = embedding_dim * self.num_cat\n",
    "\t\tinput_dim = self.num_cont_features + total_cat_dim\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tprev_dim = input_dim\n",
    "\t\tfor h in hidden_dims:\n",
    "\t\t\tlayers.append(nn.Linear(prev_dim, h))\n",
    "\t\t\tlayers.append(nn.ReLU())\n",
    "\t\t\tlayers.append(nn.Dropout(dropout))\n",
    "\t\t\tprev_dim = h\n",
    "\n",
    "\t\tlayers.append(nn.Linear(prev_dim, 1))  # regresión escalar\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\"\"\"\n",
    "\t\tx: [batch, num_cat + num_cont_features]\n",
    "\t\tcolumnas en el orden:\n",
    "\t\t[CATEGORICAL_FEATURES..., NUMERICAL_FEATURES..., NUMERICAL_IS_MISSING...]\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# 1) Separar categóricas y numéricas del gran vector\n",
    "\t\tx_cat_ids = x[:, :self.num_cat].long()     # [batch, num_cat]\n",
    "\t\tx_cont = x[:, self.num_cat:].float()       # [batch, num_cont_features]\n",
    "\n",
    "\t\t# 2) Embeddings categóricas\n",
    "\t\temb_list = []\n",
    "\t\tfor i, feat in enumerate(self.cat_features):\n",
    "\t\t\temb = self.embeddings[feat](x_cat_ids[:, i])  # [batch, embedding_dim]\n",
    "\t\t\temb_list.append(emb)\n",
    "\n",
    "\t\tif emb_list:\n",
    "\t\t\tx_cat_emb = torch.cat(emb_list, dim=1)        # [batch, embedding_dim * num_cat]\n",
    "\t\t\tx_input = torch.cat([x_cont, x_cat_emb], dim=1)\n",
    "\t\telse:\n",
    "\t\t\tx_input = x_cont\n",
    "\n",
    "\t\tout = self.mlp(x_input)   # [batch, 1]\n",
    "\t\treturn out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839706c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test partition 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     29\u001b[39m test_dataset = TensorDataset(test_features_tensor)\n\u001b[32m     30\u001b[39m test_loader = DataLoader(dataset=test_dataset, \n\u001b[32m     31\u001b[39m \t\t\t\t\t\t\tbatch_size=\u001b[32m32768\u001b[39m, \n\u001b[32m     32\u001b[39m \t\t\t\t\t\t\tnum_workers=\u001b[32m0\u001b[39m,\n\u001b[32m     33\u001b[39m \t\t\t\t\t\t\tpin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     34\u001b[39m \t\t\t\t\t\t\tshuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m model = \u001b[43mEmbeddingMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATEGORICAL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cont_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUMERICAL_FEATURES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     37\u001b[39m model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mmodel_checkpoint_v4_1.pth\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     38\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:485\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    486\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    492\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'"
     ]
    }
   ],
   "source": [
    "DATASET_TEST_PATH = \"./data/test\"\n",
    "\n",
    "ddf_test = dd.read_parquet(\n",
    "\tDATASET_TEST_PATH,\n",
    "\tengine=\"pyarrow\",\n",
    "\tcolumns=[obj for obj in COLS_TO_READ if obj != \"iap_revenue_d7\"] + [\"row_id\"]\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "row_ids = []\n",
    "\n",
    "partition_iterator = ddf_test.to_delayed()\n",
    "\n",
    "for partition_idx, partition in enumerate(partition_iterator):\n",
    "\n",
    "\tprint(f\"Processing test partition {partition_idx+1}\")\n",
    "\n",
    "\ttest_pdf = partition.compute()\n",
    "\n",
    "\n",
    "\trow_ids.extend(test_pdf[\"row_id\"].values)\n",
    "\ttest_pdf = process_partition(test_pdf)\n",
    "\n",
    "\ttest_features_tensor = torch.tensor(\n",
    "\t\ttest_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\n",
    "\ttest_dataset = TensorDataset(test_features_tensor)\n",
    "\ttest_loader = DataLoader(dataset=test_dataset, \n",
    "\t\t\t\t\t\t\t\tbatch_size=32768, \n",
    "\t\t\t\t\t\t\t\tnum_workers=0,\n",
    "\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\tshuffle=False)\n",
    "\t\n",
    "\tmodel = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "\tmodel.load_state_dict(torch.load(\"model_checkpoint_v4_1.pth\"))\n",
    "\tmodel.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_idx, (data,) in enumerate(test_loader):\n",
    "\t\t\tdata = data.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\toutput = model(data)\n",
    "\n",
    "\t\t\tpredictions.extend(output.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4895a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria.flores\\AppData\\Local\\Temp\\ipykernel_21512\\2318167038.py:3: RuntimeWarning: overflow encountered in expm1\n",
      "  \"iap_revenue_d7\": np.maximum(np.expm1(predictions),0.0)  # inverse of log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 row_id  iap_revenue_d7\n",
      "0  e2f514a9-d922-4a17-bf94-f228bf4cd82f    1.016614e-03\n",
      "1  4bfc70d3-d619-410a-9683-4cd759f30f32    4.789741e-03\n",
      "2  ad433b66-b41e-4157-a6fd-24cd30701f6a    2.974149e-08\n",
      "3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f    4.402779e-03\n",
      "4  81b73a45-c395-4d08-a4a3-513873440db3    5.463762e-06\n",
      "Test predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "\t\"row_id\": row_ids,\n",
    "\t\"iap_revenue_d7\": np.clip(np.expm1(predictions),a_min=0.0,a_max=10000.0)  # inverse of log1p\n",
    "})\n",
    "print(pred_df.head())\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b1e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test partition 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     29\u001b[39m test_dataset = TensorDataset(test_features_tensor)\n\u001b[32m     30\u001b[39m test_loader = DataLoader(dataset=test_dataset, \n\u001b[32m     31\u001b[39m \t\t\t\t\t\t\tbatch_size=\u001b[32m32768\u001b[39m, \n\u001b[32m     32\u001b[39m \t\t\t\t\t\t\tnum_workers=\u001b[32m0\u001b[39m,\n\u001b[32m     33\u001b[39m \t\t\t\t\t\t\tpin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     34\u001b[39m \t\t\t\t\t\t\tshuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m model = \u001b[43mEmbeddingMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATEGORICAL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cont_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUMERICAL_FEATURES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     37\u001b[39m model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mmodel_checkpoint_v3_5.pth\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     38\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria.flores\\Documents\\Projects\\hacks\\datathon2025-smadex\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:485\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    486\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    492\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: EmbeddingMLP.__init__() got an unexpected keyword argument 'cat_cardinalities'"
     ]
    }
   ],
   "source": [
    "DATASET_TEST_PATH = \"./data/test\"\n",
    "\n",
    "ddf_test = dd.read_parquet(\n",
    "\tDATASET_TEST_PATH,\n",
    "\tengine=\"pyarrow\",\n",
    "\tcolumns=[obj for obj in COLS_TO_READ if obj != \"iap_revenue_d7\"] + [\"row_id\"]\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "row_ids = []\n",
    "\n",
    "partition_iterator = ddf_test.to_delayed()\n",
    "\n",
    "for partition_idx, partition in enumerate(partition_iterator):\n",
    "\n",
    "\tprint(f\"Processing test partition {partition_idx+1}\")\n",
    "\n",
    "\ttest_pdf = partition.compute()\n",
    "\n",
    "\n",
    "\trow_ids.extend(test_pdf[\"row_id\"].values)\n",
    "\ttest_pdf = process_partition(test_pdf)\n",
    "\n",
    "\ttest_features_tensor = torch.tensor(\n",
    "\t\ttest_pdf[CATEGORICAL_FEATURES + NUMERICAL_FEATURES + [f\"{col}_is_missing\" for col in NUMERICAL_FEATURES]].values, \n",
    "\t\tdtype=torch.float32\n",
    "\t)\n",
    "\n",
    "\ttest_dataset = TensorDataset(test_features_tensor)\n",
    "\ttest_loader = DataLoader(dataset=test_dataset, \n",
    "\t\t\t\t\t\t\t\tbatch_size=32768, \n",
    "\t\t\t\t\t\t\t\tnum_workers=0,\n",
    "\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\tshuffle=False)\n",
    "\t\n",
    "\tmodel = EmbeddingMLP(cat_cardinalities=cat_cardinalities, cat_features=CATEGORICAL_FEATURES, num_cont_features=2*len(NUMERICAL_FEATURES)).to(device)\n",
    "\tmodel.load_state_dict(torch.load(\"model_checkpoint_v3_5.pth\"))\n",
    "\tmodel.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_idx, (data,) in enumerate(test_loader):\n",
    "\t\t\tdata = data.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\toutput = model(data)\n",
    "\n",
    "\t\t\tpredictions.extend(output.cpu().numpy())\n",
    "\n",
    "predictions_exp = np.expm1(predictions)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "\t\"row_id\": row_ids,\n",
    "\t\"iap_revenue_d7\": np.clip(predictions_exp, a_min=0.0, a_max=10000.0)\n",
    "})\n",
    "print(pred_df.head())\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon2025-smadex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
